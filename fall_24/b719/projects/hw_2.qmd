---
title: "BIOSTAT 719 - Homework 2"
---



## Problem 1

> Show that the following probability density functions belong to the exponential family. Define a(·), b(·), c(·), d(·) components.
>
>**Part (a):** Pareto distribution $f(y; \theta) = \theta y^{-\theta-1}$

For all of these, we must show that the function holds the form $\exp\left[ a(y)b(\theta) + c(\theta) + d(y) \right]$. Let's rewrite the Pareto PDF just a smidge: 


\begin{align*}
f(y; \theta) &= \exp[-\theta \log(y)+ \log(\theta) - \log(y)]\\
&= \exp[a(y)b(\theta) + c(\theta) + d(y)],
\end{align*}


Where: 

* $a(y) = \log(y)$
* $b(\theta) = -\theta$
* $c(\theta) = \log(\theta)$
* $d(y) = -\log(y)$

We can clearly see that the Pareto distribution is a member of the exponential family. 


> **Part (b):** Exponential distribution $f(y; \theta) = \theta^{-y\theta}$

Once again, let's rewrite this PDF:


\begin{align*}
f(y; \theta) &= \exp[-y\theta + \log(\theta)]\\
&= \exp[a(y)b(\theta) + c(\theta) + d(y)],
\end{align*}

Where: 

* $a(y) = y$
* $b(\theta) = -\theta$
* $c(\theta) = \log(\theta)$
* $d(y) = 0$

The exponential distribution, thankfully, is a member of the exponential family. Otherwise, I think that would get awkward at family reunions.


> **Part (c):** Negative binomial distribution $f(y; \theta) = {y + r - 1 \choose r - 1} \theta^r(1 - \theta)^y$, where $r$ is known

Using the same method, we get the following:


\begin{align*}
f(y; \theta) &= exp\left[y\log(1 - \theta) + r\log(\theta) + \log\left[{y + r - 1\choose r - 1} \right]\right]\\
&= \exp[a(y)b(\theta) + c(\theta) + d(y)],
\end{align*}

Where: 

* $a(y) = y$
* $b(\theta) = \log(1 - \theta)$
* $c(\theta) = r\log(\theta)$
* $d(y) = \log\left[{y + r - 1\choose r - 1} \right]$

The negative binomial distribution is also part of the exponential family. 

**Part (d):** Extreme value (Gumbel) distribution $f(y; \theta) = \frac{1}{\Phi} exp\left \{  \frac{y - \theta}{\Phi} - exp\left[ \frac{y - \theta}{\Phi} \right]  \right \}$, where $\Phi > 0$ is considered a nuissance parameter. 

Similar to how we conducted part (c), we are going to ignore nuissance parameters (i.e. treat them as constants) and focus on the parameter of interest, $\theta$. 

We can rewrite the PDF as follows: 


\begin{align*}
f(y; \theta) &= exp\left[ -exp\left[ \frac{y - \theta}{\Phi} \right] - \frac{\theta}{\Phi} + \frac{y}{\Phi} + \log\left(\frac{1}{\Phi} \right)\right]\\
&= \exp[a(y)b(\theta) + c(\theta) + d(y)],
\end{align*}


Where: 

* $a(y) = -e^{y/\Phi}$
* $b(\theta) = e^{-\theta/\Phi}$
* $c(\theta) = -\frac{\theta}{\Phi}$
* $d(y) = \frac{y}{\Phi} + \log\left[\frac{1}{\Phi} \right]$

This, too, is part of the exponential family. 


## Problem 2


> Consider a randome variable Y with the following Gamma distribution with a scale parameter, $\beta$, of interest, and a known shape parameter $\alpha$: 
>
> $$
>f(y; \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}y^{\alpha - 1}e^{-y\beta}, \text{  where } y > 0, \quad \alpha > 0, \quad \beta > 0
>$$

>**Part (a):**

> Does this distribution belong to the exponential family?

To answer this question, let's once again see if we can rewrite this function in terms of a(.), b(.), c(.), and d(.). Note that because $\beta$ is the parameter of interest, we treat $\alpha$ as the nuissance parameter. 


$$
\begin{align*}
f(y; \theta) &= exp\left[ -y\beta + \alpha \log(\beta) - \log\left( \Gamma(\alpha)\right)+ \alpha\log(y) - \log(y)\right]\\
&= \exp[a(y)b(\beta) + c(\beta) + d(y)],
\end{align*}
$$

Where: 

* $a(y) = y$
* $b(\beta) = -\beta$
* $c(\theta) = \alpha \log(\beta) - \log\left( \Gamma(\alpha)\right)$
* $d(y) = \alpha\log(y) - \log(y)$

Yes, the Gamma distribution belongs to the exponential family. This will assist us in deriving expectation and variance. 

**Part (b):**

> Derive expectation of Y

Recall that for any random variable $Y$ with a distribution belonging to the exponential family, $E[a(y)] = -c'(\theta)/b'(\theta)$. We can solve for these values and evaluate $E(a(y))$, which is $E[Y]$. 


\begin{align*}
-c'(\beta) &= - \frac{d c(\beta)}{d\beta}\\
&= -\frac{d}{d\beta}\left[ \alpha \log(\beta) - \log\left( \Gamma(\alpha)\right)\right]\\
&= -\frac{\alpha}{\beta}\\
b'(\beta) &= \frac{d b(\beta)}{d\beta}\\
&= \frac{d}{d\beta} - \beta\\
&= -1\\
-c'(\beta)/b'(\beta) &= -\frac{\alpha}{\beta}/-1\\
&= \frac{\alpha}{\beta}
\end{align*}


**Part (c):**

> Derive the variance of Y

Recall another important property of the exponential family. For any random variable $Y$ with a distribution belonging to the exponential family, $V[a(y)] = \frac{b''(\theta)c'(\theta) - c''(\theta)b'(\theta)}{[b'(\theta)]^3}$. 


\begin{align*}
b'(\beta) &= -1\\
c'(\beta) &= \frac{\alpha}{\beta}\\
b''(\beta) &= 0\\
c''(\beta) &= -\frac{\alpha}{\beta^2}\\
V[a(y)] &= \frac{b''(\beta)c'(\beta) - c''(\beta)b'(\beta)}{[b'(\beta)]^3}\\
&= \frac{0 - \frac{\alpha}{\beta^2}}{[-1]^3}\\
&= \frac{\alpha}{\beta^2}
\end{align*}



**Part (d):**

> Derive variance of the score statistic

Recall that the variance of the score statistic, also known as the Fisher's information, can be expressed as follows: $V[U] = I(\theta) = \frac{b''(\theta) c'(\theta)}{b'(\theta)} - c''(\theta)$. Once again, we can plug and chug. 


\begin{align*}
b'(\beta) &= -1\\
c'(\beta) &= \frac{\alpha}{\beta}\\
b''(\beta) &= 0\\
c''(\beta) &= -\frac{\alpha}{\beta^2}\\
V[U] &= \frac{b''(\beta)c'(\beta) }{b'(\beta)} - c''(\beta)\\
&= 0 - \left( -\frac{\alpha}{\beta^2}\right)\\
&= \frac{\alpha}{\beta^2}
\end{align*}


## Problem 3 

> Derive expression for information (consider $\theta$ the parameter of interest) for a single observation from the Weibull distribution:
>
>$$
>f(y; \lambda , \theta) = \frac{\lambda y^{\lambda - 1}}{\theta^{\lambda}}\exp\left[ -\left(\frac{y}{\theta}\right)^{\lambda}\right],
>$$
>
> where $y \ge 0, \lambda > 0,$ and $\theta > 0$. Show all your work. 

Let's start by identifying a(.), b(.), c(.), and d(.). 


\begin{align*}
f(y; \lambda, \theta) &= \frac{\lambda y^{\lambda - 1}}{\theta^{\lambda}}\exp\left[ -\left(\frac{y}{\theta}\right)^{\lambda}\right]\\
&= \exp\left[- \frac{y^{\lambda}}{\theta^{\lambda}} -\lambda \log(\theta) - \log(y) + \lambda\log(y) + \log(\lambda)  \right]\\
&= \exp[a(y)b(\theta) + c(\theta) + d(y)],
\end{align*}

where 

* $a(y) = y^{\lambda}$
* $b(\theta) = -\frac{1}{\theta^{\lambda}}$
* $c(\theta) = -\lambda \log(\theta)$
* $d(y) = - \log(y) + \lambda\log(y) + \log(\lambda)$

Using our previously helpful formula for calculating Fisher's information, we can derive an expression for $I(\theta)$:

\begin{align*}
b'(\theta) &= \lambda \theta^{-\lambda - 1}\\
c'(\theta) &= - \lambda\theta^{-1}\\
b''(\theta) &= (-\lambda - 1)\lambda\theta^{-\lambda - 2}\\
c''(\theta) &= \lambda\theta^{-2}\\
I(\theta) &= \frac{b''(\beta)c'(\beta) }{b'(\beta)} - c''(\beta)\\
&= \frac{\left[ (-\lambda-1)\lambda\theta^{-\lambda - 2}\right]\left[-\lambda\theta^{-1}\right]}{\lambda\theta^{-\lambda-1}} - \lambda\theta^{-2}\\
&= (-\lambda)(-\lambda-1)(\lambda)(\lambda^{-1})(\theta^{-\lambda-3})(\theta^{\lambda+1})-\lambda\theta^{-2}\\
&= (-\lambda)(-\lambda-1)(\theta^{-2})-\lambda\theta^{-2}\\
&= (\lambda\theta^{-2})(\lambda+1)-\lambda\theta^{-2}\\
&= \lambda\theta^{-2}((\lambda+1)-1)\\
&= \lambda\theta^{-2}(\lambda)\\
&= \left(\frac{\lambda}{\theta}\right)^{2}\\
\end{align*}


## Problem 4

> Suppose $Y_1, ..., Y_n$ are independent random variables, each with the Pareto distribution:
>
> $$
> f(y_i;\theta) = \frac{\theta}{y_i^{\theta + 1}}, \text{ where } y_i > 1, \theta > 0 \text{ for all } i = 1, ..., n
> $$
>
> and
>
> $$
> E(Y_i) = (\beta_0 + \beta_1x_i)^2
> $$
>
> **Part (a):** Does this distribution have the canonical form?

Let's first examine this PDF using the definition of the exponential family: 

$$
\begin{align*}
f(y_i; \theta) &= \frac{\theta}{y_i^{\theta + 1}}\\
&= \exp\left[\log(\theta) - (\theta + 1)\log(y_i)\right]\\
&= \exp\left[- \log(y_i)\theta + \log(\theta) - \log(y_i) \right]\\
&= \exp\left[a(y_i)b(\theta) + c(\theta) + d(y_i)\right],
\end{align*}
$$

where 

* $a(y_i) = - \log(y_i)$
* $b(\theta) = \theta$
* $c(\theta) = \log(\theta)$
* $d(y_i) = -\log(y_i)$

Because $a(y_i) \ne y_i$, this distribution does **not** have the canonical form. 

> **Part (b):** Are the distributions of all the $Y_i$’s of the same form?

Because each $Y_i$ follows the same distribution, $b_i(\theta) = b(\theta)$, $c_i(\theta) = c(\theta)$ and $d_i(y) = d(y)$ for all $i = 1,2,...,n$. This being the case, we know that all $Y_i$'s are of the same form. 


> **Part (c):** What is the link function? Is it monotone and differentiable?

The link function is a transformation of $E(Y_i)$ such that $g(E(Y_i)) = X_i^T\beta$. Because we know that $E(Y_i) = (X_i^T\beta)^2$, the link function is $\sqrt{E(Y_i)}$, which is both differentiable and monotone. 

> **Part (d):** Given your answers in (a) to (c), is this a generalized linear model? Give reasons for your answer.

While two of the three properties of GLMs are met by the pareto distribution, importantly, it does **not** have the canonical form. This is not a generalized linear model. 

## End of problem set 

[Back](../index.qmd)
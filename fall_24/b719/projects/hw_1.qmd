---
title: "BIOSTAT 719 - Homework 1"
---

```{r echo = FALSE, warning = FALSE, message = FALSE}
###################################### R ######################################
library(tidyverse)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center")
###############################################################################
```

## Problem 1

**(6 points) [Textbook Exercise 1.6]**

> The data  in  Table  1.4 are  the  numbers  of  females and males in the progeny of 16 female light brown apple moths in Muswellbrook, New South Wales, Australia (from Lewis 1987).

```{r echo = FALSE}
####### ### ### ### ### ### ### ### ### ### ### # R  ## ## ### ###############################
library(tidyverse)
pgroup <- 1:16
females <- c(
    18, 31, 34, 33, 27, 33, 28, 23, 33, 12, 19, 25, 14, 4, 22, 7
)
males <- c(
    11, 22, 27, 29, 24, 29, 25, 26, 38, 14, 23, 31, 20, 6, 34, 12
)
table_data <- data.frame(pgroup = pgroup, females = females, males = males)
pander::pander(table_data, col.names = c("Progeny Group", "Females", "Males"))
###############################################################################
```

> Let $Y_i$ denote the number of females in each of the 16 groups of progeny in each group ($i = 1, ..., 16$). Suppose the $Y_i$'s are independent random variables each with the Binomial distribution

$$
f(y_i; \theta) = {n_i \choose y_i}\theta^{y_i}(1 - \theta)^{n_i - y_i}
$$

> Find the maximum likelihood estimator (MLE) of $\theta$ using calculus and evaluate it for these data. 

Let's first find the MLE using calculus. Let's review the steps: 

1. Calculate the log-likelihood function
2. Take the first derivative of the log-likelihood function (this is the 'score' function)
3. Set the score function equal to 0 and solve for $\hat{\theta}$
4. Take the second derivative of the function to determine whether the function is concave down (i.e. has a maximum value at $\hat{\theta}$)

With that in mind, let's jump in! 

\begin{align*}
\ell (\theta_i ; y_i) &= \sum_{i = 1}^{16} \log\left( f(y_i; \theta) \right)\\
&= \sum_{i = 1}^{16} \log\left[ {n_i \choose y_i}\theta^{y_i}(1 - \theta)^{n_i - y_i}  \right]\\
&= \sum_{i = 1}^{16} \log {n_i \choose y_i} + \log(\theta)\sum_{i = 1}^{16} y_i + log(1 - \theta)\sum_{i = 1}^{16}(n_i - y_i)  \\
\frac{d \ell}{d\theta} &= 0 + \sum_{i = 1}^{16} \frac{y_i}{\theta} - \frac{\sum_{i = 1}^{16} n_i - \sum_{i = 1}^{16} y_i}{1 - \theta} \\
\underline{\text{set}} \quad 0 &=  \sum_{i = 1}^{16} \frac{y_i}{\hat{\theta}} - \frac{\sum_{i = 1}^{16} n_i - \sum_{i = 1}^{16} y_i}{1 - \hat{\theta}} \\ 
\implies \sum_{i = 1}^{16} \frac{y_i}{\hat{\theta}} &= \frac{\sum_{i = 1}^{16} n_i - \sum_{i = 1}^{16} y_i}{1 - \hat{\theta}} \\
\implies  \hat{\theta} \sum_{i = 1}^{16} n_i &= \sum_{i = 1}^{16} y_i \\
\implies \hat{\theta} &= \frac{1}{\sum_{i = 1}^{16} n_i} \sum_{i = 1}^{16} y_i
\end{align*}


To verify whether this is an MLE or not, let's now complete step 4 and take the second derivative of the log-likelihood function to see whether it is concave down at $\hat{\theta}$. 


\begin{align*}
\frac{d^2\ell}{d\theta^2} &= -\sum_{i = 1}^{16} \frac{y_i}{\theta^2} - \frac{\sum_{i = 1}^{16} n_i - \sum_{i = 1}^{16} y_i}{(1 - \theta)^2}
\end{align*}

Note that because $0 \le \theta \le 1$, and because $y = 0, 1, 2, ...$, and because $n = 1, 2, 3, ...$, the second derivative of the log-likelihood function will be negative for all values of $\theta$. That means that the first derivative has a maximum at $\hat{\theta}$. 


Thus, the MLE for a random sample of size 16 is $\hat{\theta} = \frac{1}{\sum_{i = 1}^{16} n_i} \sum_{i = 1}^{16} y_i$. Intuitively, this makes sense, because this is essentially the total number of females in all progenies divided by the total number of individuals. This value for $\hat{\theta}$ is evaluated to be $363/(371+363) = 0.49$. 


## Problem 2

**(12 points)**

> Let $Y_i, ... , Y_n$ be independent random variables from the exponential distribution

$$
f(y_i; \lambda) = \lambda e^{-\lambda y_i}, \quad y_i > 0, \lambda > 0
$$

**Part (a):**

> What is the maximum likelihood estimator (MLE) of $\lambda$? Show all the derivation details.

Let's follow the same four steps to find this out! 


\begin{align*}
\ell(\lambda) &= \sum_{i = 1}^n \log [f(y; \lambda)]\\
&= \sum_{i = 1}^n \log [\lambda e^{-\lambda y_i}]\\
&= \sum_{i = 1}^n \log (\lambda) - \sum_{i = 1}^n\lambda y_i\\
&= n \log (\lambda) - \sum_{i = 1}^n\lambda y_i\\
\frac{d\ell}{d\lambda} &= \frac{n}{\lambda} - \sum_{i = 1}^n y_i\\
\underline{\text{set}}\quad 0 &= \frac{n}{\hat{\lambda}} - \sum_{i = 1}^n y_i\\
\implies \frac{n}{\hat{\lambda}} &= \sum_{i = 1}^n y_i\\
\implies \hat{\lambda} &= \frac{n}{\sum_{i = 1}^n y_i}\\
\implies \hat{\lambda} &= \frac{1}{\bar{Y}}
\end{align*}

This is a fun result, especially considering the fact that the rate parameter is the inverse of the scale parameter. So, by the invariance property of MLEs, we know that the MLE for $\theta$ (the scale parameter) is $\bar{Y}$. 

However, remember that we still need to check the second derivative to ensure this actually is an MLE. 

We can easily see that the second derivative of the log-likelihood function, $\frac{d^2\ell}{d\lambda^2}$, is $-\frac{n}{\lambda^2}$. Considering that neither $n$ nor $\lambda$ can take on a negative value, the first derivative (i.e. the score function) will always be concave down, and we can state with certainty that the MLE for $\lambda$ is $\frac{1}{\bar{Y}}$

**Part (b):**

> Suppose $\lambda = e^{\beta}$. Find the MLE of $\beta$. 

We are given that $\lambda = e^{\beta}$. Thus, $\beta = \log{\lambda}$. By the invariance property of MLEs, if $g(\lambda)$ is a function of $\lambda$, then the MLE of $g(\theta)$ is $g(\hat{\theta})$. 

Let $g(\theta) = \log(\lambda) = \beta$. $g(\hat{\lambda}) = \log\left( \frac{1}{\bar{Y}}\right)$, which is the MLE of $\beta$. 

**Part (c):**

> Consider 150 observations $y_i, (i = 1, 2, ..., 150)$ from the exponential distribution with the sum of the 150 observations equal to 30. What is the numerical evaluation of the MLE of $\lambda$ and $\beta$? 


Easy peasy. We did all the hard stuff already, so it's just plug and chug time!


For $\hat{\lambda}$:

\begin{align*}
\hat{\lambda} &= \frac{1}{\bar{Y}}\\
&= \frac{1}{\frac{30}{150}}\\
&= 5
\end{align*}

For $\hat{\beta}$:

\begin{align*}
\hat{\beta} &= \log\left[\frac{1}{\bar{Y}}\right]\\
&= \log(5)
\end{align*}

Thus, the numerical evaluations of the MLEs of $\lambda$ and $\beta$ are $5$ and $\log(5)$, respectively. 
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Introduction to Statistical Theory and Methods I (BIOSTAT 701)\nApplied Biostatistical Methods I (BIOSTAT 702)\nIntroduction to the Practice of Biostatistics I (BIOSTAT 703)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nProgramming, Data Structures, and Algorithms in C++ (ECE 551D)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)\n\n\n\n\nIntroduction to Statistical Theory and Methods II (BIOSTAT 704)\nApplied Biostatistical Methods II (BIOSTAT 705)\nIntroduction to the Practice of Biostatistics II (BIOSTAT 706)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nSoftware Tools for Data Science (BIOSTAT 821)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)\n\n\n\n\nStatistical Methods for Learning and Discovery (BIOSTAT 707)\nGeneralized Linear Models (BIOSTAT 719)\nStatistical Programming for Big Data (BIOSTAT 823)\n\n(Note: This semester is in progress)\n\n\n\n\nLongitudinal and Correlated Data Analysis (BIOSTAT 718)\nIntroduction to Applied Bayesian Analysis (BIOSTAT 724)\nSpecial Topics in Biomedical Engineering: Spatial Omics (BME 590)\nComputational Sequence Biology (CBB 561)\n\n(Note: This semester has not yet started)\nThis is Austin Allen’s website for his Duke MB Program Projects. Looking for Austin’s personal website? Navigate here."
  },
  {
    "objectID": "index.html#fall-2023",
    "href": "index.html#fall-2023",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Introduction to Statistical Theory and Methods I (BIOSTAT 701)\nApplied Biostatistical Methods I (BIOSTAT 702)\nIntroduction to the Practice of Biostatistics I (BIOSTAT 703)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nProgramming, Data Structures, and Algorithms in C++ (ECE 551D)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)"
  },
  {
    "objectID": "index.html#spring-2024",
    "href": "index.html#spring-2024",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Introduction to Statistical Theory and Methods II (BIOSTAT 704)\nApplied Biostatistical Methods II (BIOSTAT 705)\nIntroduction to the Practice of Biostatistics II (BIOSTAT 706)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nSoftware Tools for Data Science (BIOSTAT 821)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)"
  },
  {
    "objectID": "index.html#fall-2024",
    "href": "index.html#fall-2024",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Statistical Methods for Learning and Discovery (BIOSTAT 707)\nGeneralized Linear Models (BIOSTAT 719)\nStatistical Programming for Big Data (BIOSTAT 823)\n\n(Note: This semester is in progress)"
  },
  {
    "objectID": "index.html#spring-2025",
    "href": "index.html#spring-2025",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Longitudinal and Correlated Data Analysis (BIOSTAT 718)\nIntroduction to Applied Bayesian Analysis (BIOSTAT 724)\nSpecial Topics in Biomedical Engineering: Spatial Omics (BME 590)\nComputational Sequence Biology (CBB 561)\n\n(Note: This semester has not yet started)\nThis is Austin Allen’s website for his Duke MB Program Projects. Looking for Austin’s personal website? Navigate here."
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW2_SQL.html",
    "href": "fall_24/b823/projects/downloads/HW2_SQL.html",
    "title": "Biostat 823 - Fall 2024: Homework 2",
    "section": "",
    "text": "Note: You should submit a reproducible literate programming notebook (Jupyter or Rmarkdown) with SQL statements to generate the answers for these questions (samples of reproducible notebooks for SQL can be found in the course’s GitHub repository. Using SQLite as the RDBMS is not mandatory but highly recommended."
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW2_SQL.html#question-1-38-points",
    "href": "fall_24/b823/projects/downloads/HW2_SQL.html#question-1-38-points",
    "title": "Biostat 823 - Fall 2024: Homework 2",
    "section": "Question 1 (38 points)",
    "text": "Question 1 (38 points)\nYou will be using the Chinook database to answer the following questions. This is a sample database available for various RDBMSs, including SQLite. The .db file for SQLite provided to you was created by executing the corresponding SQLite script. You can recreate the SQLite database like this:\n$ sqlite3 chinook.db &lt; Chinook_Sqlite.sql\nThe Chinook data model represents a digital media store, including tables for artists, albums, media tracks, invoices and customers. You can find more information about this database by referring to its Github Repository.\nNote: The E-R diagram given there (and shown below) does not use crow’s foot notation to signify relationship cardinalities; instead, only lines are drawn between entities that have a relationship. You can infer the general cardinality by which entity on one end of the line has a foreign key to the entity on the other end (i.e., has a column that isn’t the primary key but is named the same as the primary key of the other table); the side with the foreign key has cardinality 0..n, whereas the other has cardinality 0..1 (whether 0 or 1 depends on whether the foreign key is NULLable or not). Notice that some tables have two foreign keys and thus resolve an n:n relationship between the two entities referenced by the foreign keys; some of these have no other attributes and thus are “hash tables”, whereas others have other attributes and are thus weak entities as discussed in the lesson on relational modeling.\n\nComplete the following tasks. You will need to connect to the Chinook.db database. (E.g., using %sql magic if you use Jupyter Notebook powered by DCC: use %load_ext sql to load the SQL extension, and then use %sql sqlite:///&lt;YOUR PATH&gt;/Chinook.db to connect with database Chinook.db.)\n\n(3 points) Find all tables in the database and display the first 3 rows of tables Customer, Employee, Invoice, and InvoiceLine with column headings. We will be primarily working on these four tables.\n(2 points) Provide a query showing Customers (just full names, customer ID and country) who are not in the US. You only need to display the first 10 rows, and you don’t have to concatenate the customers’ first and last names.\n(2 points) Provide a query showing a unique list of billing countries from the Invoice table.\n(2 points) Provide a query only showing the Customers from Germany.\n(2 points) Provide a query showing only the Employees who are Sales Agents (i.e., title = ‘Sales Support Agent’).\n(3 points) Provide a query showing the invoices of customers who are from Germany. The resultant table should show the customer’s full name, Invoice ID, Date of the invoice and billing country.\n(3 points) Provide a query that shows the invoice Total, invoice date, Customer first and last name, and Country. Order by invoice date.\n(4 points) Provide a query that shows the invoices associated with each sales agent for customers from Germany. The resultant table should include the Sales Agent’s full name.\n(2 points) How many Invoices were there in 2021 and 2022? (Hint: the clause between datetime(‘2021-01-01 00:00:00’) and datetime(‘2022-12-31 00:00:00’) will help you filter out invoices during this time frame.)\n(2 points) Looking at the InvoiceLine table, provide a query that counts the number of line items for Invoice ID 37.\n(3 points) Looking at the InvoiceLine table, provide a query that counts the number of line items for each Invoice.\n(2 points) Provide a query that shows the number of invoices by (per) country.\n(2 points) Provide a query that shows total sales made by each sales agent.\n(3 points) Which sales agent made the most in sales in 2021? (Hint: count the total sales in 2021 grouped by sales agent.)\n(3 points) Provide a query that shows the total sales per country. Which country’s customers spent the most?\n\nRemark: When specific output requirements aren’t stated, you may choose to display all columns in your resulting tables or display only some of them (but you must include the primary key). For example, for question 4 showing customers from Germany, you can display all columns in table Customer, or only display selected columns (but you must include primary key CustomerId)."
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW2_SQL.html#question-2-12-points",
    "href": "fall_24/b823/projects/downloads/HW2_SQL.html#question-2-12-points",
    "title": "Biostat 823 - Fall 2024: Homework 2",
    "section": "Question 2 (12 points)",
    "text": "Question 2 (12 points)\nYou will be working with an SQLite3 database pets.db with 3 tables dog, treat and dog_treat. The dog_treat table is a linker table showing which dog ate which treat.\nNote: Although it would be best practice to always attach an E-R diagram (and potentially scripts to populate the database from scratch), in real scenarios this is often not the case. Instead, you may need to explore the tables and their columns using the tools afforded by the RDBMS or other exploratory data analysis environments. This problem emulates such a sitation – all we have at first is a database file. To explore, for example the sqlite3 command line tool has so-called dot-commands (such as .tables), and the Jupyter Lab SQL extension supports %sqlcmd commands for inspecting tables and columns, and one for exploring the content of tables (%sqlcmd explore --table TABLE_NAME).\nComplete the following tasks:\n\n(6 points) Show a table of ALL dogs and the treats with calories that they ate with column names dog, treat, calorie. A dog that did not eat any treats should still be present in the table.\n(6 points) Show a table with two columns dog and total_calories where only dogs that have eaten more than 500 calories are displayed."
  },
  {
    "objectID": "fall_24/b823/projects/downloads/b823_hw2.html",
    "href": "fall_24/b823/projects/downloads/b823_hw2.html",
    "title": "Question 1",
    "section": "",
    "text": "import sqlite3\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Set directory, db, and file paths\ndir_path = \"/content/drive/MyDrive/ColabNotebooks/homework_projects\"\ndb_path = dir_path + \"/chinook.db\"\nsql_file = dir_path + \"/Chinook_Sqlite.sql\"\n\n# Establish a SQLite3 connection\nconn = sqlite3.connect(db_path)\ncursor = conn.cursor()\n\n# Read the Chinook_SQLite.sql file\nwith open(sql_file, 'r') as file:\n  sql_script = file.read()\n\n# Execute the SQL script to create the tables and populate them\ncursor.executescript(sql_script)\n\n# Commit the changes and close the connection\nconn.commit()\nconn.close()\n\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n%load_ext sql\n%sql sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/b823_hw2.html#question-1",
    "href": "fall_24/b823/projects/downloads/b823_hw2.html#question-1",
    "title": "Question 1",
    "section": "Question 1",
    "text": "Question 1\nYou will be using the Chinook database to answer the following questions. This is a sample database available for various RDBMSs, including SQLite. The .db file for SQLite provided to you was created by executing the corresponding SQLite script.\nComplete the following tasks. You will need to connect to the Chinook.db database. (E.g., using %sql magic if you use Jupyter Notebook powered by DCC: use %load_ext sql to load the SQL extension, and then use %sql sqlite:////Chinook.db to connect with database Chinook.db.)\nRemark: When specific output requirements aren’t stated, you may choose to display all columns in your resulting tables or display only some of them (but you must include the primary key). For example, for question 4 showing customers from Germany, you can display all columns in table Customer, or only display selected columns (but you must include primary key CustomerId).\n\nPart 1\n(3 points) Find all tables in the database and display the first 3 rows of tables Customer, Employee, Invoice, and InvoiceLine with column headings. We will be primarily working on these four tables.\n\nOne nice feature of SQLite is that for any SQLite database, there is a table called sqlite_master that contains metadata relating to all other tables. In particular, there is a column called name that stores all table names in the data base.\nLet’s answer this question by first querying the name column of the sqlite_master table:\n\n\n%%sql\nSELECT name FROM sqlite_master;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nname\n\n\n\n\nAlbum\n\n\nArtist\n\n\nCustomer\n\n\nEmployee\n\n\nGenre\n\n\nInvoice\n\n\nInvoiceLine\n\n\nMediaType\n\n\nPlaylist\n\n\nPlaylistTrack\n\n\nsqlite_autoindex_PlaylistTrack_1\n\n\nTrack\n\n\nIFK_AlbumArtistId\n\n\nIFK_CustomerSupportRepId\n\n\nIFK_EmployeeReportsTo\n\n\nIFK_InvoiceCustomerId\n\n\nIFK_InvoiceLineInvoiceId\n\n\nIFK_InvoiceLineTrackId\n\n\nIFK_PlaylistTrackPlaylistId\n\n\nIFK_PlaylistTrackTrackId\n\n\nIFK_TrackAlbumId\n\n\nIFK_TrackGenreId\n\n\nIFK_TrackMediaTypeId\n\n\n\n\n\n\nThe problem states that we’re interested in the tables Customer, Employee, Invoice, and InvoiceLine. Let’s see if we can display the first three rows of each of these:\n\n\n%%sql\n-- Display the first three rows of the four tables of interest\nSELECT * FROM Customer LIMIT 3;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\nDone.\nDone.\nDone.\n\n\n\n\n\nInvoiceLineId\nInvoiceId\nTrackId\nUnitPrice\nQuantity\n\n\n\n\n1\n1\n2\n0.99\n1\n\n\n2\n1\n4\n0.99\n1\n\n\n3\n2\n6\n0.99\n1\n\n\n\n\n\n\n%sql SELECT * FROM Employee LIMIT 3;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nEmployeeId\nLastName\nFirstName\nTitle\nReportsTo\nBirthDate\nHireDate\nAddress\nCity\nState\nCountry\nPostalCode\nPhone\nFax\nEmail\n\n\n\n\n1\nAdams\nAndrew\nGeneral Manager\nNone\n1962-02-18 00:00:00\n2002-08-14 00:00:00\n11120 Jasper Ave NW\nEdmonton\nAB\nCanada\nT5K 2N1\n+1 (780) 428-9482\n+1 (780) 428-3457\nandrew@chinookcorp.com\n\n\n2\nEdwards\nNancy\nSales Manager\n1\n1958-12-08 00:00:00\n2002-05-01 00:00:00\n825 8 Ave SW\nCalgary\nAB\nCanada\nT2P 2T3\n+1 (403) 262-3443\n+1 (403) 262-3322\nnancy@chinookcorp.com\n\n\n3\nPeacock\nJane\nSales Support Agent\n2\n1973-08-29 00:00:00\n2002-04-01 00:00:00\n1111 6 Ave SW\nCalgary\nAB\nCanada\nT2P 5M5\n+1 (403) 262-3443\n+1 (403) 262-6712\njane@chinookcorp.com\n\n\n\n\n\n\n%sql SELECT * FROM Invoice LIMIT 3;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nInvoiceId\nCustomerId\nInvoiceDate\nBillingAddress\nBillingCity\nBillingState\nBillingCountry\nBillingPostalCode\nTotal\n\n\n\n\n1\n2\n2021-01-01 00:00:00\nTheodor-Heuss-Straße 34\nStuttgart\nNone\nGermany\n70174\n1.98\n\n\n2\n4\n2021-01-02 00:00:00\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n3.96\n\n\n3\n8\n2021-01-03 00:00:00\nGrétrystraat 63\nBrussels\nNone\nBelgium\n1000\n5.94\n\n\n\n\n\n\n%sql SELECT * FROM InvoiceLine LIMIT 3;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nInvoiceLineId\nInvoiceId\nTrackId\nUnitPrice\nQuantity\n\n\n\n\n1\n1\n2\n0.99\n1\n\n\n2\n1\n4\n0.99\n1\n\n\n3\n2\n6\n0.99\n1\n\n\n\n\n\n\n\nPart 2\n(2 points) Provide a query showing Customers (just full names, customer ID and country) who are not in the US. You only need to display the first 10 rows, and you don’t have to concatenate the customers’ first and last names.\n\n%%sql\n-- Display the first 10 rows of Customers outside of the USA\nSELECT CustomerID, FirstName, LastName, Country\nFROM Customer WHERE Country != 'USA' LIMIT 10;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nCustomerId\nFirstName\nLastName\nCountry\n\n\n\n\n1\nLuís\nGonçalves\nBrazil\n\n\n2\nLeonie\nKöhler\nGermany\n\n\n3\nFrançois\nTremblay\nCanada\n\n\n4\nBjørn\nHansen\nNorway\n\n\n5\nFrantišek\nWichterlová\nCzech Republic\n\n\n6\nHelena\nHolý\nCzech Republic\n\n\n7\nAstrid\nGruber\nAustria\n\n\n8\nDaan\nPeeters\nBelgium\n\n\n9\nKara\nNielsen\nDenmark\n\n\n10\nEduardo\nMartins\nBrazil\n\n\n\n\n\n\n\nPart 3\n(2 points) Provide a query showing a unique list of billing countries from the Invoice table.\n\n%%sql\n-- Display unique billing countries from the invoice table\nSELECT DISTINCT BillingCountry FROM Invoice;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nCOUNT(DISTINCT BillingCountry)\n\n\n\n\n24\n\n\n\n\n\n\n\nPart 4\n(2 points) Provide a query only showing the Customers from Germany.\n\n%%sql\n-- Display customers from Germany\nSELECT * FROM Customer WHERE Country = 'Germany';\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nCustomerId\nFirstName\nLastName\nCompany\nAddress\nCity\nState\nCountry\nPostalCode\nPhone\nFax\nEmail\nSupportRepId\n\n\n\n\n2\nLeonie\nKöhler\nNone\nTheodor-Heuss-Straße 34\nStuttgart\nNone\nGermany\n70174\n+49 0711 2842222\nNone\nleonekohler@surfeu.de\n5\n\n\n36\nHannah\nSchneider\nNone\nTauentzienstraße 8\nBerlin\nNone\nGermany\n10789\n+49 030 26550280\nNone\nhannah.schneider@yahoo.de\n5\n\n\n37\nFynn\nZimmermann\nNone\nBerger Straße 10\nFrankfurt\nNone\nGermany\n60316\n+49 069 40598889\nNone\nfzimmermann@yahoo.de\n3\n\n\n38\nNiklas\nSchröder\nNone\nBarbarossastraße 19\nBerlin\nNone\nGermany\n10779\n+49 030 2141444\nNone\nnschroder@surfeu.de\n3\n\n\n\n\n\n\n\nPart 5\n(2 points) Provide a query showing only the Employees who are Sales Agents (i.e., title = ‘Sales Support Agent’).\n\n%%sql\n-- Display employees with the title, 'Sales Support Agent'\nSELECT * FROM Employee WHERE Title = 'Sales Support Agent';\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nEmployeeId\nLastName\nFirstName\nTitle\nReportsTo\nBirthDate\nHireDate\nAddress\nCity\nState\nCountry\nPostalCode\nPhone\nFax\nEmail\n\n\n\n\n3\nPeacock\nJane\nSales Support Agent\n2\n1973-08-29 00:00:00\n2002-04-01 00:00:00\n1111 6 Ave SW\nCalgary\nAB\nCanada\nT2P 5M5\n+1 (403) 262-3443\n+1 (403) 262-6712\njane@chinookcorp.com\n\n\n4\nPark\nMargaret\nSales Support Agent\n2\n1947-09-19 00:00:00\n2003-05-03 00:00:00\n683 10 Street SW\nCalgary\nAB\nCanada\nT2P 5G3\n+1 (403) 263-4423\n+1 (403) 263-4289\nmargaret@chinookcorp.com\n\n\n5\nJohnson\nSteve\nSales Support Agent\n2\n1965-03-03 00:00:00\n2003-10-17 00:00:00\n7727B 41 Ave\nCalgary\nAB\nCanada\nT3B 1Y7\n1 (780) 836-9987\n1 (780) 836-9543\nsteve@chinookcorp.com\n\n\n\n\n\n\n\nPart 6\n(3 points) Provide a query showing the invoices of customers who are from Germany. The resultant table should show the customer’s full name, Invoice ID, Date of the invoice and billing country.\n\n%%sql\n-- Show the customer name, invoice id, invoice date and billing country for\n-- customers from Germany\nSELECT c.FirstName, c.LastName, i.InvoiceID, i.InvoiceDate, i.BillingCountry\nFROM Customer AS c\nINNER JOIN Invoice AS i using(CustomerID)\nWHERE c.Country = 'Germany';\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nFirstName\nLastName\nInvoiceId\nInvoiceDate\nBillingCountry\n\n\n\n\nLeonie\nKöhler\n1\n2021-01-01 00:00:00\nGermany\n\n\nLeonie\nKöhler\n12\n2021-02-11 00:00:00\nGermany\n\n\nLeonie\nKöhler\n67\n2021-10-12 00:00:00\nGermany\n\n\nLeonie\nKöhler\n196\n2023-05-19 00:00:00\nGermany\n\n\nLeonie\nKöhler\n219\n2023-08-21 00:00:00\nGermany\n\n\nLeonie\nKöhler\n241\n2023-11-23 00:00:00\nGermany\n\n\nLeonie\nKöhler\n293\n2024-07-13 00:00:00\nGermany\n\n\nHannah\nSchneider\n29\n2021-05-05 00:00:00\nGermany\n\n\nHannah\nSchneider\n40\n2021-06-15 00:00:00\nGermany\n\n\nHannah\nSchneider\n95\n2022-02-13 00:00:00\nGermany\n\n\nHannah\nSchneider\n224\n2023-09-20 00:00:00\nGermany\n\n\nHannah\nSchneider\n247\n2023-12-23 00:00:00\nGermany\n\n\nHannah\nSchneider\n269\n2024-03-26 00:00:00\nGermany\n\n\nHannah\nSchneider\n321\n2024-11-14 00:00:00\nGermany\n\n\nFynn\nZimmermann\n6\n2021-01-19 00:00:00\nGermany\n\n\nFynn\nZimmermann\n127\n2022-07-13 00:00:00\nGermany\n\n\nFynn\nZimmermann\n138\n2022-08-23 00:00:00\nGermany\n\n\nFynn\nZimmermann\n193\n2023-04-23 00:00:00\nGermany\n\n\nFynn\nZimmermann\n322\n2024-11-27 00:00:00\nGermany\n\n\nFynn\nZimmermann\n345\n2025-03-01 00:00:00\nGermany\n\n\nFynn\nZimmermann\n367\n2025-06-03 00:00:00\nGermany\n\n\nNiklas\nSchröder\n7\n2021-02-01 00:00:00\nGermany\n\n\nNiklas\nSchröder\n30\n2021-05-06 00:00:00\nGermany\n\n\nNiklas\nSchröder\n52\n2021-08-08 00:00:00\nGermany\n\n\nNiklas\nSchröder\n104\n2022-03-29 00:00:00\nGermany\n\n\nNiklas\nSchröder\n225\n2023-09-20 00:00:00\nGermany\n\n\nNiklas\nSchröder\n236\n2023-10-31 00:00:00\nGermany\n\n\nNiklas\nSchröder\n291\n2024-06-30 00:00:00\nGermany\n\n\n\n\n\n\n\nPart 7\n(3 points) Provide a query that shows the invoice Total, invoice date, Customer first and last name, and Country. Order by invoice date.\n\nThe resulting table is quite long, so I’m going to display the first 10 rows only.\n\n\n%%sql\n-- Show i.Total, i.BillingDate, c.FirstName, c.LastName, c.Country; order by\n-- i.BillingDate\nSELECT c.FirstName, c.LastName, i.Total, i.InvoiceDate, i.BillingCountry\nFROM Customer AS c\nINNER JOIN Invoice AS i using(CustomerId)\nORDER BY i.InvoiceDate LIMIT 10;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nFirstName\nLastName\nTotal\nInvoiceDate\nBillingCountry\n\n\n\n\nLeonie\nKöhler\n1.98\n2021-01-01 00:00:00\nGermany\n\n\nBjørn\nHansen\n3.96\n2021-01-02 00:00:00\nNorway\n\n\nDaan\nPeeters\n5.94\n2021-01-03 00:00:00\nBelgium\n\n\nMark\nPhilips\n8.91\n2021-01-06 00:00:00\nCanada\n\n\nJohn\nGordon\n13.86\n2021-01-11 00:00:00\nUSA\n\n\nFynn\nZimmermann\n0.99\n2021-01-19 00:00:00\nGermany\n\n\nNiklas\nSchröder\n1.98\n2021-02-01 00:00:00\nGermany\n\n\nDominique\nLefebvre\n1.98\n2021-02-01 00:00:00\nFrance\n\n\nWyatt\nGirard\n3.96\n2021-02-02 00:00:00\nFrance\n\n\nHugh\nO'Reilly\n5.94\n2021-02-03 00:00:00\nIreland\n\n\n\n\n\n\n\nPart 8\n(4 points) Provide a query that shows the invoices associated with each sales agent for customers from Germany. The resultant table should include the Sales Agent’s full name.\n\nI’m choosing to display the invoice ID, date, total, and country.\n\n\n%%sql\n-- Show invoices associated with sales agent for customers from Germany\nSELECT e.FirstName, e.LastName, i.InvoiceId, i.Total, i.InvoiceDate,  c.Country\nFROM Employee AS e\nINNER JOIN Customer AS c ON e.EmployeeId = c.SupportRepId\nINNER JOIN Invoice AS i using(CustomerId)\nWHERE c.Country = 'Germany';\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nFirstName\nLastName\nInvoiceId\nTotal\nInvoiceDate\nCountry\n\n\n\n\nSteve\nJohnson\n1\n1.98\n2021-01-01 00:00:00\nGermany\n\n\nSteve\nJohnson\n12\n13.86\n2021-02-11 00:00:00\nGermany\n\n\nSteve\nJohnson\n67\n8.91\n2021-10-12 00:00:00\nGermany\n\n\nSteve\nJohnson\n196\n1.98\n2023-05-19 00:00:00\nGermany\n\n\nSteve\nJohnson\n219\n3.96\n2023-08-21 00:00:00\nGermany\n\n\nSteve\nJohnson\n241\n5.94\n2023-11-23 00:00:00\nGermany\n\n\nSteve\nJohnson\n293\n0.99\n2024-07-13 00:00:00\nGermany\n\n\nSteve\nJohnson\n29\n1.98\n2021-05-05 00:00:00\nGermany\n\n\nSteve\nJohnson\n40\n13.86\n2021-06-15 00:00:00\nGermany\n\n\nSteve\nJohnson\n95\n8.91\n2022-02-13 00:00:00\nGermany\n\n\nSteve\nJohnson\n224\n1.98\n2023-09-20 00:00:00\nGermany\n\n\nSteve\nJohnson\n247\n3.96\n2023-12-23 00:00:00\nGermany\n\n\nSteve\nJohnson\n269\n5.94\n2024-03-26 00:00:00\nGermany\n\n\nSteve\nJohnson\n321\n0.99\n2024-11-14 00:00:00\nGermany\n\n\nJane\nPeacock\n6\n0.99\n2021-01-19 00:00:00\nGermany\n\n\nJane\nPeacock\n127\n1.98\n2022-07-13 00:00:00\nGermany\n\n\nJane\nPeacock\n138\n13.86\n2022-08-23 00:00:00\nGermany\n\n\nJane\nPeacock\n193\n14.91\n2023-04-23 00:00:00\nGermany\n\n\nJane\nPeacock\n322\n1.98\n2024-11-27 00:00:00\nGermany\n\n\nJane\nPeacock\n345\n3.96\n2025-03-01 00:00:00\nGermany\n\n\nJane\nPeacock\n367\n5.94\n2025-06-03 00:00:00\nGermany\n\n\nJane\nPeacock\n7\n1.98\n2021-02-01 00:00:00\nGermany\n\n\nJane\nPeacock\n30\n3.96\n2021-05-06 00:00:00\nGermany\n\n\nJane\nPeacock\n52\n5.94\n2021-08-08 00:00:00\nGermany\n\n\nJane\nPeacock\n104\n0.99\n2022-03-29 00:00:00\nGermany\n\n\nJane\nPeacock\n225\n1.98\n2023-09-20 00:00:00\nGermany\n\n\nJane\nPeacock\n236\n13.86\n2023-10-31 00:00:00\nGermany\n\n\nJane\nPeacock\n291\n8.91\n2024-06-30 00:00:00\nGermany\n\n\n\n\n\n\n\nPart 9\n(2 points) How many Invoices were there in 2021 and 2022? (Hint: the clause between datetime(‘2021-01-01 00:00:00’) and datetime(‘2022-12-31 00:00:00’) will help you filter out invoices during this time frame.)\n\n%%sql\n-- Find the total number of invoices between 2021 and 2022\nSELECT COUNT(*) AS 'Count of Invoices in 2021 and 2022'\nFROM Invoice WHERE InvoiceDate BETWEEN datetime('2021-01-01 00:00:00')\nAND datetime('2022-12-31 00:00:00');\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nCount of Invoices in 2021 and 2022\n\n\n\n\n166\n\n\n\n\n\n\n\nPart 10\n(2 points) Looking at the InvoiceLine table, provide a query that counts the number of line items for Invoice ID 37.\n\n%%sql\n-- Count the number of line items for Invoice ID 37\nSELECT COUNT(*) AS 'Number of Line Items for Invoice ID 37'\nFROM InvoiceLine WHERE InvoiceId = 37;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nNumber of Line Items for Invoice ID 37\n\n\n\n\n4\n\n\n\n\n\n\n\nPart 11\n(3 points) Looking at the InvoiceLine table, provide a query that counts the number of line items for each Invoice.\n\nOnce again, this is a long table. I’m going to limit the result to 10 rows.\n\n\n%%sql\n-- Count the number of line items for each invoice\nSELECT InvoiceId, COUNT(*) As 'Number of Line Items'\nFROM InvoiceLine GROUP BY InvoiceId LIMIT 10;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nInvoiceId\nNumber of Line Items\n\n\n\n\n1\n2\n\n\n2\n4\n\n\n3\n6\n\n\n4\n9\n\n\n5\n14\n\n\n6\n1\n\n\n7\n2\n\n\n8\n2\n\n\n9\n4\n\n\n10\n6\n\n\n\n\n\n\n\nPart 12\n(2 points) Provide a query that shows the number of invoices by (per) country.\n\n%%sql\n-- Count the number of invoices per country\nSELECT BillingCountry, COUNT(*) AS 'Number of Invoices'\nFROM Invoice GROUP BY BillingCountry;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nBillingCountry\nNumber of Invoices\n\n\n\n\nArgentina\n7\n\n\nAustralia\n7\n\n\nAustria\n7\n\n\nBelgium\n7\n\n\nBrazil\n35\n\n\nCanada\n56\n\n\nChile\n7\n\n\nCzech Republic\n14\n\n\nDenmark\n7\n\n\nFinland\n7\n\n\nFrance\n35\n\n\nGermany\n28\n\n\nHungary\n7\n\n\nIndia\n13\n\n\nIreland\n7\n\n\nItaly\n7\n\n\nNetherlands\n7\n\n\nNorway\n7\n\n\nPoland\n7\n\n\nPortugal\n14\n\n\nSpain\n7\n\n\nSweden\n7\n\n\nUSA\n91\n\n\nUnited Kingdom\n21\n\n\n\n\n\n\n\nPart 13\n(2 points) Provide a query that shows total sales made by each sales agent.\n\n%%sql\n-- Show the sales made by each agent\nSELECT e.EmployeeId, e.FirstName, e.LastName, ROUND(COALESCE(SUM(i.Total), 0))\nAS TotalSales\nFROM Employee AS e\nLEFT OUTER JOIN Customer AS c ON e.EmployeeId = c.SupportRepId\nLEFT OUTER JOIN Invoice AS i USING(CustomerID)\nGROUP BY e.EmployeeId\nORDER BY TotalSales DESC;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nEmployeeId\nFirstName\nLastName\nTotalSales\n\n\n\n\n3\nJane\nPeacock\n833.0\n\n\n4\nMargaret\nPark\n775.0\n\n\n5\nSteve\nJohnson\n720.0\n\n\n1\nAndrew\nAdams\n0.0\n\n\n2\nNancy\nEdwards\n0.0\n\n\n6\nMichael\nMitchell\n0.0\n\n\n7\nRobert\nKing\n0.0\n\n\n8\nLaura\nCallahan\n0.0\n\n\n\n\n\n\n\nPart 14\n(3 points) Which sales agent made the most in sales in 2021? (Hint: count the total sales in 2021 grouped by sales agent.)\n\n%%sql\n-- Determine which sales agent made the most sales in 2021\nSELECT ID, Firstname, LastName, TotalSales FROM (\n  SELECT e.EmployeeId AS ID, e.FirstName, e.LastName,\n  ROUND(COALESCE(SUM(i.Total), 0)) AS TotalSales\n  FROM Employee AS e\n  LEFT OUTER JOIN Customer AS c ON e.EmployeeId = c.SupportRepId\n  LEFT OUTER JOIN Invoice AS i USING(CustomerId)\n  WHERE i.InvoiceDate BETWEEN datetime('2021-01-01 00:00:00')\n  AND datetime('2021-12-31 00:00:00')\n  GROUP BY e.EmployeeId\n) ORDER BY TotalSales DESC LIMIT 1;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nID\nFirstName\nLastName\nTotalSales\n\n\n\n\n5\nSteve\nJohnson\n164.0\n\n\n\n\n\n\n\nPart 15\n(3 points) Provide a query that shows the total sales per country. Which country’s customers spent the most?\n\n%%sql\n-- Determine which country's customers spent the most\nSELECT BillingCountry, ROUND(COALESCE(SUM(Total), 0)) AS TotalSales\nFROM Invoice GROUP BY BillingCountry ORDER BY TotalSales DESC LIMIT 1;\n\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\nDone.\n\n\n\n\n\nBillingCountry\nTotalSales\n\n\n\n\nUSA\n523.0"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/b823_hw2.html#question-2",
    "href": "fall_24/b823/projects/downloads/b823_hw2.html#question-2",
    "title": "Question 1",
    "section": "Question 2",
    "text": "Question 2\nYou will be working with an SQLite3 database pets.db with 3 tables dog, treat and dog_treat. The dog_treat table is a linker table showing which dog ate which treat.\nNote: Although it would be best practice to always attach an E-R diagram (and potentially scripts to populate the database from scratch), in real scenarios this is often not the case. Instead, you may need to explore the tables and their columns using the tools afforded by the RDBMS or other exploratory data analysis environments. This problem emulates such a sitation – all we have at first is a database file. To explore, for example the sqlite3 command line tool has so-called dot-commands (such as .tables), and the Jupyter Lab SQL extension supports %sqlcmd commands for inspecting tables and columns, and one for exploring the content of tables (%sqlcmd explore –table TABLE_NAME).\nComplete the following tasks:\n\nPart 1\n(6 points) Show a table of ALL dogs and the treats with calories that they ate with column names dog, treat, calorie. A dog that did not eat any treats should still be present in the table.\n\n%sql sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/pets.db\n\n\n%%sql\n-- Find out the treats and calories each dog has consumed\nSELECT d.name AS dog, t.name AS treat, COALESCE(t.calories, 0) AS calories\nFROM dog AS d\nLEFT OUTER JOIN dog_treat USING(dog_id)\nLEFT OUTER JOIN treat AS t USING(treat_id)\nORDER BY dog, calories DESC;\n\n   sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/pets.db\nDone.\n\n\n\n\n\ndog\ntreat\ncalories\n\n\n\n\nFido\nBones\n500\n\n\nFido\nChewies\n100\n\n\nLassie\nNone\n0\n\n\nRoger\nBones\n500\n\n\nWoof\nBones\n500\n\n\nWoof\nRaw Meat\n450\n\n\n\n\n\n\n\nPart 2\n(6 points) Show a table with two columns dog and total_calories where only dogs that have eaten more than 500 calories are displayed.\n\n%%sql\nSELECT d.name AS dog, SUM(t.calories) AS total_calories\nFROM dog AS d\nLEFT OUTER JOIN dog_treat USING(dog_id)\nLEFT OUTER JOIN treat AS t USING(treat_id)\nGROUP BY d.dog_id HAVING total_calories &gt; 500\nORDER BY total_calories DESC;\n\n   sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/chinook.db\n * sqlite:////content/drive/MyDrive/ColabNotebooks/homework_projects/pets.db\nDone.\n\n\n\n\n\ndog\ntotal_calories\n\n\n\n\nWoof\n950\n\n\nFido\n600\n\n\n\n\n\n\n# Rendering the file to .html\n!jupyter nbconvert /content/drive/MyDrive/ColabNotebooks/homework_projects/b823_hw2.ipynb --to html\n\n[NbConvertApp] Converting notebook /content/drive/MyDrive/ColabNotebooks/homework_projects/b823_hw2.ipynb to html\n[NbConvertApp] Writing 684154 bytes to /content/drive/MyDrive/ColabNotebooks/homework_projects/b823_hw2.html"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html",
    "href": "fall_24/b823/notes/september/sep_10.html",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "Navigate to https://dcc-ondemand-01.oit.duke.edu\nLogin with Duke credentials when prompted\nSelect Interactive Apps from the menu bar\nSelect Jupyter Lab Apptainer\nSelect the number of hours estimated (probably a good idea to increase it by an hour)\nALWAYS increase the memory above the default of 2 (app will not launch properly)\nSelect path\n\n/opt/apps/containers/community/biostat/biostat-823-jupyter.sif\nHit Select once you’ve identified everything correctly\n\nHit Launch\n\n\n\n\n\nOne interesting note about joins is that we’re usually taking a step backwards from normalization. This is totally acceptable and appropriate. Sometimes we need to summarize data in a different or non-standard way\nInner Joins\n\nInner Joins are the default, but it’s always important to specify for clarity\nExample: SELECT c.Name AS Course, c.Semester, l.Name AS Lesson FROM Course AS c INNER JOIN Lesson AS l ON (c.Course_OID = l.Course_OID);\nNo null values (I believe… unless there were existing NULL values in the selected columns before the join?)\nIf the column names are the same (as is the case in the example with Course_OID in both tables), we can shorten it to USING(...)\n\nOuter Joins\n\nSpecify whether it’s a left outer join or a right outer join\nKeeps all rows in the specified table (left or right), but it drops any rows in the adjacent table that do not match rows in the “outer” table\nExample: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all courses are kept whether or not they have a Room (think online class), but any room that does NOT have a course is dropped (i.e. the kitchen probably doesn’t make it onto the output table)\n\nExample: SELECT DISTINCT i.Name AS Instructor, c.Name AS Course, c.Semester, r.Name AS Room FROM Instructor AS i LEFT OUTER JOIN Lesson AS l USING (Instructor_OID) LEFT OUTER JOIN Course AS c USING (Course_OID) LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all instructors are listed; only courses taught by instructors are listed, but not every instructor has a course; next, only rooms that have a course are listed, but not every course has a room\n\n\nExistential subquery vs join\n\nSome queries that can e answered joining related facts may be easier to define and faster to execute by querying for existence of a fact. For example, which instructors are teaching in a given semester:\n\nSELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID)\nI’ll be honest, this is the most challenging bit of code for me in this lecture. I’m going to try to break it down a little (for my own sanity)\n\nFirst, we want to select an instructor name and email from the instructor table under a given condition. That’s simple enough\nThe given condition is that something exists\nThe ‘something’ that exists is that we are able to select 1 (the number) for every row in an inner-joined table\nThat inner-joined table is Lesson and Course joined on Course_OID, but only where the semester is Fall 2023. Thus, only lessons taught in courses from fall 2023 will appear in this joined table\nOh. One more condition. AND the lesson instructor is the same as the instructor we’re querying? I don’t totally understand this\nAnyhoot, if these conditions are met, there is a 1 selected from this table for every row, and if at least one row ‘exists’, then that instructor taught a course in fall 2023, and his or her name and email address will appear on the output table\n\n\n\n\n\n\n\n\nWhat is the address for the DCC On-Demand website?\nLook at this code: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID). Assume neither Course nor Room have any null values. What is the name of the column that could have null values in the output table?\nThe following code uses existential subquerying, which can be more efficient than a join if your goal is to check that a condition is met: SELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID). 1. What are the two columns in the output table? 2. Which professors will be displayed in the output table? Only those who had a BLANK during BLANK BLANK"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html#logging-into-the-jupyter-apptainer",
    "href": "fall_24/b823/notes/september/sep_10.html#logging-into-the-jupyter-apptainer",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "Navigate to https://dcc-ondemand-01.oit.duke.edu\nLogin with Duke credentials when prompted\nSelect Interactive Apps from the menu bar\nSelect Jupyter Lab Apptainer\nSelect the number of hours estimated (probably a good idea to increase it by an hour)\nALWAYS increase the memory above the default of 2 (app will not launch properly)\nSelect path\n\n/opt/apps/containers/community/biostat/biostat-823-jupyter.sif\nHit Select once you’ve identified everything correctly\n\nHit Launch"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html#joining-tables",
    "href": "fall_24/b823/notes/september/sep_10.html#joining-tables",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "One interesting note about joins is that we’re usually taking a step backwards from normalization. This is totally acceptable and appropriate. Sometimes we need to summarize data in a different or non-standard way\nInner Joins\n\nInner Joins are the default, but it’s always important to specify for clarity\nExample: SELECT c.Name AS Course, c.Semester, l.Name AS Lesson FROM Course AS c INNER JOIN Lesson AS l ON (c.Course_OID = l.Course_OID);\nNo null values (I believe… unless there were existing NULL values in the selected columns before the join?)\nIf the column names are the same (as is the case in the example with Course_OID in both tables), we can shorten it to USING(...)\n\nOuter Joins\n\nSpecify whether it’s a left outer join or a right outer join\nKeeps all rows in the specified table (left or right), but it drops any rows in the adjacent table that do not match rows in the “outer” table\nExample: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all courses are kept whether or not they have a Room (think online class), but any room that does NOT have a course is dropped (i.e. the kitchen probably doesn’t make it onto the output table)\n\nExample: SELECT DISTINCT i.Name AS Instructor, c.Name AS Course, c.Semester, r.Name AS Room FROM Instructor AS i LEFT OUTER JOIN Lesson AS l USING (Instructor_OID) LEFT OUTER JOIN Course AS c USING (Course_OID) LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all instructors are listed; only courses taught by instructors are listed, but not every instructor has a course; next, only rooms that have a course are listed, but not every course has a room\n\n\nExistential subquery vs join\n\nSome queries that can e answered joining related facts may be easier to define and faster to execute by querying for existence of a fact. For example, which instructors are teaching in a given semester:\n\nSELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID)\nI’ll be honest, this is the most challenging bit of code for me in this lecture. I’m going to try to break it down a little (for my own sanity)\n\nFirst, we want to select an instructor name and email from the instructor table under a given condition. That’s simple enough\nThe given condition is that something exists\nThe ‘something’ that exists is that we are able to select 1 (the number) for every row in an inner-joined table\nThat inner-joined table is Lesson and Course joined on Course_OID, but only where the semester is Fall 2023. Thus, only lessons taught in courses from fall 2023 will appear in this joined table\nOh. One more condition. AND the lesson instructor is the same as the instructor we’re querying? I don’t totally understand this\nAnyhoot, if these conditions are met, there is a 1 selected from this table for every row, and if at least one row ‘exists’, then that instructor taught a course in fall 2023, and his or her name and email address will appear on the output table"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html#three-take-aways",
    "href": "fall_24/b823/notes/september/sep_10.html#three-take-aways",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "What is the address for the DCC On-Demand website?\nLook at this code: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID). Assume neither Course nor Room have any null values. What is the name of the column that could have null values in the output table?\nThe following code uses existential subquerying, which can be more efficient than a join if your goal is to check that a condition is met: SELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID). 1. What are the two columns in the output table? 2. Which professors will be displayed in the output table? Only those who had a BLANK during BLANK BLANK"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html#back",
    "href": "fall_24/b823/notes/september/sep_10.html#back",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "Back",
    "text": "Back"
  },
  {
    "objectID": "fall_24/b823/index.html",
    "href": "fall_24/b823/index.html",
    "title": "Statistical Programming for Big Data",
    "section": "",
    "text": "September\n\n\nOctober\n\n\nNovember\nTo review notes taken during class, follow this link."
  },
  {
    "objectID": "fall_24/b719/projects/hw_3.html",
    "href": "fall_24/b719/projects/hw_3.html",
    "title": "BIOSTAT 719 - Homework 3",
    "section": "",
    "text": "As in Homework 1, consider 150 observations \\(Y_i, i = 1, . . . , 150\\), from the exponential distribution:\n\\[\nf(y_i; \\lambda) = \\lambda e^{-\\lambda y_i}, \\quad y_i &gt; 0, \\lambda &gt; 0\n\\]\nwith the sum of these 150 observations equal to 30.\nPart (a): Starting from \\(\\lambda_{\\text{start}} = 1\\) obtain the next iteration of the Newton-Raphson algorithm. Show details of your work. Do NOT use computer.\n\nRecall that the score function for a set of independent exponential random variables is \\(U = \\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i\\). Thus, \\(U' = -\\frac{n}{\\lambda^2}\\). We also know that \\(\\lambda_1 = \\lambda_{\\text{start}} - \\frac{U_{\\text{start}}}{U'_{\\text{start}}}\\). Plugging and chugging, we can see:\n\\[\\begin{align*}\n\\lambda_1 &= \\lambda_{\\text{start}} - \\frac{U_{\\text{start}}}{U'_{\\text{start}}}\\\\\n&= 1 - \\frac{\\frac{n}{1} - \\sum y_i}{-\\frac{n}{1^2}}\\\\\n&= 1 - \\frac{150 - 30}{-150}\\\\\n&= 1.8\n\\end{align*}\\]\nHere we have the after one iteration of the Newton-Raphson algoritm, \\(\\lambda_1 = 1.8\\).\n\nPart (b): Starting from \\(\\lambda_{\\text{start}} = 1\\) obtain the next iteration of the Method of Scoring algorithm. Show details of your work. Do NOT use computer.\n\nThe formula for the method of score algorithm is this: \\(\\lambda_1 = \\lambda_{\\text{start}} + \\frac{U_{\\text{start}}}{I(\\lambda)_{\\text{start}}}\\). This being the case, we need to find the Fisher’s information for \\(\\lambda\\). I’m going to use the fact that the exponential distribution belongs to the exponential family to help me solve this:\n\\[\\begin{align*}\nI(\\lambda_i) &= \\frac{b''(\\lambda_i)c'(\\lambda_i)}{b'(\\lambda_i)} - c''(\\lambda_i)\\\\\nb(\\lambda_i) &= -\\lambda_i\\\\\nb'(\\lambda_i) &= -1\\\\\nb''(\\lambda_i) &= 0\\\\\nc(\\lambda_i) &= log(\\lambda_i)\\\\\nc'(\\lambda_i) &= \\frac{1}{\\lambda_i}\\\\\nc''(\\lambda_i) &= -\\frac{1}{\\lambda_i^2}\\\\\nI(\\lambda_i) &= \\frac{b''(\\lambda_i)c'(\\lambda_i)}{b'(\\lambda_i)} - c''(\\lambda_i)\\\\\n&= \\frac{(0)(1/\\lambda_i)}{-1} - \\left(-\\frac{1}{\\lambda_i^2}\\right)\\\\\n&= \\frac{1}{\\lambda_i^2}\\\\\n\\end{align*}\\]\nBecause each \\(y_i\\) is i.i.d., \\(\\lambda_i\\) is equivalent and we can add the information for each \\(\\lambda_i\\) quite easily:\n\\[\\begin{align*}\nI(\\lambda_{\\text{start}}) &= \\sum_{i = 1}^n I(\\lambda_i)\\\\\n&= \\sum_{i = 1}^n \\frac{1}{\\lambda^2}\\\\\n&= \\sum_{i = 1}^n \\frac{1}{1^2}\\\\\n&= n\n\\end{align*}\\]\nWe know from the previous example that \\(U = \\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i\\). Thus,\n\\[\\begin{align*}\n\\lambda_1 &= \\lambda_{start} + \\frac{U}{I(\\lambda_{\\text{start}})}\\\\\n&= \\lambda_{start} + \\frac{\\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i}{n}\\\\\n&= 1 + \\frac{\\frac{150}{1} - 30}{150}\\\\\n&= 1 + 0.8\\\\\n&= 1.8\n\\end{align*}\\]\n\nPart (c): Are your answers in (1) and (2) different? Explain why.\n\nThey are the same! This is because the exponential distribution belongs to the exponential family and has the canonical form, so \\(I(\\lambda) = -U'(\\lambda)\\). Thus, each will evaluate to the same value."
  },
  {
    "objectID": "fall_24/b719/projects/hw_3.html#problem-1",
    "href": "fall_24/b719/projects/hw_3.html#problem-1",
    "title": "BIOSTAT 719 - Homework 3",
    "section": "",
    "text": "As in Homework 1, consider 150 observations \\(Y_i, i = 1, . . . , 150\\), from the exponential distribution:\n\\[\nf(y_i; \\lambda) = \\lambda e^{-\\lambda y_i}, \\quad y_i &gt; 0, \\lambda &gt; 0\n\\]\nwith the sum of these 150 observations equal to 30.\nPart (a): Starting from \\(\\lambda_{\\text{start}} = 1\\) obtain the next iteration of the Newton-Raphson algorithm. Show details of your work. Do NOT use computer.\n\nRecall that the score function for a set of independent exponential random variables is \\(U = \\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i\\). Thus, \\(U' = -\\frac{n}{\\lambda^2}\\). We also know that \\(\\lambda_1 = \\lambda_{\\text{start}} - \\frac{U_{\\text{start}}}{U'_{\\text{start}}}\\). Plugging and chugging, we can see:\n\\[\\begin{align*}\n\\lambda_1 &= \\lambda_{\\text{start}} - \\frac{U_{\\text{start}}}{U'_{\\text{start}}}\\\\\n&= 1 - \\frac{\\frac{n}{1} - \\sum y_i}{-\\frac{n}{1^2}}\\\\\n&= 1 - \\frac{150 - 30}{-150}\\\\\n&= 1.8\n\\end{align*}\\]\nHere we have the after one iteration of the Newton-Raphson algoritm, \\(\\lambda_1 = 1.8\\).\n\nPart (b): Starting from \\(\\lambda_{\\text{start}} = 1\\) obtain the next iteration of the Method of Scoring algorithm. Show details of your work. Do NOT use computer.\n\nThe formula for the method of score algorithm is this: \\(\\lambda_1 = \\lambda_{\\text{start}} + \\frac{U_{\\text{start}}}{I(\\lambda)_{\\text{start}}}\\). This being the case, we need to find the Fisher’s information for \\(\\lambda\\). I’m going to use the fact that the exponential distribution belongs to the exponential family to help me solve this:\n\\[\\begin{align*}\nI(\\lambda_i) &= \\frac{b''(\\lambda_i)c'(\\lambda_i)}{b'(\\lambda_i)} - c''(\\lambda_i)\\\\\nb(\\lambda_i) &= -\\lambda_i\\\\\nb'(\\lambda_i) &= -1\\\\\nb''(\\lambda_i) &= 0\\\\\nc(\\lambda_i) &= log(\\lambda_i)\\\\\nc'(\\lambda_i) &= \\frac{1}{\\lambda_i}\\\\\nc''(\\lambda_i) &= -\\frac{1}{\\lambda_i^2}\\\\\nI(\\lambda_i) &= \\frac{b''(\\lambda_i)c'(\\lambda_i)}{b'(\\lambda_i)} - c''(\\lambda_i)\\\\\n&= \\frac{(0)(1/\\lambda_i)}{-1} - \\left(-\\frac{1}{\\lambda_i^2}\\right)\\\\\n&= \\frac{1}{\\lambda_i^2}\\\\\n\\end{align*}\\]\nBecause each \\(y_i\\) is i.i.d., \\(\\lambda_i\\) is equivalent and we can add the information for each \\(\\lambda_i\\) quite easily:\n\\[\\begin{align*}\nI(\\lambda_{\\text{start}}) &= \\sum_{i = 1}^n I(\\lambda_i)\\\\\n&= \\sum_{i = 1}^n \\frac{1}{\\lambda^2}\\\\\n&= \\sum_{i = 1}^n \\frac{1}{1^2}\\\\\n&= n\n\\end{align*}\\]\nWe know from the previous example that \\(U = \\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i\\). Thus,\n\\[\\begin{align*}\n\\lambda_1 &= \\lambda_{start} + \\frac{U}{I(\\lambda_{\\text{start}})}\\\\\n&= \\lambda_{start} + \\frac{\\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i}{n}\\\\\n&= 1 + \\frac{\\frac{150}{1} - 30}{150}\\\\\n&= 1 + 0.8\\\\\n&= 1.8\n\\end{align*}\\]\n\nPart (c): Are your answers in (1) and (2) different? Explain why.\n\nThey are the same! This is because the exponential distribution belongs to the exponential family and has the canonical form, so \\(I(\\lambda) = -U'(\\lambda)\\). Thus, each will evaluate to the same value."
  },
  {
    "objectID": "fall_24/b719/projects/hw_3.html#problem-2",
    "href": "fall_24/b719/projects/hw_3.html#problem-2",
    "title": "BIOSTAT 719 - Homework 3",
    "section": "Problem 2",
    "text": "Problem 2\nSuppose \\(Y_1, . . . , Y_n\\) are independent random variables following the normal distribution \\(Y_i \\sim N(\\log(\\beta), \\sigma^2)\\), where \\(\\sigma^2\\) is known. Derive matrix based formula for an iteratively re-weighted least squares (IRWLS) procedure for ML estimation of \\(\\beta\\).\n[Hint: first, present matrices X and W, and vector for the formula \\(b^{(m)} = \\left( X^TW^{(m - 1)}X \\right)^{-1}X^TW^{(m - 1)}z^{(m-1)}\\), and then simplify this formula as feasible.]\n\nThe first thing that I will note is that \\(\\mu = \\log(\\beta)\\). This implies two things. First, \\(\\log(\\beta) = \\log(X^T\\beta)\\), which implies that \\(X_{n\\times 1} = \\left[\\begin{matrix} 1\\\\1\\\\...\\\\1 \\end{matrix}\\right]\\), and that \\(\\beta_{1\\times 1} = \\left[\\begin{matrix} \\beta_0 \\end{matrix}\\right]\\). The second implication is that because \\(\\mu = \\log(\\beta)\\), \\(g(\\mu) = \\exp(\\beta)\\).\nLet’s now calculate \\(w_i\\) for the diagonal matrix \\(W\\):\n\\[\\begin{align*}\nw_i &= \\frac{1}{Var(Y_i)}\\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2\\\\\n&= \\frac{1}{\\sigma^2} \\left(\\frac{1}{\\beta} \\right)^2\\\\\n\\begin{bmatrix}\n\\frac{1}{\\sigma^2\\beta^2} & 0 & \\cdots & 0\\\\\n0 & \\frac{1}{\\sigma^2\\beta^2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\frac{1}{\\sigma^2\\beta^2}\n\\end{bmatrix}\n\\end{align*}\\]\nThe next thing to calculate is the \\(z\\) vector. Recall that \\(z_i = \\sum_{k = 1}^p x_{ik}b_i^{(m -1)} + (y_i - \\mu_i)\\left(\\frac{\\partial  \\eta_i}{\\partial \\mu_i} \\right)\\). Note that because we only have a single parameter, and because the \\(X\\) matrix is a \\(n\\times 1\\) matrix of \\(1\\)’s, \\(\\sum_{k = 1}^p x_{ik}b_i^{(m -1)}\\) simplifies to \\(b^{(m - 1)}\\) (denoted below as \\(\\beta\\)).\n\\[\\begin{align*}\nz_i &= \\beta + (y_i - \\mu_i) \\frac{\\partial\\eta_i}{\\partial \\mu_i}\\\\\n&= \\beta + [y_i - \\log(\\beta)]\\beta\\\\\n\\implies z &= \\beta + Y\\beta - \\log(\\beta)\\beta\\\\\n&= \\beta\\left(1 + Y - \\log(\\beta)\\right)\n\\end{align*}\\]\nThis gives rise to the following matrix based formula for an teratively re-weighted procedure for ML estimation of \\(\\beta\\):\n\\[\\begin{align*}\nb^{(m)} &= \\left( X^TW^{(m - 1)}X \\right)^{-1}X^TW^{(m - 1)}z^{(m-1)}\\\\\n&= \\left(\n    \\begin{bmatrix}1 \\cdots 1\\end{bmatrix}\\frac{1}{\\beta^2\\sigma^2}I\n    \\begin{bmatrix}1\\\\ \\vdots \\\\1\\end{bmatrix}\n    \\right)^{-1}\n    \\begin{bmatrix}1 \\cdots 1\\end{bmatrix} \\frac{1}{\\beta^2\\sigma^2}I \\beta\\left(1 + Y - \\log(\\beta)\\right)\\\\\n&= \\left(\\frac{n}{\\beta^2\\sigma^2}\\right)^{-1}\\frac{1}{\\beta^2\\sigma^2}\\begin{bmatrix}1 \\cdots 1\\end{bmatrix} \\beta\\left(1 + Y - \\log(\\beta)\\right)\\\\\n&= \\frac{1}{n}\\begin{bmatrix}\\beta\\cdots\\beta\\end{bmatrix}(1 + Y - \\log(\\beta))\n\\end{align*}\\]"
  },
  {
    "objectID": "fall_24/b719/projects/hw_3.html#problem-3",
    "href": "fall_24/b719/projects/hw_3.html#problem-3",
    "title": "BIOSTAT 719 - Homework 3",
    "section": "Problem 3",
    "text": "Problem 3\nConsider Table 4.3 [Textbook page 67] discussed in class. We now use a log link function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyi\n2\n3\n6\n7\n8\n9\n10\n12\n15\n\n\nxi\n-1\n-1\n0\n0\n0\n0\n1\n1\n1\n\n\n\n\n\nPart (a): Fit the model \\(\\log(E(Y_i)) = \\beta_0 + \\beta_1x_i\\) with the GLM function in R.\n\nBelow is the fitted model and the code used to obtain it:\n\n\n###################################### R ######################################\n# Set the data\nyi &lt;- c(2, 3, 6, 7, 8, 9, 10, 12, 15)\nxi &lt;- c(-1, -1, 0, 0, 0, 0, 1, 1, 1)\n\n# Fit the model \npoisson_model &lt;- glm(yi ~ xi, family = poisson(link = \"log\"))\n\n# Print the model and point estimates \ntable_title &lt;- \"Model: E(Y) = beta0 + beta1 X\"\nb_0 &lt;- round(poisson_model$coefficients[1], 3)\nb_1 &lt;- round(poisson_model$coefficients[2], 3)\nmodel_table &lt;- gt::gt(\n    data.frame(Parameter = c(\"beta0\", \"beta1\"), Estimate = c(b_0, b_1))\n)  %&gt;% \n    gt::tab_header(title = table_title)\nmodel_table\n\n\n\n\n\n\n\nModel: E(Y) = beta0 + beta1 X\n\n\nParameter\nEstimate\n\n\n\n\nbeta0\n1.889\n\n\nbeta1\n0.670\n\n\n\n\n\n\n###############################################################################\n\nPart (b): We want to obtain:\n\nMaximum likelihood estimates for \\(\\beta_0\\) and \\(\\beta_1\\)\nVariance-covariance matrix of the MLE for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nWrite an iteratively reweighted least squares R function similar to the one covered in class but accounting for the log link. In addition, provide all the details of the algebraic derivations.\n\nLet’s first derive the important terms:\n\n\\(n = 9\\)\n\\(p = 2\\)\n\\(\\beta = \\begin{bmatrix} \\beta_0\\\\ \\beta_1\\end{bmatrix}\\)\n\\(X^T = \\begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\-1 & -1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\end{bmatrix}\\)\n\\(\\log(\\mu) = \\eta \\implies \\mu = e^{\\eta}\\)\n\\(\\frac{\\partial \\eta}{\\partial\\mu} = \\frac{1}{e^{\\eta}}\\)\n\\(\\frac{\\partial \\mu}{\\partial\\eta} = e^{\\eta}\\)\n\\(w_{ii} = \\frac{1}{Var(Y_i)}\\left(\\frac{\\partial \\mu}{\\partial\\eta}\\right)^2 = \\frac{\\exp[(2)(\\beta_0 + \\beta_1x_i)]}{(\\beta_0 + \\beta_1x_i)}\\)\n\\(z_i = \\eta_i + (y_i - e^{\\eta_i})e^{-\\eta_i} = \\beta_0 + \\beta_1 x_i + \\frac{y_i}{ e^{\\beta_0 + \\beta_1 x_i}} - 1\\)\n\\(I = X^TWX = \\begin{bmatrix} \\sum_{i = 1}^n w_{ii} & \\sum_{i = 1}^n w_{ii} x_{i} \\\\\\sum_{i = 1}^n w_{ii}x_i & \\sum_{i = 1}^n w_{ii}x_i^2\\end{bmatrix}\\)\n\\(X^T Wz = \\begin{bmatrix} \\sum_{i = 1}^n w_iz_i\\\\ \\sum_{i = 1}^n w_ix_iz_i \\end{bmatrix}\\)\n\nWith this setup, we can implement our algorithm in R. I’m going to use the same initial values of 7 and 5 used in class for \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\n\n####################################### R #####################################\n# Define function to find w_ii \nw &lt;- function(y_i, x_i, b_0, b_1) {\n    return(exp((b_0 + b_1*x_i)))\n}\n# Define function to find z_i \nz &lt;- function(y_i, x_i, b_0, b_1) {\n    return(b_0 + b_1*x_i + y_i/(exp(b_0 + b_1*x_i)) - 1)\n\n}\n\n# Define function to estimate b_0 and b_1 \nmle_estimation &lt;- function(y_i, x_i, w, z, b_0_init, b_1_init, n_iterations) {\n    # y_i: vector of outcome variable \n    # x_i: vector of 1 predictor (how could we generalize this?)\n    # w: function to find w_ii from y_i, x_i, b_0, and b_1 \n    # z: function to find z_i from y_i, x_i, b_0, and b_1 \n    # b_0_init: initial value for b_0 estimate used in first iteration \n    # b_1_init: initial value for b_1 estimate used in first iteration \n    # n_iterations: number of cycles to run (could it be more useful to use \n    # while loop instead of for loop?)\n\n    # Define vector of estimates \n    b &lt;- c(b_0_init, b_1_init)\n\n    # Define matrix to be returned \n    stored_estimates &lt;- matrix(numeric(n_iterations*2), ncol = 2)\n\n    # Perform IRWLS procedure to estimate the MLEs for \\beta_0 and \\beta_1\n    for (i in 1:n_iterations) {\n        b_0 &lt;- b[1]\n        b_1 &lt;- b[2]\n        x_matrix &lt;- cbind(rep(1, length(x_i)), x_i)\n        # Find w_ii and z_i \n        w_ii &lt;- w(y_i, x_i, b_0, b_1)\n        z_i  &lt;- z(y_i, x_i, b_0, b_1)\n        # For readability, think of (X^T W X)^-1 as matrix_a and (X^T W z) as \n        # matrix_b\n        matrix_a &lt;- solve(t(x_matrix) %*% diag(w_ii) %*% x_matrix)\n        matrix_b &lt;- t(x_matrix) %*% diag(w_ii) %*% z_i\n        b &lt;- matrix_a %*% matrix_b \n        stored_estimates[i, ] &lt;- b\n    }\n    return(stored_estimates)\n}\n\n# Record data for analysis \ny_i &lt;-  c(2, 3, 6, 7, 8, 9, 10, 12, 15)\nx_i &lt;- c(-1, -1, 0, 0, 0, 0, 1, 1, 1)\n\n# Set initial values for b_0 and b_1 \nb_0_init &lt;- 5\nb_1_init &lt;- 7 \n\n# Use IRWLS to estimate b_0 and b_1 \nb &lt;- mle_estimation(y_i, x_i, w, z, b_0_init, b_1_init, 15)\n\n# Print results in a pretty table \nb_table &lt;- as.data.frame(b)  %&gt;% \n    mutate(\n        Iteration = 1:15,\n        Beta0 = V1,\n        Beta1 = V2\n    )  %&gt;% \n    select(Iteration, Beta0, Beta1)  %&gt;% \n    gt::gt()  %&gt;% \n    gt::tab_header(\n        title = \"Results: Estimates for Beta0 and Beta1 in IRWLS Procedure\"\n    )\nb_table\n\n\n\n\n\n\n\nResults: Estimates for Beta0 and Beta1 in IRWLS Procedure\n\n\nIteration\nBeta0\nBeta1\n\n\n\n\n1\n4.067257\n6.9328087\n\n\n2\n3.238167\n6.7620773\n\n\n3\n2.629619\n6.3711097\n\n\n4\n2.348223\n5.6538244\n\n\n5\n2.296974\n4.7086591\n\n\n6\n2.284874\n3.7305386\n\n\n7\n2.256773\n2.7854100\n\n\n8\n2.192685\n1.9229649\n\n\n9\n2.078582\n1.2335379\n\n\n10\n1.954098\n0.8196265\n\n\n11\n1.895948\n0.6832292\n\n\n12\n1.889336\n0.6699088\n\n\n13\n1.889272\n0.6697856\n\n\n14\n1.889272\n0.6697856\n\n\n15\n1.889272\n0.6697856\n\n\n\n\n\n\n###############################################################################\n\n\nIt looks like we converge at iteration 13. Not too bad! Our MLE estimates are 1.889 for \\(\\beta_0\\) and 0.670 for \\(\\beta_1\\).\nLet’s now get the Information matrix to solve for the variance-covariance matrix.\n\n\n###################################### R ######################################\n# Recall that I = X^T W X \n\n# Set parameters and calculate w_ii \nb_0 &lt;- 1.889272\nb_1 &lt;- 0.6697856 \nx_matrix &lt;- cbind(rep(1, 9), x_i)\nw_ii &lt;- w(y_i, x_i, b_0, b_1)\n \n# Calculate I \ni &lt;- t(x_matrix) %*% diag(w_ii) %*% x_matrix \n# Calculate variance-covariance matrix \nvar_cov_matrix &lt;- solve(i)\nrownames(var_cov_matrix) &lt;- c(\"Beta0\", \"Beta1\")\ncolnames(var_cov_matrix) &lt;- c(\"Beta0\", \"Beta1\")\nvar_cov_df &lt;- as.data.frame(var_cov_matrix)  %&gt;% \n    tibble::rownames_to_column(var = \"Parameter\")\n\n# Print table \nvar_cov_table &lt;- var_cov_df  %&gt;% \n    gt::gt()  %&gt;% \n    gt::tab_header(title = \"Variance-Covariance Matrix\")\nvar_cov_table\n\n\n\n\n\n\n\nVariance-Covariance Matrix\n\n\nParameter\nBeta0\nBeta1\n\n\n\n\nBeta0\n0.02019584\n-0.01419063\n\n\nBeta1\n-0.01419063\n0.03192892\n\n\n\n\n\n\n###############################################################################\n\nPart (c): Compare values obtained in (a) and (b).\n\nLet’s print the tables from our model in part a:\n\n\n###################################### R ######################################\n# Create a table for the var_cov_matrix\nmodel_var_cov_matrix &lt;- summary(poisson_model)$cov.unscaled\nrownames(model_var_cov_matrix) &lt;- c(\"Beta0\", \"Beta1\")\ncolnames(model_var_cov_matrix) &lt;- c(\"Beta0\", \"Beta1\")\nmodel_var_cov_df &lt;- as.data.frame(model_var_cov_matrix)  %&gt;% \n    tibble::rownames_to_column(var = \"Parameter\") \nmodel_var_cov_table &lt;- model_var_cov_df  %&gt;% \n    gt::gt()  %&gt;% \n    gt::tab_header(title = \"Variance-Covariance Matrix\")\n# Print both tables \nmodel_table\n\n\n\n\n\n\n\nModel: E(Y) = beta0 + beta1 X\n\n\nParameter\nEstimate\n\n\n\n\nbeta0\n1.889\n\n\nbeta1\n0.670\n\n\n\n\n\n\nmodel_var_cov_table\n\n\n\n\n\n\n\nVariance-Covariance Matrix\n\n\nParameter\nBeta0\nBeta1\n\n\n\n\nBeta0\n0.02019583\n-0.01419063\n\n\nBeta1\n-0.01419063\n0.03192891\n\n\n\n\n\n\n###############################################################################\n\n\nWe are pretty much spot on! That’s amazing. Both our estimates match (rounded to three decimals), and the same is true of the variance-covariance matrix. All in a day’s work."
  },
  {
    "objectID": "fall_24/b719/projects/hw_1.html",
    "href": "fall_24/b719/projects/hw_1.html",
    "title": "BIOSTAT 719 - Homework 1",
    "section": "",
    "text": "(6 points) [Textbook Exercise 1.6]\n\nThe data in Table 1.4 are the numbers of females and males in the progeny of 16 female light brown apple moths in Muswellbrook, New South Wales, Australia (from Lewis 1987).\n\n\n\n\n\n\n\n\n\n\n\nProgeny Group\nFemales\nMales\n\n\n\n\n1\n18\n11\n\n\n2\n31\n22\n\n\n3\n34\n27\n\n\n4\n33\n29\n\n\n5\n27\n24\n\n\n6\n33\n29\n\n\n7\n28\n25\n\n\n8\n23\n26\n\n\n9\n33\n38\n\n\n10\n12\n14\n\n\n11\n19\n23\n\n\n12\n25\n31\n\n\n13\n14\n20\n\n\n14\n4\n6\n\n\n15\n22\n34\n\n\n16\n7\n12\n\n\n\n\n\n\nLet \\(Y_i\\) denote the number of females in each of the 16 groups of progeny in each group (\\(i = 1, ..., 16\\)). Suppose the \\(Y_i\\)’s are independent random variables each with the Binomial distribution\n\n\\[\nf(y_i; \\theta) = {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}\n\\]\n\nFind the maximum likelihood estimator (MLE) of \\(\\theta\\) using calculus and evaluate it for these data.\n\nLet’s first find the MLE using calculus. Let’s review the steps:\n\nCalculate the log-likelihood function\nTake the first derivative of the log-likelihood function (this is the ‘score’ function)\nSet the score function equal to 0 and solve for \\(\\hat{\\theta}\\)\nTake the second derivative of the function to determine whether the function is concave down (i.e. has a maximum value at \\(\\hat{\\theta}\\))\n\nWith that in mind, let’s jump in!\n\\[\\begin{align*}\n\\ell (\\theta_i ; y_i) &= \\sum_{i = 1}^{16} \\log\\left( f(y_i; \\theta) \\right)\\\\\n&= \\sum_{i = 1}^{16} \\log\\left[ {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}  \\right]\\\\\n&= \\sum_{i = 1}^{16} \\log {n_i \\choose y_i} + \\log(\\theta)\\sum_{i = 1}^{16} y_i + log(1 - \\theta)\\sum_{i = 1}^{16}(n_i - y_i)  \\\\\n\\frac{d \\ell}{d\\theta} &= 0 + \\sum_{i = 1}^{16} \\frac{y_i}{\\theta} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\theta} \\\\\n\\underline{\\text{set}} \\quad 0 &=  \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} &= \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies  \\hat{\\theta} \\sum_{i = 1}^{16} n_i &= \\sum_{i = 1}^{16} y_i \\\\\n\\implies \\hat{\\theta} &= \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\n\\end{align*}\\]\nTo verify whether this is an MLE or not, let’s now complete step 4 and take the second derivative of the log-likelihood function to see whether it is concave down at \\(\\hat{\\theta}\\).\n\\[\\begin{align*}\n\\frac{d^2\\ell}{d\\theta^2} &= -\\sum_{i = 1}^{16} \\frac{y_i}{\\theta^2} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{(1 - \\theta)^2}\n\\end{align*}\\]\nNote that because \\(0 \\le \\theta \\le 1\\), and because \\(y = 0, 1, 2, ...\\), and because \\(n = 1, 2, 3, ...\\), the second derivative of the log-likelihood function will be negative for all values of \\(\\theta\\). That means that the first derivative has a maximum at \\(\\hat{\\theta}\\).\nThus, the MLE for a random sample of size 16 is \\(\\hat{\\theta} = \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\\). Intuitively, this makes sense, because this is essentially the total number of females in all progenies divided by the total number of individuals. This value for \\(\\hat{\\theta}\\) is evaluated to be \\(363/(371+363) = 0.49\\)."
  },
  {
    "objectID": "fall_24/b719/projects/hw_1.html#problem-1",
    "href": "fall_24/b719/projects/hw_1.html#problem-1",
    "title": "BIOSTAT 719 - Homework 1",
    "section": "",
    "text": "(6 points) [Textbook Exercise 1.6]\n\nThe data in Table 1.4 are the numbers of females and males in the progeny of 16 female light brown apple moths in Muswellbrook, New South Wales, Australia (from Lewis 1987).\n\n\n\n\n\n\n\n\n\n\n\nProgeny Group\nFemales\nMales\n\n\n\n\n1\n18\n11\n\n\n2\n31\n22\n\n\n3\n34\n27\n\n\n4\n33\n29\n\n\n5\n27\n24\n\n\n6\n33\n29\n\n\n7\n28\n25\n\n\n8\n23\n26\n\n\n9\n33\n38\n\n\n10\n12\n14\n\n\n11\n19\n23\n\n\n12\n25\n31\n\n\n13\n14\n20\n\n\n14\n4\n6\n\n\n15\n22\n34\n\n\n16\n7\n12\n\n\n\n\n\n\nLet \\(Y_i\\) denote the number of females in each of the 16 groups of progeny in each group (\\(i = 1, ..., 16\\)). Suppose the \\(Y_i\\)’s are independent random variables each with the Binomial distribution\n\n\\[\nf(y_i; \\theta) = {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}\n\\]\n\nFind the maximum likelihood estimator (MLE) of \\(\\theta\\) using calculus and evaluate it for these data.\n\nLet’s first find the MLE using calculus. Let’s review the steps:\n\nCalculate the log-likelihood function\nTake the first derivative of the log-likelihood function (this is the ‘score’ function)\nSet the score function equal to 0 and solve for \\(\\hat{\\theta}\\)\nTake the second derivative of the function to determine whether the function is concave down (i.e. has a maximum value at \\(\\hat{\\theta}\\))\n\nWith that in mind, let’s jump in!\n\\[\\begin{align*}\n\\ell (\\theta_i ; y_i) &= \\sum_{i = 1}^{16} \\log\\left( f(y_i; \\theta) \\right)\\\\\n&= \\sum_{i = 1}^{16} \\log\\left[ {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}  \\right]\\\\\n&= \\sum_{i = 1}^{16} \\log {n_i \\choose y_i} + \\log(\\theta)\\sum_{i = 1}^{16} y_i + log(1 - \\theta)\\sum_{i = 1}^{16}(n_i - y_i)  \\\\\n\\frac{d \\ell}{d\\theta} &= 0 + \\sum_{i = 1}^{16} \\frac{y_i}{\\theta} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\theta} \\\\\n\\underline{\\text{set}} \\quad 0 &=  \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} &= \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies  \\hat{\\theta} \\sum_{i = 1}^{16} n_i &= \\sum_{i = 1}^{16} y_i \\\\\n\\implies \\hat{\\theta} &= \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\n\\end{align*}\\]\nTo verify whether this is an MLE or not, let’s now complete step 4 and take the second derivative of the log-likelihood function to see whether it is concave down at \\(\\hat{\\theta}\\).\n\\[\\begin{align*}\n\\frac{d^2\\ell}{d\\theta^2} &= -\\sum_{i = 1}^{16} \\frac{y_i}{\\theta^2} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{(1 - \\theta)^2}\n\\end{align*}\\]\nNote that because \\(0 \\le \\theta \\le 1\\), and because \\(y = 0, 1, 2, ...\\), and because \\(n = 1, 2, 3, ...\\), the second derivative of the log-likelihood function will be negative for all values of \\(\\theta\\). That means that the first derivative has a maximum at \\(\\hat{\\theta}\\).\nThus, the MLE for a random sample of size 16 is \\(\\hat{\\theta} = \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\\). Intuitively, this makes sense, because this is essentially the total number of females in all progenies divided by the total number of individuals. This value for \\(\\hat{\\theta}\\) is evaluated to be \\(363/(371+363) = 0.49\\)."
  },
  {
    "objectID": "fall_24/b719/projects/hw_1.html#problem-2",
    "href": "fall_24/b719/projects/hw_1.html#problem-2",
    "title": "BIOSTAT 719 - Homework 1",
    "section": "Problem 2",
    "text": "Problem 2\n(12 points)\n\nLet \\(Y_i, ... , Y_n\\) be independent random variables from the exponential distribution\n\n\\[\nf(y_i; \\lambda) = \\lambda e^{-\\lambda y_i}, \\quad y_i &gt; 0, \\lambda &gt; 0\n\\]\nPart (a):\n\nWhat is the maximum likelihood estimator (MLE) of \\(\\lambda\\)? Show all the derivation details.\n\nLet’s follow the same four steps to find this out!\n\\[\\begin{align*}\n\\ell(\\lambda) &= \\sum_{i = 1}^n \\log [f(y; \\lambda)]\\\\\n&= \\sum_{i = 1}^n \\log [\\lambda e^{-\\lambda y_i}]\\\\\n&= \\sum_{i = 1}^n \\log (\\lambda) - \\sum_{i = 1}^n\\lambda y_i\\\\\n&= n \\log (\\lambda) - \\sum_{i = 1}^n\\lambda y_i\\\\\n\\frac{d\\ell}{d\\lambda} &= \\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i\\\\\n\\underline{\\text{set}}\\quad 0 &= \\frac{n}{\\hat{\\lambda}} - \\sum_{i = 1}^n y_i\\\\\n\\implies \\frac{n}{\\hat{\\lambda}} &= \\sum_{i = 1}^n y_i\\\\\n\\implies \\hat{\\lambda} &= \\frac{n}{\\sum_{i = 1}^n y_i}\\\\\n\\implies \\hat{\\lambda} &= \\frac{1}{\\bar{Y}}\n\\end{align*}\\]\nThis is a fun result, especially considering the fact that the rate parameter is the inverse of the scale parameter. So, by the invariance property of MLEs, we know that the MLE for \\(\\theta\\) (the scale parameter) is \\(\\bar{Y}\\).\nHowever, remember that we still need to check the second derivative to ensure this actually is an MLE.\nWe can easily see that the second derivative of the log-likelihood function, \\(\\frac{d^2\\ell}{d\\lambda^2}\\), is \\(-\\frac{n}{\\lambda^2}\\). Considering that neither \\(n\\) nor \\(\\lambda\\) can take on a negative value, the first derivative (i.e. the score function) will always be concave down, and we can state with certainty that the MLE for \\(\\lambda\\) is \\(\\frac{1}{\\bar{Y}}\\)\nPart (b):\n\nSuppose \\(\\lambda = e^{\\beta}\\). Find the MLE of \\(\\beta\\).\n\nWe are given that \\(\\lambda = e^{\\beta}\\). Thus, \\(\\beta = \\log{\\lambda}\\). By the invariance property of MLEs, if \\(g(\\lambda)\\) is a function of \\(\\lambda\\), then the MLE of \\(g(\\theta)\\) is \\(g(\\hat{\\theta})\\).\nLet \\(g(\\theta) = \\log(\\lambda) = \\beta\\). \\(g(\\hat{\\lambda}) = \\log\\left( \\frac{1}{\\bar{Y}}\\right)\\), which is the MLE of \\(\\beta\\).\nPart (c):\n\nConsider 150 observations \\(y_i, (i = 1, 2, ..., 150)\\) from the exponential distribution with the sum of the 150 observations equal to 30. What is the numerical evaluation of the MLE of \\(\\lambda\\) and \\(\\beta\\)?\n\nEasy peasy. We did all the hard stuff already, so it’s just plug and chug time!\nFor \\(\\hat{\\lambda}\\):\n\\[\\begin{align*}\n\\hat{\\lambda} &= \\frac{1}{\\bar{Y}}\\\\\n&= \\frac{1}{\\frac{30}{150}}\\\\\n&= 5\n\\end{align*}\\]\nFor \\(\\hat{\\beta}\\):\n\\[\\begin{align*}\n\\hat{\\beta} &= \\log\\left[\\frac{1}{\\bar{Y}}\\right]\\\\\n&= \\log(5)\n\\end{align*}\\]\nThus, the numerical evaluations of the MLEs of \\(\\lambda\\) and \\(\\beta\\) are \\(5\\) and \\(\\log(5)\\), respectively."
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_10.html",
    "href": "fall_24/b719/notes/september/sep_10.html",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "This method can be used anytime we are unable to use another method (like the calculus method) for some reason (e.g. closed form, unable to differentiate etc.)\nThe underlying theory for this method is to find tangential lines to the first derivative of the log-likelihood function at informed values of \\(\\theta\\). As we update \\(\\theta\\), we will approach the tangential line at \\(\\theta^{(m)}\\) where \\(y(\\theta^{(m)}) = 0\\)\nMethod:\n\nStart with a reasonable value for \\(\\theta^{(0)}\\). For example, you might try the sample mean or a value close to what you believe could represent the MLE\nI’m going to be skipping some of the math in this step and cutting to the chase. Again, you can find this formula by solving for the equation of the line tangent to the score function at \\(\\theta^{(m)}\\). \\(\\theta^{(m)} = \\theta^{(m - 1)} - \\frac{U(\\theta^{(m - 1)})}{U'(\\theta^{(m - 1)})}\\)\nIterate through values of \\(m\\) until \\(\\theta^{(m)} - \\theta^{(m - 1)}\\) is satisfactorily small\n\nHere’s an example in R that looks at data distributed i.i.d. from a Poisson distribution: \\(Y_i \\sim POI(\\theta)\\)\n\n\n###################################### R ######################################\n# Count of cyclones distributed as Y_i \\sim POI(\\theta)\nseason &lt;- 1:13\ncyclone_count &lt;- c(6, 5, 4, 6, 6, 3, 12, 7, 4, 2, 6, 7, 4)\n\n# Function to find U and dU of Poisson distribution\npoisson_function &lt;- function(y, theta) {\n    # y: vector of sample values\n    # theta: arbitrary value of theta for log-liklihood input\n    n &lt;- length(y)\n    u &lt;- sum(y) * (1 / theta) - n\n    du &lt;- -sum(y) * (1 / theta^2)\n    return(list(u = u, du = du))\n}\n\ntheta0 &lt;- 3.5\ntmp.iter &lt;- NULL\nfor (i in 1:10) {\n    one &lt;- poisson_function(cyclone_count, theta0)\n    tmp.iter &lt;- rbind(tmp.iter, c(theta = theta0, u = one$u, du = one$du))\n    theta0 &lt;- theta0 - one$u / one$du\n}\n\n# Function to find MLE through the Newton-Raphson algorithm\nnewton_raphson_algorithm &lt;- function(f, y, theta_0) {\n    # f: function returning u and du of given PDF\n    # y: vector of sample values\n    # theta_0: initial value for theta\n\n    # Set theta_m for algorithm\n    theta_m &lt;- theta_0 + 1\n    # Set return matrix to null\n    return_matrix &lt;- NULL\n    # Set counter to protect against endless looping\n    loop_counter &lt;- 0\n    while (theta_m - theta_0 &gt; 0.0000001 & loop_counter &lt; 1000) {\n        if (loop_counter &gt; 0) {\n            theta_0 &lt;- theta_m\n        }\n        function_list &lt;- f(y, theta_0)\n        u &lt;- function_list$u\n        du &lt;- function_list$du\n        return_matrix &lt;- rbind(\n            return_matrix, c(theta = theta_0, u = u, du = du)\n        )\n        theta_m &lt;- theta_0 - u / du\n        loop_counter &lt;- loop_counter + 1\n    }\n\n    # Return the matrix\n    return(return_matrix)\n}\n\n# Let's test it out!\nmy_matrix &lt;- newton_raphson_algorithm(\n    poisson_function, cyclone_count, 3.5\n)\nprint(paste(\"The mean of the Cyclone Counts is\", mean(cyclone_count)))\n\n[1] \"The mean of the Cyclone Counts is 5.53846153846154\"\n\nprint(\"Matrix of Algorithm Iterations:\")\n\n[1] \"Matrix of Algorithm Iterations:\"\n\nprint(my_matrix)\n\n        theta            u        du\n[1,] 3.500000 7.571429e+00 -5.877551\n[2,] 4.788194 2.036983e+00 -3.140429\n[3,] 5.436827 2.430192e-01 -2.435799\n[4,] 5.536596 4.379219e-03 -2.348804\n[5,] 5.538461 1.474204e-06 -2.347223\n[6,] 5.538462 1.669775e-13 -2.347222\n\n###############################################################################\n\n\n\n\n\nThe method of scoring algorithm is extremely similar to the Newton-Raphson algorithm. However, if you know that your distribution meets the requirements for GLMs, then you can use the method of scoring algorithm. This can be easier to calculate (because of GLM properties), and it’s also a little more efficient than the Newton-Raphon algorithm\nThis is built on the idea that we can approximate \\(U'\\) with \\(E[U']\\), and \\(I = -E[U']\\)\nMethod\n\nSimilar to the NRA, we start with some value for \\(\\theta^{(0)}\\). We then implement the same iterative process as before, using this new formula to calculate \\(\\theta^{(m)}\\): \\(\\theta^{(m)} = \\theta^{(m - 1)} + \\frac{U(\\theta^{(m - 1)})}{I(\\theta^{(m - 1)})}\\)\n\n\n\n\n\n\n\nWhat is the formula to calculate ^{(m)} for the Newton-Raphson algorithm?\nWhat is the formula to calculate ^{(m)} for the method of scoring algorithm?\nThe Newton-Raphson algorithm and the method of scoring are very similar, but the method of scoring can be easier to implement and more efficient in application. What requirements must be satisfied to use this method?\n\n^{(m)} = ^{(m - 1)} - \n^{(m)} = ^{(m - 1)} + \nPDF must satisfy reqirements for GLMs"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_10.html#numerical-methods-for-approximating-mles",
    "href": "fall_24/b719/notes/september/sep_10.html#numerical-methods-for-approximating-mles",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "This method can be used anytime we are unable to use another method (like the calculus method) for some reason (e.g. closed form, unable to differentiate etc.)\nThe underlying theory for this method is to find tangential lines to the first derivative of the log-likelihood function at informed values of \\(\\theta\\). As we update \\(\\theta\\), we will approach the tangential line at \\(\\theta^{(m)}\\) where \\(y(\\theta^{(m)}) = 0\\)\nMethod:\n\nStart with a reasonable value for \\(\\theta^{(0)}\\). For example, you might try the sample mean or a value close to what you believe could represent the MLE\nI’m going to be skipping some of the math in this step and cutting to the chase. Again, you can find this formula by solving for the equation of the line tangent to the score function at \\(\\theta^{(m)}\\). \\(\\theta^{(m)} = \\theta^{(m - 1)} - \\frac{U(\\theta^{(m - 1)})}{U'(\\theta^{(m - 1)})}\\)\nIterate through values of \\(m\\) until \\(\\theta^{(m)} - \\theta^{(m - 1)}\\) is satisfactorily small\n\nHere’s an example in R that looks at data distributed i.i.d. from a Poisson distribution: \\(Y_i \\sim POI(\\theta)\\)\n\n\n###################################### R ######################################\n# Count of cyclones distributed as Y_i \\sim POI(\\theta)\nseason &lt;- 1:13\ncyclone_count &lt;- c(6, 5, 4, 6, 6, 3, 12, 7, 4, 2, 6, 7, 4)\n\n# Function to find U and dU of Poisson distribution\npoisson_function &lt;- function(y, theta) {\n    # y: vector of sample values\n    # theta: arbitrary value of theta for log-liklihood input\n    n &lt;- length(y)\n    u &lt;- sum(y) * (1 / theta) - n\n    du &lt;- -sum(y) * (1 / theta^2)\n    return(list(u = u, du = du))\n}\n\ntheta0 &lt;- 3.5\ntmp.iter &lt;- NULL\nfor (i in 1:10) {\n    one &lt;- poisson_function(cyclone_count, theta0)\n    tmp.iter &lt;- rbind(tmp.iter, c(theta = theta0, u = one$u, du = one$du))\n    theta0 &lt;- theta0 - one$u / one$du\n}\n\n# Function to find MLE through the Newton-Raphson algorithm\nnewton_raphson_algorithm &lt;- function(f, y, theta_0) {\n    # f: function returning u and du of given PDF\n    # y: vector of sample values\n    # theta_0: initial value for theta\n\n    # Set theta_m for algorithm\n    theta_m &lt;- theta_0 + 1\n    # Set return matrix to null\n    return_matrix &lt;- NULL\n    # Set counter to protect against endless looping\n    loop_counter &lt;- 0\n    while (theta_m - theta_0 &gt; 0.0000001 & loop_counter &lt; 1000) {\n        if (loop_counter &gt; 0) {\n            theta_0 &lt;- theta_m\n        }\n        function_list &lt;- f(y, theta_0)\n        u &lt;- function_list$u\n        du &lt;- function_list$du\n        return_matrix &lt;- rbind(\n            return_matrix, c(theta = theta_0, u = u, du = du)\n        )\n        theta_m &lt;- theta_0 - u / du\n        loop_counter &lt;- loop_counter + 1\n    }\n\n    # Return the matrix\n    return(return_matrix)\n}\n\n# Let's test it out!\nmy_matrix &lt;- newton_raphson_algorithm(\n    poisson_function, cyclone_count, 3.5\n)\nprint(paste(\"The mean of the Cyclone Counts is\", mean(cyclone_count)))\n\n[1] \"The mean of the Cyclone Counts is 5.53846153846154\"\n\nprint(\"Matrix of Algorithm Iterations:\")\n\n[1] \"Matrix of Algorithm Iterations:\"\n\nprint(my_matrix)\n\n        theta            u        du\n[1,] 3.500000 7.571429e+00 -5.877551\n[2,] 4.788194 2.036983e+00 -3.140429\n[3,] 5.436827 2.430192e-01 -2.435799\n[4,] 5.536596 4.379219e-03 -2.348804\n[5,] 5.538461 1.474204e-06 -2.347223\n[6,] 5.538462 1.669775e-13 -2.347222\n\n###############################################################################\n\n\n\n\n\nThe method of scoring algorithm is extremely similar to the Newton-Raphson algorithm. However, if you know that your distribution meets the requirements for GLMs, then you can use the method of scoring algorithm. This can be easier to calculate (because of GLM properties), and it’s also a little more efficient than the Newton-Raphon algorithm\nThis is built on the idea that we can approximate \\(U'\\) with \\(E[U']\\), and \\(I = -E[U']\\)\nMethod\n\nSimilar to the NRA, we start with some value for \\(\\theta^{(0)}\\). We then implement the same iterative process as before, using this new formula to calculate \\(\\theta^{(m)}\\): \\(\\theta^{(m)} = \\theta^{(m - 1)} + \\frac{U(\\theta^{(m - 1)})}{I(\\theta^{(m - 1)})}\\)"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_10.html#three-take-aways",
    "href": "fall_24/b719/notes/september/sep_10.html#three-take-aways",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "What is the formula to calculate ^{(m)} for the Newton-Raphson algorithm?\nWhat is the formula to calculate ^{(m)} for the method of scoring algorithm?\nThe Newton-Raphson algorithm and the method of scoring are very similar, but the method of scoring can be easier to implement and more efficient in application. What requirements must be satisfied to use this method?\n\n^{(m)} = ^{(m - 1)} - \n^{(m)} = ^{(m - 1)} + \nPDF must satisfy reqirements for GLMs"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html",
    "href": "fall_24/b719/notes/august/aug_27.html",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "",
    "text": "The textbook for this course is “An Introduction to Generalized Linear Models, Fourth Edition” by Chapman and Hall.\nShe uses R and SAS.\nCourse grading scale:\n\nAttendance and pop quiz: 5%\nHomework: 20%\nMidterm: 25%\nFinal: 30%\nGroup Research Project: 20%\n\nHomework\n\nThere will be 9 weekly homework assignments. Each are due at the beginning of the class on the due day.\nSubmit as a PDF document.\n10% deduction in score for late submissions.\n\nExams\n\nMidterm and final exams will be in-class.\nThere is a one-page cheat sheet allowed (double-sided).\nRules for exams:\n\nOnly bring calculator (non-graphing) and pencil + eraser\nExams are proctored.\n\n\nGroup research project\n\nWill require analysis of a data set, final report, and final presentation.\n\nPop quizzes\n\nThe primary purpose of the pop quiz is to check attendance.\n\nCommunications\n\nTeams channel in MS Teams\n\nAI Policies\n\nAI is allowed. However, be transparent about use.\nFact checking is my own responsibility.\nAI-generated content without proper acknowledgement or references will be treated as plagiarism.\n\n\n\n\n\n\n\n\nHere’s the Syllabus\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake ownership of your projects. This is one of the hardest things to teach to students.\nMake a check-list for all that needs to be done in all the analyses you come across.\nHwanhee once met an incredibly driven student that was studying in her program while simultaneously working on a part-time PhD program and conducting government research in Korea. But even with his full plate, he took meticulous notes during each class period and would pester her about the lectures. She said that she was annoyed at him and asked him why he cared so much. He responded, “This is my last chance to go through this course material in this class. I won’t get another chance, and I have to learn this now.”"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#syllabus",
    "href": "fall_24/b719/notes/august/aug_27.html#syllabus",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "",
    "text": "The textbook for this course is “An Introduction to Generalized Linear Models, Fourth Edition” by Chapman and Hall.\nShe uses R and SAS.\nCourse grading scale:\n\nAttendance and pop quiz: 5%\nHomework: 20%\nMidterm: 25%\nFinal: 30%\nGroup Research Project: 20%\n\nHomework\n\nThere will be 9 weekly homework assignments. Each are due at the beginning of the class on the due day.\nSubmit as a PDF document.\n10% deduction in score for late submissions.\n\nExams\n\nMidterm and final exams will be in-class.\nThere is a one-page cheat sheet allowed (double-sided).\nRules for exams:\n\nOnly bring calculator (non-graphing) and pencil + eraser\nExams are proctored.\n\n\nGroup research project\n\nWill require analysis of a data set, final report, and final presentation.\n\nPop quizzes\n\nThe primary purpose of the pop quiz is to check attendance.\n\nCommunications\n\nTeams channel in MS Teams\n\nAI Policies\n\nAI is allowed. However, be transparent about use.\nFact checking is my own responsibility.\nAI-generated content without proper acknowledgement or references will be treated as plagiarism.\n\n\n\n\n\n\n\n\nHere’s the Syllabus"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#three-take-aways",
    "href": "fall_24/b719/notes/august/aug_27.html#three-take-aways",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "",
    "text": "Take ownership of your projects. This is one of the hardest things to teach to students.\nMake a check-list for all that needs to be done in all the analyses you come across.\nHwanhee once met an incredibly driven student that was studying in her program while simultaneously working on a part-time PhD program and conducting government research in Korea. But even with his full plate, he took meticulous notes during each class period and would pester her about the lectures. She said that she was annoyed at him and asked him why he cared so much. He responded, “This is my last chance to go through this course material in this class. I won’t get another chance, and I have to learn this now.”"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#review-of-linear-regression-concepts",
    "href": "fall_24/b719/notes/august/aug_27.html#review-of-linear-regression-concepts",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Review of Linear Regression Concepts",
    "text": "Review of Linear Regression Concepts\n\nSay we have two variables, X and Y (predictor and response). Y (the response or outcome) is regarded as a random variable. Explanatory variables (X) are treated as fixed by the experimental design.\nTypes of outcomes:\n\nContinous (e.g. BMI, SBP) - linear regression\nBinary (e.g. death \\(\\in\\) {1, 0}) - logistic regression\nCategorical (nominal, ordinal) - nominal/ordinal logistic regression\nCounts (e.g. number of hospitalizations) - Poisson regression"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#likelihood-and-mles",
    "href": "fall_24/b719/notes/august/aug_27.html#likelihood-and-mles",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Likelihood and MLEs",
    "text": "Likelihood and MLEs\n\n\\(f(y; \\theta)\\) is a probability distribution, where \\(\\theta\\) represents the parameters of the distribution\n\\(L(\\theta; y)\\) is a likelihood function: \\(L(\\theta; y) = \\prod_{i = 1}^n f(y; \\theta)\\)\n\\(l(\\theta; y)\\) is the log likelihood function: \\(l(\\theta; y) = \\sum log f(y; \\theta)\\)\nMLE \\(\\hat{\\theta}\\) of parameter $$ is the value which maximizes the likelihood function: \\(L(\\hat{\\theta}; y) \\ge L(\\theta; y)\\), for all \\(\\theta \\in \\Omega\\) (this is also true for the log-likelihood function)\nHow do you get the MLE? Take the derivative of the log-likelihood function, set to 0, and solve. Then take the second derivative and check the sign."
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#linear-model-specifications",
    "href": "fall_24/b719/notes/august/aug_27.html#linear-model-specifications",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Linear Model Specifications",
    "text": "Linear Model Specifications\n\nModel Fitting Process:\n\n\nSpecify model\n\nFind the probability distribution of Y (what type of variable? what tests are appropriate? etc.)\nEquation linking response and explanatory variable\n\nEstimation of parameters in the model\ncheck adequacy of model (residuals, deviance, AIC, etc.)\nMake inference - confidence intervals, interpretation of results, hypothesis testing\n\n\nIn class, there was an example with women from the town and country (1, 0) and some sort of response variable (I think it was a count). Hwanhee said that in this case, where we have a factor variable, there are two main ways we can think about Y. First, we can construct our Y vector as a \\(n\\text{ x } 1\\) dimmensional vector. The model formula, then, would look like this: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon\\), where \\(X_i = \\begin{cases}1, \\quad \\text{From town}\\\\0, \\quad \\text{From city} \\end{cases}\\). The other way to think about this is to construct our \\(Y\\) vector to be \\(j \\text{ x } k\\) dimmensional, where \\(j\\) is the number of levels of our factor variable and \\(k\\) is the number of individuals in group \\(j\\). In this case, the model formula would be \\(Y_{jk} = \\mu + \\alpha_j + \\epsilon_k\\), where \\(\\mu\\) is the mean of the reference group and \\(\\alpha\\) is the difference between the means of the reference group and group \\(j\\)."
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#three-take-aways-1",
    "href": "fall_24/b719/notes/august/aug_27.html#three-take-aways-1",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nFor a response and explanatory variable, X is considered fixed and Y is considered a random variable.\nThe four steps for the calculus method for finding an MLE is as follows:\n\nFind the log-likelihood function: \\(\\ell (\\theta | y) = \\sum_{i = 1}^n \\log \\left( f(y; \\theta)\\right)\\)\nTake the derivative\nSet to zero and solve\nTake the second derivative\n\nThe are two primary ways of structuring a model formula with a factor-level variable:\n\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\), where \\(X\\) is the factor-level variable\n\\(Y_{jk} = \\mu + Z_{jk} \\alpha + epsilon_{jk}\\), where \\(\\mu\\) is the mean of the reference group and \\(\\alpha\\) is the difference in means of the two groups (assuming two groups)\n\n\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html",
    "href": "fall_24/b707/notes/september/sep_9.html",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "",
    "text": "Here’s a cheat-sheet for plotting:\n\nContinuous variable\n\nHistograms\nKernel density\nBox plot\nviolin plot\nQQ-plot\n\nCategorical variable\n\nBar chart\nPi chart (ew)\n\nVisualizing relationship between variables\n\nHexbin plots\nScatter plot matrix\n3D plots\nTrellis plots and heatmap\n\nTime-series and longitudinal data visualizations\n\nTime series plots\nSpaghetti plots\nLasagna plots (yum)\n\nSpecializaed visualization techniques\n\nNetworks and maps, survival plots, waterfall and forest plots, Venn diagrams\n\n\n\n\n\n\nHistogram\n\nWhen should you plot frequency? When should you plot density?\n\nIf sample size is important, plot frequency\nWhen comparing the shape of the data to another distribution, use density\n\n\nDensity plot\n\nWhat is density?\n\nThe area under the curve = 1\n\n\nBox plot\n\nWhat information do we lose going from a density plot/histogram to a box plot?\n\nWe lose important information about the shape of the distribution. Looking at the box plot is like looking down at an aerial map of a mountain range. We can get an idea of its shape, but we can’t necessarily see the contours of the peaks and ridges that we would get from the ground looking up\n\nWhat information do we gain?\n\nWe gain median + IQR information, plus whiskers\n\n\nViolin plot\n\nAttempts to bridge the density and the box plot\nHere’s my question. Why not overlay the box plot at the bottom of a histogram/density curve?\n\nHexbin plot\n\nThis is like a heat map plotting two continuous variables. I think the advantage of this over a 2D scatterplot is that if we have very large datasets, it gives us additional information about the density of the data\n\nQQ-plot\n\nWe can set a QQ-plot to see where the expected quantiles and the actual quantiles align for any distribution\n\nDistribution of the p-value\n\nDid you know that under the null hypothesis, the p-value is distributed uniformly between 0 and 1? So, if you have LOTS and LOTS of p-values (think GWAS), you can create a density plot to see if it appears to be uniformly distributed\n\n\n\n\n\n\nBar plot\n\nWhat’s the difference between ggplot2::geom_bar and ggplot2::geom_col?\n\ngeom_bar is for frequency bar plots, and geom_col is for a numeric y value (like revenue) and multiple categories (departments A, B, and C)\n\n\nPie plot\n\nEmphasizes proportions\nOften hard to interpret because our eyes aren’t great at area\n\n\n\n\n\n\nGroup vs Stacked bar plots\n\nGrouped bar plots emphasizes comparison within groups\nStacked bar plots emphasizes comparison between groups\n\nScatter plot matrix\n\nUseful to look at relationships between multiple continous variables\nCan add correlation as another dimension to the plot (e.g. color)\n\n3D scatter plots\n\nCan check linearity in 3 dimensions\n\nTrellis plot\n\nStratify to see within group detail\nIs often helpful to have all groups in a single plot too for comparison between groups\n\nHeatmaps\n\nGood for correlation, gene mapping, etc.\n\nTime\n\nTime-series\nSpaghettie plot\nLasagna plot\n\nNetworks\n\nMapping knowledge networks\n\nArc diagram\n\nSimilar information as a network graph, emphasizes different things\n\nMap plots\n\nOften a “heat” map\n\nK-M curves\nWaterfall plots\nForest plot\n\nUseful in meta-analysis\n\nVenn diagram\n\nOverlapping data\n\n\n\n\n\n\n\nPicture a QQ plot generated with this R code: x &lt;- runif(1000, 0, 1); car::qqPlot(x). This will create a plot shaped like an ‘S’. This is because the y values left of the line are much HIGHER/LOWER than expected in a normal distribution, and the y values right of the line are much HIGHER/LOWER than expected in a normal distribution. Using this logic, plots with both tails above the line will be RIGHT/LEFT skewed (think: lower y-values onto the line shifts distribution in what direction?), and plots with both tails below the line will be RIGHT/LEFT skewed (think: raising y-values onto the line shifts distribution in what direction?).\nIn what type of analyses are forest plots particularly useful?\nHistograms can be constructed to show either frequency counts or density. Explain what density is (i.e. the actual value on the y-axis)."
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html#data-visualization",
    "href": "fall_24/b707/notes/september/sep_9.html#data-visualization",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "",
    "text": "Here’s a cheat-sheet for plotting:\n\nContinuous variable\n\nHistograms\nKernel density\nBox plot\nviolin plot\nQQ-plot\n\nCategorical variable\n\nBar chart\nPi chart (ew)\n\nVisualizing relationship between variables\n\nHexbin plots\nScatter plot matrix\n3D plots\nTrellis plots and heatmap\n\nTime-series and longitudinal data visualizations\n\nTime series plots\nSpaghetti plots\nLasagna plots (yum)\n\nSpecializaed visualization techniques\n\nNetworks and maps, survival plots, waterfall and forest plots, Venn diagrams\n\n\n\n\n\n\nHistogram\n\nWhen should you plot frequency? When should you plot density?\n\nIf sample size is important, plot frequency\nWhen comparing the shape of the data to another distribution, use density\n\n\nDensity plot\n\nWhat is density?\n\nThe area under the curve = 1\n\n\nBox plot\n\nWhat information do we lose going from a density plot/histogram to a box plot?\n\nWe lose important information about the shape of the distribution. Looking at the box plot is like looking down at an aerial map of a mountain range. We can get an idea of its shape, but we can’t necessarily see the contours of the peaks and ridges that we would get from the ground looking up\n\nWhat information do we gain?\n\nWe gain median + IQR information, plus whiskers\n\n\nViolin plot\n\nAttempts to bridge the density and the box plot\nHere’s my question. Why not overlay the box plot at the bottom of a histogram/density curve?\n\nHexbin plot\n\nThis is like a heat map plotting two continuous variables. I think the advantage of this over a 2D scatterplot is that if we have very large datasets, it gives us additional information about the density of the data\n\nQQ-plot\n\nWe can set a QQ-plot to see where the expected quantiles and the actual quantiles align for any distribution\n\nDistribution of the p-value\n\nDid you know that under the null hypothesis, the p-value is distributed uniformly between 0 and 1? So, if you have LOTS and LOTS of p-values (think GWAS), you can create a density plot to see if it appears to be uniformly distributed\n\n\n\n\n\n\nBar plot\n\nWhat’s the difference between ggplot2::geom_bar and ggplot2::geom_col?\n\ngeom_bar is for frequency bar plots, and geom_col is for a numeric y value (like revenue) and multiple categories (departments A, B, and C)\n\n\nPie plot\n\nEmphasizes proportions\nOften hard to interpret because our eyes aren’t great at area\n\n\n\n\n\n\nGroup vs Stacked bar plots\n\nGrouped bar plots emphasizes comparison within groups\nStacked bar plots emphasizes comparison between groups\n\nScatter plot matrix\n\nUseful to look at relationships between multiple continous variables\nCan add correlation as another dimension to the plot (e.g. color)\n\n3D scatter plots\n\nCan check linearity in 3 dimensions\n\nTrellis plot\n\nStratify to see within group detail\nIs often helpful to have all groups in a single plot too for comparison between groups\n\nHeatmaps\n\nGood for correlation, gene mapping, etc.\n\nTime\n\nTime-series\nSpaghettie plot\nLasagna plot\n\nNetworks\n\nMapping knowledge networks\n\nArc diagram\n\nSimilar information as a network graph, emphasizes different things\n\nMap plots\n\nOften a “heat” map\n\nK-M curves\nWaterfall plots\nForest plot\n\nUseful in meta-analysis\n\nVenn diagram\n\nOverlapping data"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html#three-take-aways",
    "href": "fall_24/b707/notes/september/sep_9.html#three-take-aways",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "",
    "text": "Picture a QQ plot generated with this R code: x &lt;- runif(1000, 0, 1); car::qqPlot(x). This will create a plot shaped like an ‘S’. This is because the y values left of the line are much HIGHER/LOWER than expected in a normal distribution, and the y values right of the line are much HIGHER/LOWER than expected in a normal distribution. Using this logic, plots with both tails above the line will be RIGHT/LEFT skewed (think: lower y-values onto the line shifts distribution in what direction?), and plots with both tails below the line will be RIGHT/LEFT skewed (think: raising y-values onto the line shifts distribution in what direction?).\nIn what type of analyses are forest plots particularly useful?\nHistograms can be constructed to show either frequency counts or density. Explain what density is (i.e. the actual value on the y-axis)."
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html#smoothing",
    "href": "fall_24/b707/notes/september/sep_9.html#smoothing",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "Smoothing",
    "text": "Smoothing\nThere are several important smoothing methods we went over in class. After the lecture, I was curious as to why smoothing is important in the context of statistical learning. I asked ChatGPT:\n\nPrompt: I’m in a statistical learning class. We’re going to get into machine learning methods later in the course, but for now, we’re talking about smoothing. For example, we talked about the difference between running mean, kernel smoothing, local regression (LOWESS), and splining. We spent a lot of the class discussion on creating smoother vs more granular lines with the method, parameters etc. Here’s my question. Why is this going to be useful in the context of machine learning? Please write a paragraph explaining how these concepts are applied in machine learning.\nResponse: Smoothing techniques, like running mean, kernel smoothing, local regression (LOWESS), and splining, are foundational for understanding how to balance flexibility and interpretability in models. In machine learning, these methods help illustrate the trade-off between bias and variance, which is crucial when building predictive models. By controlling the smoothness of a function, you can prevent overfitting to noisy data (high variance) or underfitting (high bias) by finding the right complexity for your model. Techniques like splines or kernel smoothing are often used in feature engineering, where creating smoother, more representative features improves the predictive power of machine learning models. Additionally, methods like LOESS can be used for understanding relationships in the data, especially in non-linear regression models.\n\nInteresting. It sounds like the principles are important, but maybe we will also use these methods when constructing models in the future? Hopefully there will be more to report later on in the semester. As a reminder, the four goals of this course are to 1. create reproducible reports, 2. create high-quality graphs, 3. create a predictive model, and 4. explore a new dataset. I think I’m most curious about whether this falls into the category of 2 or 3, but maybe both.\nRegardless, let’s look at these smoothing methods:\n\nRunning Mean\n\nMethod\n\nChoose a window (h)\n\\(\\hat{y_j} = \\frac{1}{k} \\sum_{i = 1}^k y_i\\), such that \\(|x_i - x| \\le h\\) for all \\(i = 1, 2, ..., k\\)\nRepeat this for all values of \\(x_j, \\quad j = 1,2,..., n\\)\n\nThis is the simplest method. Large values of \\(h\\) will yield smoother lines. Small values of \\(h\\) will yield more sensitive lines\n\nHere’s an R function for the running mean:\n\nCodePlot\n\n\n\n###################################### R ######################################\n# Create function for running mean:\nrunning_mean &lt;- function(x, y, h) {\n    # Arrange the data from lowest x_i to sample max\n    dat &lt;- dplyr::arrange(data.frame(x = x, y = y), x)\n    x_sorted &lt;- dat$x\n    running_means &lt;- numeric(nrow(dat))\n    # For each x_i, calculate the mean y value within the window (+- h)\n    for (i in 1:nrow(dat)) {\n        running_mean &lt;- mean(dat[abs(dat$x - x_sorted[i]) &lt;= h, \"y\"])\n        running_means[i] &lt;- running_mean\n    }\n    return(running_means)\n}\n\n# Test the function with dummy data\nset.seed(42)\nn &lt;- 250\nh &lt;- 10\nx &lt;- seq(0, 100, length.out = n)\ny &lt;- 10 + 2.5 * x + rnorm(n, 0, 55) # simulate a regression model\nr_means &lt;- running_mean(x, y, h)\ndat &lt;- data.frame(x = x, y = y, r_means = r_means)\n\n# Build the plot\nr_means_example &lt;- ggplot2::ggplot(dat) +\n    ggplot2::geom_point(ggplot2::aes(x = x, y = y), col = \"skyblue\", size = 1) +\n    ggplot2::geom_line(\n        ggplot2::aes(x = x, y = r_means),\n        col = \"firebrick\", lty = 2\n    ) +\n    ggplot2::labs(x = \"X\", y = \"Y\", title = \"Running Means Example\") +\n    ggplot2::theme_bw() +\n    ggplot2::theme(\n        text = ggplot2::element_text(family = \"serif\"),\n        plot.title = ggplot2::element_text(hjust = .5, face = \"bold\")\n    )\n###############################################################################\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Smoothing\n\nKernel smoothing craetes a smoother line than running means. Where the running means is “all or nothing” for each \\(y_i\\) used to calculate the window mean, kernel smoothing applies a weight function to the \\(y_i\\)’s in the window. For example, a value of \\(y\\) with a corresponding \\(x\\) value close to \\(x\\) (i.e. the center of the window) will have a higher weight than a value of \\(y\\) with its corresponding \\(x\\) value closer to the edge of the window.\nFormula:\n\n\\(\\hat{g}(x) = \\frac{\\sum_{i = 1}^n y_i w(\\frac{x_i - x}{h})}{\\sum_{i = 1}^n w(\\frac{x_i - x}{h})}\\)\nA couple of important things to note about this formula: it’s a lot simpler than it looks. The numerator is the weighted average of all \\(y_i\\) values in the window, with the weight being a function of the distance from the center. The denomitor is simple the sum of the weights, which will be 1 in most cases\n\nThe running average is a special case of the kernel smoother where all weights are uniform across the window of h\nEdge effects\n\nThink about the literal edge cases (x max and x min). The window will not take in values lower than the min at the min, and it won’t take values larger than the max at the max. This creates a bias because the window mean does not have sufficient data at the edges\n\n\n\n\nLocal Regression\n\nLOcally WEighted Scatterplot Smoothing (LOWESS) performs a non-parametric regression within a window of the point of interest. This takes away the problem with the edge effects\nLike the previous methods, a unique \\(\\hat{y_i}\\) is calculated for each \\(x_i\\), and a smooth line is created from these estimates\n\n\n\nSplines\n\nSplines are similar to local regression. However, instead of creating \\(n\\) models, we define \\(k\\) knots (points of interest where we think the model should change) and create (I think) \\(k + 1\\) models"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html#three-take-aways-1",
    "href": "fall_24/b707/notes/september/sep_9.html#three-take-aways-1",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nIn kernel smoothing, the “edge effect” is when the edges are BLANK because of limited data at the sample BLANK and the sample BLANK"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html#back",
    "href": "fall_24/b707/notes/september/sep_9.html#back",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "Back",
    "text": "Back"
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html",
    "href": "fall_24/b707/notes/august/aug_26.html",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "Prediction and inference.\n\nWe care about both. How do we manage that “tug-of-war”?\n\nParametric vs Non-parametric\n\nParametric models are conveniently powerful, but they require assumptions to be met.\nNon-parametric models are convenient in that they do not require assumptions to be met, but the trade off is power\n\nAccuracy vs Interpretability\n\nChuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. On the far end (high flexibility, low interpretability) were support vector machines. On the near end (high interpretability, low flexibility) was LASSO. Several others were somewhere in the middle.\n\n\n\n\n\n\nCreate reproducible reports\nCreate high-quality graphs\nExplore a new dataset\nBuild a predictive model\n\n\n\n\n\nThe first day of class, Chuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. What was on the near end of the spectrum (high interpretability, low flexibility)? What was on the far end of the spectrum (high interpretability, low flexibility)?\nWhat are the four goals for this semester?\nDescribe the trade-off between power and flexibiliy in the context of parametric and non-parametric models."
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#trade-offs",
    "href": "fall_24/b707/notes/august/aug_26.html#trade-offs",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "Prediction and inference.\n\nWe care about both. How do we manage that “tug-of-war”?\n\nParametric vs Non-parametric\n\nParametric models are conveniently powerful, but they require assumptions to be met.\nNon-parametric models are convenient in that they do not require assumptions to be met, but the trade off is power\n\nAccuracy vs Interpretability\n\nChuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. On the far end (high flexibility, low interpretability) were support vector machines. On the near end (high interpretability, low flexibility) was LASSO. Several others were somewhere in the middle."
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#goals-by-the-end-of-the-semester",
    "href": "fall_24/b707/notes/august/aug_26.html#goals-by-the-end-of-the-semester",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "Create reproducible reports\nCreate high-quality graphs\nExplore a new dataset\nBuild a predictive model"
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#three-take-aways",
    "href": "fall_24/b707/notes/august/aug_26.html#three-take-aways",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "The first day of class, Chuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. What was on the near end of the spectrum (high interpretability, low flexibility)? What was on the far end of the spectrum (high interpretability, low flexibility)?\nWhat are the four goals for this semester?\nDescribe the trade-off between power and flexibiliy in the context of parametric and non-parametric models."
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#reproducible-research",
    "href": "fall_24/b707/notes/august/aug_26.html#reproducible-research",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nSteps in Research\n\nSpecify the population\nState the research question\nFormulate the hypothesis\nDesign the experiment\nSpecify the experimentor\nSpecify the data collection process\nCreate an analysis plan\nSpecify the analyst\nWrite the code\nEstimate the truth\nMake claims\n\n\n\nImportant concepts of a scientific study\n\nPublications\nReproducible: Given same data, plan, code, etc. obtain the exact results\nReplicable: Given the same population, hypothesis and design, obtain similar results\nFalse discovery: The claim at the end of a study is not equal to the claim you would make if you could observe all data from the population, given your hypothesis, study design, and analysis plan\nP-hacking: Given a population, hypothesis, design, data, plan and analysis, the code changes to match the desired statement\nFile drawer effect: The probability of publication depends on the caim made at the conclusion of a scientific study\n\n\n\nReproducibility Do’s and Don’ts\n\nDo:\n\nIdentify source data\nAutomate process\nDocument code\nUse version control\nTrack software environment\nSet random number generator\nConsider full pipeline\n\nDon’t:\n\nOnly save output\nDo things by hand\nPoint and click"
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#three-take-aways-1",
    "href": "fall_24/b707/notes/august/aug_26.html#three-take-aways-1",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nThe claim at the end of a study is not equal to the claim you would make if you could observe all data from the population, given your hypothesis, study design, and analysis plan. What is this?\nGiven a population, hypothesis, design, data, plan and analysis, the code changes to match the desired statement. What is this?\nThe probability of publication depends on the caim made at the conclusion of a scientific study. What is this?\n\nBack"
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "Austin's MB Projects",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Austin’s MB Projects",
    "section": "",
    "text": "This website was created by Austin Allen. All projects shown here were completed during his time in the Master of Biostatistics (MB) Program at Duke University. Projects are organized by course.\nWhile these projects were assigned by instructors and were completed between August 2023 and May 2025, you can check out Austin’s personal website to see projects selected by him to demonstrate skills as well as to have a little fun."
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "Austin's MB Projects",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "fall_24/b707/index.html",
    "href": "fall_24/b707/index.html",
    "title": "Statistical Methods for Learning and Discovery",
    "section": "",
    "text": "September\n\nHomework 1\nHomework 2\n\n\n\nOctober\n\n\nNovember\nTo review notes taken during class, follow this link.\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/notes.html",
    "href": "fall_24/b707/notes/notes.html",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "",
    "text": "Week of August 26, 2024 - August 28, 2024"
  },
  {
    "objectID": "fall_24/b707/notes/notes.html#august",
    "href": "fall_24/b707/notes/notes.html#august",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "",
    "text": "Week of August 26, 2024 - August 28, 2024"
  },
  {
    "objectID": "fall_24/b707/notes/notes.html#september",
    "href": "fall_24/b707/notes/notes.html#september",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "September",
    "text": "September\n\nWeek of September 4, 2024\nWeek of September 9, 2024 - September 11, 2024\nWeek of September 16, 2024 - September 18, 2024"
  },
  {
    "objectID": "fall_24/b707/notes/notes.html#october",
    "href": "fall_24/b707/notes/notes.html#october",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "October",
    "text": "October"
  },
  {
    "objectID": "fall_24/b707/notes/notes.html#november",
    "href": "fall_24/b707/notes/notes.html#november",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "November",
    "text": "November"
  },
  {
    "objectID": "fall_24/b707/notes/notes.html#back",
    "href": "fall_24/b707/notes/notes.html#back",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "Back",
    "text": "Back"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html",
    "href": "fall_24/b707/notes/september/sep_4.html",
    "title": "September 4, 2024",
    "section": "",
    "text": "What’s the difference between the joins? Left, right, inner, outer, and full - not to mention the option to exclude A from B, etc.\nWhat’s the difference between merge and join?\n\nMerge loses the order of observations\n\n\n\n\n\n\n\n\nCentral tendancy\n\nMean, median, mode\n\nVariability\n\nStandard deviation, IQR\n\nSkewness\n\n\n\n\n\nCorrelation\n\nWhat’s the difference between Pearson’s \\(r\\), Spearman’s \\(\\rho\\), and Kendall’s \\(\\tau\\)?\n\nPearson’s \\(r\\) is parametric\nSpearman’s \\(\\rho\\) is non-parametric and is rank-based\nKendall’s \\(\\tau\\) is non-parametric and can handle categorical variables\n\n\nVariability by group\nTrajectory\n\nSlope\n\n\n\n\n\n\n\n\n\nUnivariate non-response\n\nSingle variable missing data\n\nMultivariate two patterns\n\nI think this is when there are two patterns in the data. Some variables have no missing data, and other variables have missing data in the same places\n\nMonotone\n\nThis is when missingness is exacerbated with variables. For example, let’s say we yave X1, X2, and X3. X1 is missing values, and because X2 and X3 depend on X1, any missingness in X1 is also present in X2 and X3. Then suppose X3 depends on X2, any missingness in X2 shows up in X3 as well. Finally, X3 may have its own missing values. What we get is a steady increase in missingness from X1 to X2 and X3.\n\n\nGeneral\n\nNo patterns are evidence\n\nFile matching\n\nA good example is in a full join of two datasets. Suppose one dataset had variable A and the other had variable B, and both datasets had unique individuals. We will observe a pattern where there is no common missingness for A and B, nor will there be any individual who has both A and B present.\n\nFactor analysis\n\nThis is when there is an entire variable is missing\n\n\n\n\n\n\nMissing Completely At Random (MCAR)\n\n\\(E(Y^c) = E(Y^O)\\)\nA patient is scheduled for a visit but breaks his arms while skiing\nApproach: use complete case (exclude missing values)\n\nMissing At Random (MAR)\n\n\\(E(Y^c) = E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is recorded\nApproach: multiple imputation\n\nMissing Not At Random (MNAR)\n\n\\(E(Y^c) \\ne E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is NOT recorded\nApproach: Model-based correction\n\n\n\n\n\n\n\nWhich non-parametric test of correlation can handle categorical variables?\nIf values are missing completely at random (MCAR), what should your approach be?\nIf values are missing at random (MAR), what should your approach be?\n\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#coding-concepts",
    "href": "fall_24/b707/notes/september/sep_4.html#coding-concepts",
    "title": "September 4, 2024",
    "section": "",
    "text": "What’s the difference between the joins? Left, right, inner, outer, and full - not to mention the option to exclude A from B, etc.\nWhat’s the difference between merge and join?\n\nMerge loses the order of observations"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#describing-variables",
    "href": "fall_24/b707/notes/september/sep_4.html#describing-variables",
    "title": "September 4, 2024",
    "section": "",
    "text": "Central tendancy\n\nMean, median, mode\n\nVariability\n\nStandard deviation, IQR\n\nSkewness\n\n\n\n\n\nCorrelation\n\nWhat’s the difference between Pearson’s \\(r\\), Spearman’s \\(\\rho\\), and Kendall’s \\(\\tau\\)?\n\nPearson’s \\(r\\) is parametric\nSpearman’s \\(\\rho\\) is non-parametric and is rank-based\nKendall’s \\(\\tau\\) is non-parametric and can handle categorical variables\n\n\nVariability by group\nTrajectory\n\nSlope"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#missing-data",
    "href": "fall_24/b707/notes/september/sep_4.html#missing-data",
    "title": "September 4, 2024",
    "section": "",
    "text": "Univariate non-response\n\nSingle variable missing data\n\nMultivariate two patterns\n\nI think this is when there are two patterns in the data. Some variables have no missing data, and other variables have missing data in the same places\n\nMonotone\n\nThis is when missingness is exacerbated with variables. For example, let’s say we yave X1, X2, and X3. X1 is missing values, and because X2 and X3 depend on X1, any missingness in X1 is also present in X2 and X3. Then suppose X3 depends on X2, any missingness in X2 shows up in X3 as well. Finally, X3 may have its own missing values. What we get is a steady increase in missingness from X1 to X2 and X3.\n\n\nGeneral\n\nNo patterns are evidence\n\nFile matching\n\nA good example is in a full join of two datasets. Suppose one dataset had variable A and the other had variable B, and both datasets had unique individuals. We will observe a pattern where there is no common missingness for A and B, nor will there be any individual who has both A and B present.\n\nFactor analysis\n\nThis is when there is an entire variable is missing\n\n\n\n\n\n\nMissing Completely At Random (MCAR)\n\n\\(E(Y^c) = E(Y^O)\\)\nA patient is scheduled for a visit but breaks his arms while skiing\nApproach: use complete case (exclude missing values)\n\nMissing At Random (MAR)\n\n\\(E(Y^c) = E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is recorded\nApproach: multiple imputation\n\nMissing Not At Random (MNAR)\n\n\\(E(Y^c) \\ne E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is NOT recorded\nApproach: Model-based correction"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#three-take-aways",
    "href": "fall_24/b707/notes/september/sep_4.html#three-take-aways",
    "title": "September 4, 2024",
    "section": "",
    "text": "Which non-parametric test of correlation can handle categorical variables?\nIf values are missing completely at random (MCAR), what should your approach be?\nIf values are missing at random (MAR), what should your approach be?\n\nBack"
  },
  {
    "objectID": "fall_24/b707/projects/hw1.html",
    "href": "fall_24/b707/projects/hw1.html",
    "title": "BIOSTAT 707 - Homework 1",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\nb707_hw1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn [ ]:\n\n\n\n\nimport pandas as pd\nfrom numpy import floor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions¶\n\n\n\nThe goal is to get comfortable with Jupyter notebook and executable documents. You will need to submit a Jupyter notebook that should compile by itself along with the pdf. If you use any special packages make sure that their use is documented so that the user (me!) knows what needs to be loaded.\n\n\nIf your program does not compile properly it will be sent back and you will be asked to submit another version.\n\n\n\nThe website https://www.causeweb.org/tshs/surgery-timing/ describes a study looking at the time of day a surgery is performed. Download and read the associated article: “Operation timing and 30-day mortality after elective general surgery” by Sessler et al. posted on the Sakai website.\n\n\nThe study data are available in a number of formats on the website. Load the RData object directly from the website. Use the command: load(url(“http://mywebsite.com/mydata.RData”)) where ‘mywebsite.com/mydata.RData’ refers to the link address of the data you wish to use.\n\n\nRecreate Tables 1 & 2 from the paper. Comment on any discrepancies. (You don’t need to format the tables in the same way)\n\n\nFigures 3 & 4 report adjusted probability estimates. What were these probabilities adjusted for? Is this study reproducible based on the published article?\n\n\n\n\nWork¶\n\n\nLet’s start by loading in the data and building table 1.\n\n\nTable 1¶\n\n\n\n\n\n\n\n\n\n\n\n\nIn [ ]:\n\n\n\n\nimport pandas as pd\n\n# Import data\ndata_url = \"https://causeweb.org/tshs/datasets/Surgery%20Timing.xlsx\"\nsurgery_dataset = pd.read_excel(data_url)\n\n# Specify variables for table 1\nbaseline_vars = [\n    \"age\",\n    \"gender\",\n    \"race\",\n    \"asa_status\",\n    \"bmi\",\n    \"baseline_cancer\",\n    \"baseline_cvd\",\n    \"baseline_dementia\",\n    \"baseline_diabetes\",\n    \"baseline_osteoart\",\n    \"baseline_psych\",\n    \"baseline_pulmonary\",\n    \"baseline_charlson\",\n    \"mortality_rsi\",\n    \"complication_rsi\",\n]\n\ncontinuous_vars = [\n    \"age\",\n    \"bmi\",\n    \"baseline_charlson\",\n    \"mortality_rsi\",\n    \"complication_rsi\",\n]\ncategorical_vars = [\n    \"gender\",\n    \"race\",\n    \"asa_status\",\n    \"baseline_cancer\",\n    \"baseline_cvd\",\n    \"baseline_dementia\",\n    \"baseline_diabetes\",\n    \"baseline_osteoart\",\n    \"baseline_psych\",\n    \"baseline_pulmonary\",\n]\n\n# Create the first table\ntable_1 = pd.DataFrame({\"Factor\": [], \"Statistic\": []})\n\n# Summary for continuous variables\ncontinuous_summary = (\n    surgery_dataset[continuous_vars].agg([\"mean\", \"std\"]).transpose()\n)\ncontinuous_summary = continuous_summary.rename(\n    columns={\"mean\": \"Mean\", \"std\": \"Std. Dev\"}\n)\n\n\n# Summary for categorical variables\nall_cat_summaries: dict[str, dict[str, list[any]]] = {}\n\nfor var in categorical_vars:\n    counts = surgery_dataset[var].value_counts()\n    percentages = surgery_dataset[var].value_counts(normalize=True) * 100\n    cat_summary = {\n        \"Level\": counts.index,\n        \"Count\": counts.values,\n        \"Percentage\": percentages.values,\n    }\n    all_cat_summaries[var] = cat_summary\n\ntable_index = 0\n# Create a new table with formatted values\n# Example corrected code to add rows:\nfor var in baseline_vars:\n    if var in continuous_vars:\n        # Corrected to use .iloc and assign correctly formatted summary stats\n        var_mean = round(continuous_summary.loc[var].iloc[0], 2)\n        var_std_dev = round(continuous_summary.loc[var].iloc[1], 3)\n        table_1.loc[table_index] = [var, f\"{var_mean} ± {var_std_dev}\"]\n        table_index += 1\n    elif var in categorical_vars:\n        var_info = all_cat_summaries[var]\n        var_levels = var_info[\"Level\"]\n        var_counts = var_info[\"Count\"]\n        var_percents = var_info[\"Percentage\"]\n\n        table_1.loc[table_index] = [var, \"\"]\n        table_index += 1\n\n        for i in range(len(var_levels)):\n            level = var_levels[i]\n            lvl_count = var_counts[i]\n            lvl_pct = var_percents[i]\n            table_1.loc[table_index] = [\n                f\" {level}\",\n                f\"{lvl_count} ({round(lvl_pct, 1)})\",\n            ]\n            table_index += 1\n    else:\n        raise Exception(f\"Variable {var} was not included\\n\")\n\nprint(table_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                Factor       Statistic\n0                  age  57.66 ± 15.044\n1               gender                \n2                  0.0    17230 (53.8)\n3                  1.0    14768 (46.2)\n4                 race                \n5                  1.0    26488 (84.0)\n6                  2.0     3790 (12.0)\n7                  3.0      1243 (3.9)\n8           asa_status                \n9                  1.0    17261 (54.0)\n10                 2.0    13677 (42.7)\n11                 3.0      1055 (3.3)\n12                 bmi   29.45 ± 7.268\n13     baseline_cancer                \n14                   0    21043 (65.8)\n15                   1    10958 (34.2)\n16        baseline_cvd                \n17                   1    16176 (50.5)\n18                   0    15825 (49.5)\n19   baseline_dementia                \n20                   0    31759 (99.2)\n21                   1       242 (0.8)\n22   baseline_diabetes                \n23                   0    27835 (87.0)\n24                   1     4166 (13.0)\n25   baseline_osteoart                \n26                   0    26282 (82.1)\n27                   1     5719 (17.9)\n28      baseline_psych                \n29                   0    29091 (90.9)\n30                   1      2910 (9.1)\n31  baseline_pulmonary                \n32                   0    28508 (89.1)\n33                   1     3493 (10.9)\n34   baseline_charlson    1.18 ± 1.875\n35       mortality_rsi   -0.53 ± 1.038\n36    complication_rsi   -0.41 ± 1.204\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2¶\n\n\n\n\n\n\n\n\n\n\n\n\nIn [ ]:\n\n\n\n\n# Create Table 2\ntable_2 = pd.DataFrame({\"Factor\": [], \"N (%) of cases\": []})\ntable_2_vars = [\"hour\", \"dow\", \"month\", \"moonphase\"]\nall_tab2_summaries: dict[str, dict[str, list[any]]] = {}\ntable_2_data = surgery_dataset\ntable_2_data[\"hour\"] = floor(surgery_dataset[\"hour\"])\nfor var in table_2_vars:\n    counts = surgery_dataset[var].value_counts()\n    percentages = surgery_dataset[var].value_counts(normalize=True) * 100\n    cat_summary = {\n        \"Level\": counts.index,\n        \"Count\": counts.values,\n        \"Percentage\": percentages.values,\n    }\n    all_cat_summaries[var] = cat_summary\n\ntable_index = 0\nfor var in table_2_vars:\n    var_info = all_cat_summaries[var]\n    var_levels = var_info[\"Level\"]\n    var_counts = var_info[\"Count\"]\n    var_percents = var_info[\"Percentage\"]\n\n    table_2.loc[table_index] = [var, \"\"]\n    table_index += 1\n\n    for i in range(len(var_levels)):\n        level = var_levels[i]\n        lvl_count = var_counts[i]\n        lvl_pct = var_percents[i]\n        table_2.loc[table_index] = [\n            f\" {level}\",\n            f\"{lvl_count} ({round(lvl_pct, 1)})\",\n        ]\n        table_index += 1\nprint(table_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       Factor N (%) of cases\n0        hour               \n1         7.0   10631 (33.2)\n2         8.0    3807 (11.9)\n3        11.0     2855 (8.9)\n4        12.0     2763 (8.6)\n5        13.0     2623 (8.2)\n6        10.0     2501 (7.8)\n7        14.0     2063 (6.4)\n8         9.0     1664 (5.2)\n9        15.0     1267 (4.0)\n10       16.0      745 (2.3)\n11        6.0      562 (1.8)\n12       17.0      356 (1.1)\n13       18.0      163 (0.5)\n14       19.0        1 (0.0)\n15        dow               \n16          2    7008 (21.9)\n17          1    7005 (21.9)\n18          3    6266 (19.6)\n19          5    6087 (19.0)\n20          4    5635 (17.6)\n21      month               \n22          9    3208 (10.0)\n23          8     3177 (9.9)\n24          6     2994 (9.4)\n25          4     2698 (8.4)\n26          3     2697 (8.4)\n27         10     2689 (8.4)\n28          1     2670 (8.3)\n29          5     2654 (8.3)\n30         11     2544 (7.9)\n31          2     2506 (7.8)\n32          7     2325 (7.3)\n33         12     1839 (5.7)\n34  moonphase               \n35          4    8142 (25.4)\n36          2    8100 (25.3)\n37          3    8051 (25.2)\n38          1    7708 (24.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside from the fact that my tables were largely out of order (and the awkward moon-phase issue in my table), there are only a few discrepancies.\n\n\n\nTable 1\n\n\nBMI is different, most likely in how they handled missing values. No documentation is recorded\n\n\nI failed in my table to output the risk factors using mean and IQR\n\n\n\n\nTable 2\n\n\nI did not see the correct way to concatenate “moonphase,” so I left it as it was. This created an obvious problem\n\n\nThey did not include the 1 case at 19:00. I don’t see that this was documented, but I can clearly see that they lumped it in with 18:00\n\n\n\n\n\nFigures 3 and 4¶\n\n\nNow let’s talk about figures 3 and 4.\n\n\nWhat I believe they mean when they say that the probabilities were “adjusted” was that individuals were ommitted from the model based on Risk Stratification Index (RSI).\n\n\nConclusion¶\n\n\nDo I think this study is reproducible? No. At best, it may be replicable, but I’m left with an unclear sense of who and who was not excluded from the model. Figure 2 states that 76 individuals were excluded because of unavailable 30-day mortality information, but it doesn’t say anything about individuals excluded for RSI."
  },
  {
    "objectID": "fall_24/b719/index.html",
    "href": "fall_24/b719/index.html",
    "title": "Generalized Linear Models: Class Projects",
    "section": "",
    "text": "Homework 1"
  },
  {
    "objectID": "fall_24/b719/index.html#august",
    "href": "fall_24/b719/index.html#august",
    "title": "Generalized Linear Models: Class Projects",
    "section": "",
    "text": "Homework 1"
  },
  {
    "objectID": "fall_24/b719/index.html#september",
    "href": "fall_24/b719/index.html#september",
    "title": "Generalized Linear Models: Class Projects",
    "section": "September",
    "text": "September\n\nHomework 2\nHomework 3\nHomework 4"
  },
  {
    "objectID": "fall_24/b719/index.html#october",
    "href": "fall_24/b719/index.html#october",
    "title": "Generalized Linear Models: Class Projects",
    "section": "October",
    "text": "October"
  },
  {
    "objectID": "fall_24/b719/index.html#november",
    "href": "fall_24/b719/index.html#november",
    "title": "Generalized Linear Models: Class Projects",
    "section": "November",
    "text": "November\nTo review notes taken during class, follow this link.\nBack"
  },
  {
    "objectID": "fall_24/b719/notes/notes.html",
    "href": "fall_24/b719/notes/notes.html",
    "title": "Generalized Linear Models: Course Notes",
    "section": "",
    "text": "August\n\nWeek of August 27, 2024 - August 29, 2024\n\n\n\nSeptember\n\nWeek of September 3, 2024 - September 5, 2024\nWeek of September 10, 2024 - September 12, 2024\n\n\n\nOctober\n\n\nNovember\nBack"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html",
    "href": "fall_24/b719/notes/september/sep_3.html",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "When an outcome is continuous, we consider a LR model like to be: \\(E(Y_i) = \\mu_i = X_i^T \\beta\\), where \\(T_i \\sim N(\\mu_i, \\sigma^2)\\)\nFor models with non-continuous outcomes, the distribution of \\(Y_i\\) is not normal. However, they may be in the exponential family and can share some of the nice properties of normality\nExponential family definition:\n\nDistribution of Y belongs to the exponential family if it can be written in the form \\(f(y; \\theta) = exp[a(y)b(\\theta) + c(\\theta) + d(y)]\\)\nCanonical form: if \\(a(y) = y\\), the distribution is in canonical form and \\(b(\\theta)\\) is said to be the natural parameter\nIf there are other parameters other than the parameter of interest (say \\(\\sigma^2\\) when we’re interested in \\(\\mu\\)), these are said to be nuisance parameters\n\n\n\n\n\n\n\n\nIs the Poisson PDF in canonical form? If so, what is the natural parameter?\n\nPoisson PDF: \\(f(y; \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}, \\quad y = 0, 1, 2, ...\\)\n\n\\(f(y; \\theta) = exp[y \\log (\\theta) - \\theta - log(y!)]\\)\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(\\theta)\\)\n\\(c(\\theta) = -\\theta\\)\n\\(d(y) = -\\log(y!)\\)\n\nYes, it’s in canonical form, and the natural parameter is \\(\\log(\\theta)\\). Easy peasy.\n\n\n\n\nWhat is the nuisance parameter? Is it in canonical form? What (if yes) is the natural parameter?\n\n\\[\\begin{align*}\nf(y; \\theta) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[-\\frac{1}{2\\sigma^2}(y - \\mu)^2\\right]\\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2}\\right]\\\\\n&= exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2} -\\frac{1}{2}log(2\\pi\\sigma^2)  \\right]\\\\\n\\end{align*}\\]\n\n\\(a(y) = y\\)\n\\(b(\\mu) = \\frac{\\mu}{\\sigma^2}\\)\n\\(c(\\mu) = \\frac{\\mu^2}{2\\sigma^2}\\)\n\\(d(y) = - \\frac{y^2}{2\\sigma^2} - \\frac{log(2\\pi\\sigma^2)}{2}\\)\n\nThe nuisance parameter is \\(\\sigma^2\\). It is in canonical form, and the natural parameter is \\(\\mu/\\sigma^2\\).\n\n\n\n\nWhy do we care about the exponential family? Because it has some really useful properties. Specifically, we’re going to learn about calculating the mean and variance.\n\nProperty 1: \\(E[a(y)] = -c'(\\theta)/b'(\\theta)\\)\nProperty 2: \\(Var[a(y)] = \\frac{b''(\\theta) c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\)\n\n\n\n\nRecall that the score function (or score statistic) is the first derivative of the log-likelihood function. Note that because it is a function of Y, it is considered a random variable. There are two extremely interesting properties of the score function.\nLet the random variable U represent the score function.\n\nProperty 1: \\(E[U] = 0\\)\nProperty 2: \\(Var[U] = \\frac{b''(\\theta)c'(\\theta)}{b'(\\theta)} - c''(\\theta) = I(\\theta)\\), where \\(I(\\theta)\\) is the Fisher’s information of \\(\\theta\\). Isn’t that crazy? The variance of the first derivative of the log-likelihood is the Fisher’s information.\n\n\n\n\n\nSuppose f(y; ) is a PDF. How must f(y; ) be written to conclude that it belongs to the exponential family?\nProperty 1 of the exponential family has to do with E[a(y)]. What is the property?\nProperty 2 of the exponential family has to do with V(a(y)). What is the property?"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#exponential-family",
    "href": "fall_24/b719/notes/september/sep_3.html#exponential-family",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "When an outcome is continuous, we consider a LR model like to be: \\(E(Y_i) = \\mu_i = X_i^T \\beta\\), where \\(T_i \\sim N(\\mu_i, \\sigma^2)\\)\nFor models with non-continuous outcomes, the distribution of \\(Y_i\\) is not normal. However, they may be in the exponential family and can share some of the nice properties of normality\nExponential family definition:\n\nDistribution of Y belongs to the exponential family if it can be written in the form \\(f(y; \\theta) = exp[a(y)b(\\theta) + c(\\theta) + d(y)]\\)\nCanonical form: if \\(a(y) = y\\), the distribution is in canonical form and \\(b(\\theta)\\) is said to be the natural parameter\nIf there are other parameters other than the parameter of interest (say \\(\\sigma^2\\) when we’re interested in \\(\\mu\\)), these are said to be nuisance parameters"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#examples",
    "href": "fall_24/b719/notes/september/sep_3.html#examples",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Is the Poisson PDF in canonical form? If so, what is the natural parameter?\n\nPoisson PDF: \\(f(y; \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}, \\quad y = 0, 1, 2, ...\\)\n\n\\(f(y; \\theta) = exp[y \\log (\\theta) - \\theta - log(y!)]\\)\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(\\theta)\\)\n\\(c(\\theta) = -\\theta\\)\n\\(d(y) = -\\log(y!)\\)\n\nYes, it’s in canonical form, and the natural parameter is \\(\\log(\\theta)\\). Easy peasy.\n\n\n\n\nWhat is the nuisance parameter? Is it in canonical form? What (if yes) is the natural parameter?\n\n\\[\\begin{align*}\nf(y; \\theta) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[-\\frac{1}{2\\sigma^2}(y - \\mu)^2\\right]\\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2}\\right]\\\\\n&= exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2} -\\frac{1}{2}log(2\\pi\\sigma^2)  \\right]\\\\\n\\end{align*}\\]\n\n\\(a(y) = y\\)\n\\(b(\\mu) = \\frac{\\mu}{\\sigma^2}\\)\n\\(c(\\mu) = \\frac{\\mu^2}{2\\sigma^2}\\)\n\\(d(y) = - \\frac{y^2}{2\\sigma^2} - \\frac{log(2\\pi\\sigma^2)}{2}\\)\n\nThe nuisance parameter is \\(\\sigma^2\\). It is in canonical form, and the natural parameter is \\(\\mu/\\sigma^2\\)."
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#properties-of-the-exponential-family",
    "href": "fall_24/b719/notes/september/sep_3.html#properties-of-the-exponential-family",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Why do we care about the exponential family? Because it has some really useful properties. Specifically, we’re going to learn about calculating the mean and variance.\n\nProperty 1: \\(E[a(y)] = -c'(\\theta)/b'(\\theta)\\)\nProperty 2: \\(Var[a(y)] = \\frac{b''(\\theta) c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\)"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#the-score-function",
    "href": "fall_24/b719/notes/september/sep_3.html#the-score-function",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Recall that the score function (or score statistic) is the first derivative of the log-likelihood function. Note that because it is a function of Y, it is considered a random variable. There are two extremely interesting properties of the score function.\nLet the random variable U represent the score function.\n\nProperty 1: \\(E[U] = 0\\)\nProperty 2: \\(Var[U] = \\frac{b''(\\theta)c'(\\theta)}{b'(\\theta)} - c''(\\theta) = I(\\theta)\\), where \\(I(\\theta)\\) is the Fisher’s information of \\(\\theta\\). Isn’t that crazy? The variance of the first derivative of the log-likelihood is the Fisher’s information."
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#three-take-aways",
    "href": "fall_24/b719/notes/september/sep_3.html#three-take-aways",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Suppose f(y; ) is a PDF. How must f(y; ) be written to conclude that it belongs to the exponential family?\nProperty 1 of the exponential family has to do with E[a(y)]. What is the property?\nProperty 2 of the exponential family has to do with V(a(y)). What is the property?"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#information---review",
    "href": "fall_24/b719/notes/september/sep_3.html#information---review",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Information - Review",
    "text": "Information - Review\n\nWe find an MLE of \\(\\theta\\) setting U (the score function) to 0. We also know that the variance of \\(U\\) is the Fisher’s information. But what is information? How is it related to MLEs?\nInformation is a way to measure the amount of information that your data carry about the unkown parameter, \\(\\theta\\). As an aside, I verified that \\(I = 1/\\text{curvature}\\), which is really interesting."
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#glms",
    "href": "fall_24/b719/notes/september/sep_3.html#glms",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "GLMs",
    "text": "GLMs\n\nResearch Process (again)\n\nModel specification\n\n\\(y\\sim \\text{exponential family}\\)\nLinear association between Y and X with model parameter \\(E(y) = X^T\\beta\\)\nYou got yourself a GLM (this is where we are going to be in the process for the next week or two)\n\nEstimation\n\n\\(y\\sim N(\\mu, \\sigma^2)\\): \\(\\hat{\\mu}\\)\n\\(y\\sim N(\\beta X, \\sigma^2)\\): \\(\\hat{\\beta}\\)\n\\(y\\sim Pois(\\beta X)\\): \\(\\hat{\\beta}\\), this gets complicated\n\nModel fit\n\nAIC, LRT, deviance\n\nInference\n\n\n\nDefinition of the Generalized Linear Model\nGLM is defined in terms of a set of independent random variables \\(Y_1, ..., Y_n\\), each with a distribution from the exponential family and having the following three properties:\n\nThe distribution of each \\(Y_i\\) has the canonical form and depends on a single parameter \\(\\theta_i\\)\nThe distibution of all the \\(Y_i\\)’s are of the same form so that \\(b_i(.) = b(.)\\), \\(c_i(.) = c(.)\\), and \\(d_i(.) = d(.)\\).\nSuppose \\(E(Y_i) = \\mu_i\\), where \\(\\mu\\) is some function of \\(\\theta_i\\). For a GLM, there exists a transformation of \\(\\mu_i\\) such that \\(g(\\mu_i) = X_i^T\\beta = \\eta_i\\)\n\n\\(g(.)\\) is a monotone, differentiable function called the link function\n\\(X^T_i\\) is the 1 X p design vector (ith row of the design matrix X)\n\\(\\beta\\) is the p X 1 vector of parameters\n\n\n\n\nThe Link Function\n\nFor linear regression, the link function is the identity function: \\(g(\\mu_i) = \\mu_i\\)\nFor logistic regression, the link function is the logit function: \\(g(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = X^T_i \\beta\\)\nFor Poisson regression, the link function is \\(g(\\theta_i) = X^T_i\\beta = \\log(\\theta_i)\\)\nThe inverse link function gives us \\(\\mu_i\\), which can be very useful (think getting \\(\\pi\\) in logistic regression)\n\nThe inverse link function for logistic regression (called the expit) is \\(\\frac{1}{1 + e^{-x^T_i\\beta}}\\)"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#three-take-aways-1",
    "href": "fall_24/b719/notes/september/sep_3.html#three-take-aways-1",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nFor a set of independent random variables to be a GLM, there are three properties that need to be met. The first is that the distribution of each Y_i depends on a single parameter and has what special form?\nFor a set of independent random variables to be a GLM, there are three properties that need to be met. The third and most important property is that for E(Y_i) = , there exists a transformation of such that … (fill in the blank).\nWhat is the form of the inverse link function for logistic regression? (Reminder: this is the function known as the expit function)\n\nBack"
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html",
    "href": "fall_24/b719/projects/hw_2.html",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "",
    "text": "Show that the following probability density functions belong to the exponential family. Define a(·), b(·), c(·), d(·) components.\nPart (a): Pareto distribution \\(f(y; \\theta) = \\theta y^{-\\theta-1}\\)\n\nFor all of these, we must show that the function holds the form \\(\\exp\\left[ a(y)b(\\theta) + c(\\theta) + d(y) \\right]\\). Let’s rewrite the Pareto PDF just a smidge:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-\\theta \\log(y)+ \\log(\\theta) - \\log(y)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = \\log(y)\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = -\\log(y)\\)\n\nWe can clearly see that the Pareto distribution is a member of the exponential family.\n\nPart (b): Exponential distribution \\(f(y; \\theta) = \\theta^{-y\\theta}\\)\n\nOnce again, let’s rewrite this PDF:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-y\\theta + \\log(\\theta)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = 0\\)\n\nThe exponential distribution, thankfully, is a member of the exponential family. Otherwise, I think that would get awkward at family reunions.\n\nPart (c): Negative binomial distribution \\(f(y; \\theta) = {y + r - 1 \\choose r - 1} \\theta^r(1 - \\theta)^y\\), where \\(r\\) is known\n\nUsing the same method, we get the following:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[y\\log(1 - \\theta) + r\\log(\\theta) + \\log\\left[{y + r - 1\\choose r - 1} \\right]\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(1 - \\theta)\\)\n\\(c(\\theta) = r\\log(\\theta)\\)\n\\(d(y) = \\log\\left[{y + r - 1\\choose r - 1} \\right]\\)\n\nThe negative binomial distribution is also part of the exponential family.\nPart (d): Extreme value (Gumbel) distribution \\(f(y; \\theta) = \\frac{1}{\\Phi} exp\\left \\{  \\frac{y - \\theta}{\\Phi} - exp\\left[ \\frac{y - \\theta}{\\Phi} \\right]  \\right \\}\\), where \\(\\Phi &gt; 0\\) is considered a nuissance parameter.\nSimilar to how we conducted part (c), we are going to ignore nuissance parameters (i.e. treat them as constants) and focus on the parameter of interest, \\(\\theta\\).\nWe can rewrite the PDF as follows:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[ -exp\\left[ \\frac{y - \\theta}{\\Phi} \\right] - \\frac{\\theta}{\\Phi} + \\frac{y}{\\Phi} + \\log\\left(\\frac{1}{\\Phi} \\right)\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = -e^{y/\\Phi}\\)\n\\(b(\\theta) = e^{-\\theta/\\Phi}\\)\n\\(c(\\theta) = -\\frac{\\theta}{\\Phi}\\)\n\\(d(y) = \\frac{y}{\\Phi} + \\log\\left[\\frac{1}{\\Phi} \\right]\\)\n\nThis, too, is part of the exponential family."
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#problem-1",
    "href": "fall_24/b719/projects/hw_2.html#problem-1",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "",
    "text": "Show that the following probability density functions belong to the exponential family. Define a(·), b(·), c(·), d(·) components.\nPart (a): Pareto distribution \\(f(y; \\theta) = \\theta y^{-\\theta-1}\\)\n\nFor all of these, we must show that the function holds the form \\(\\exp\\left[ a(y)b(\\theta) + c(\\theta) + d(y) \\right]\\). Let’s rewrite the Pareto PDF just a smidge:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-\\theta \\log(y)+ \\log(\\theta) - \\log(y)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = \\log(y)\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = -\\log(y)\\)\n\nWe can clearly see that the Pareto distribution is a member of the exponential family.\n\nPart (b): Exponential distribution \\(f(y; \\theta) = \\theta^{-y\\theta}\\)\n\nOnce again, let’s rewrite this PDF:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-y\\theta + \\log(\\theta)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = 0\\)\n\nThe exponential distribution, thankfully, is a member of the exponential family. Otherwise, I think that would get awkward at family reunions.\n\nPart (c): Negative binomial distribution \\(f(y; \\theta) = {y + r - 1 \\choose r - 1} \\theta^r(1 - \\theta)^y\\), where \\(r\\) is known\n\nUsing the same method, we get the following:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[y\\log(1 - \\theta) + r\\log(\\theta) + \\log\\left[{y + r - 1\\choose r - 1} \\right]\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(1 - \\theta)\\)\n\\(c(\\theta) = r\\log(\\theta)\\)\n\\(d(y) = \\log\\left[{y + r - 1\\choose r - 1} \\right]\\)\n\nThe negative binomial distribution is also part of the exponential family.\nPart (d): Extreme value (Gumbel) distribution \\(f(y; \\theta) = \\frac{1}{\\Phi} exp\\left \\{  \\frac{y - \\theta}{\\Phi} - exp\\left[ \\frac{y - \\theta}{\\Phi} \\right]  \\right \\}\\), where \\(\\Phi &gt; 0\\) is considered a nuissance parameter.\nSimilar to how we conducted part (c), we are going to ignore nuissance parameters (i.e. treat them as constants) and focus on the parameter of interest, \\(\\theta\\).\nWe can rewrite the PDF as follows:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[ -exp\\left[ \\frac{y - \\theta}{\\Phi} \\right] - \\frac{\\theta}{\\Phi} + \\frac{y}{\\Phi} + \\log\\left(\\frac{1}{\\Phi} \\right)\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = -e^{y/\\Phi}\\)\n\\(b(\\theta) = e^{-\\theta/\\Phi}\\)\n\\(c(\\theta) = -\\frac{\\theta}{\\Phi}\\)\n\\(d(y) = \\frac{y}{\\Phi} + \\log\\left[\\frac{1}{\\Phi} \\right]\\)\n\nThis, too, is part of the exponential family."
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#problem-2",
    "href": "fall_24/b719/projects/hw_2.html#problem-2",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "Problem 2",
    "text": "Problem 2\n\nConsider a randome variable Y with the following Gamma distribution with a scale parameter, \\(\\beta\\), of interest, and a known shape parameter \\(\\alpha\\):\n\\[\nf(y; \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}y^{\\alpha - 1}e^{-y\\beta}, \\text{  where } y &gt; 0, \\quad \\alpha &gt; 0, \\quad \\beta &gt; 0\n\\]\n\n\nPart (a):\n\n\nDoes this distribution belong to the exponential family?\n\nTo answer this question, let’s once again see if we can rewrite this function in terms of a(.), b(.), c(.), and d(.). Note that because \\(\\beta\\) is the parameter of interest, we treat \\(\\alpha\\) as the nuissance parameter.\n\\[\n\\begin{align*}\nf(y; \\theta) &= exp\\left[ -y\\beta + \\alpha \\log(\\beta) - \\log\\left( \\Gamma(\\alpha)\\right)+ \\alpha\\log(y) - \\log(y)\\right]\\\\\n&= \\exp[a(y)b(\\beta) + c(\\beta) + d(y)],\n\\end{align*}\n\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\beta) = -\\beta\\)\n\\(c(\\theta) = \\alpha \\log(\\beta) - \\log\\left( \\Gamma(\\alpha)\\right)\\)\n\\(d(y) = \\alpha\\log(y) - \\log(y)\\)\n\nYes, the Gamma distribution belongs to the exponential family. This will assist us in deriving expectation and variance.\nPart (b):\n\nDerive expectation of Y\n\nRecall that for any random variable \\(Y\\) with a distribution belonging to the exponential family, \\(E[a(y)] = -c'(\\theta)/b'(\\theta)\\). We can solve for these values and evaluate \\(E(a(y))\\), which is \\(E[Y]\\).\n\\[\\begin{align*}\n-c'(\\beta) &= - \\frac{d c(\\beta)}{d\\beta}\\\\\n&= -\\frac{d}{d\\beta}\\left[ \\alpha \\log(\\beta) - \\log\\left( \\Gamma(\\alpha)\\right)\\right]\\\\\n&= -\\frac{\\alpha}{\\beta}\\\\\nb'(\\beta) &= \\frac{d b(\\beta)}{d\\beta}\\\\\n&= \\frac{d}{d\\beta} - \\beta\\\\\n&= -1\\\\\n-c'(\\beta)/b'(\\beta) &= -\\frac{\\alpha}{\\beta}/-1\\\\\n&= \\frac{\\alpha}{\\beta}\n\\end{align*}\\]\nPart (c):\n\nDerive the variance of Y\n\nRecall another important property of the exponential family. For any random variable \\(Y\\) with a distribution belonging to the exponential family, \\(V[a(y)] = \\frac{b''(\\theta)c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\).\n\\[\\begin{align*}\nb'(\\beta) &= -1\\\\\nc'(\\beta) &= \\frac{\\alpha}{\\beta}\\\\\nb''(\\beta) &= 0\\\\\nc''(\\beta) &= -\\frac{\\alpha}{\\beta^2}\\\\\nV[a(y)] &= \\frac{b''(\\beta)c'(\\beta) - c''(\\beta)b'(\\beta)}{[b'(\\beta)]^3}\\\\\n&= \\frac{0 - \\frac{\\alpha}{\\beta^2}}{[-1]^3}\\\\\n&= \\frac{\\alpha}{\\beta^2}\n\\end{align*}\\]\nPart (d):\n\nDerive variance of the score statistic\n\nRecall that the variance of the score statistic, also known as the Fisher’s information, can be expressed as follows: \\(V[U] = I(\\theta) = \\frac{b''(\\theta) c'(\\theta)}{b'(\\theta)} - c''(\\theta)\\). Once again, we can plug and chug.\n\\[\\begin{align*}\nb'(\\beta) &= -1\\\\\nc'(\\beta) &= \\frac{\\alpha}{\\beta}\\\\\nb''(\\beta) &= 0\\\\\nc''(\\beta) &= -\\frac{\\alpha}{\\beta^2}\\\\\nV[U] &= \\frac{b''(\\beta)c'(\\beta) }{b'(\\beta)} - c''(\\beta)\\\\\n&= 0 - \\left( -\\frac{\\alpha}{\\beta^2}\\right)\\\\\n&= \\frac{\\alpha}{\\beta^2}\n\\end{align*}\\]"
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#problem-3",
    "href": "fall_24/b719/projects/hw_2.html#problem-3",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "Problem 3",
    "text": "Problem 3\n\nDerive expression for information (consider \\(\\theta\\) the parameter of interest) for a single observation from the Weibull distribution:\n\\[\nf(y; \\lambda , \\theta) = \\frac{\\lambda y^{\\lambda - 1}}{\\theta^{\\lambda}}\\exp\\left[ -\\left(\\frac{y}{\\theta}\\right)^{\\lambda}\\right],\n\\]\nwhere \\(y \\ge 0, \\lambda &gt; 0,\\) and \\(\\theta &gt; 0\\). Show all your work.\n\nLet’s start by identifying a(.), b(.), c(.), and d(.).\n\\[\\begin{align*}\nf(y; \\lambda, \\theta) &= \\frac{\\lambda y^{\\lambda - 1}}{\\theta^{\\lambda}}\\exp\\left[ -\\left(\\frac{y}{\\theta}\\right)^{\\lambda}\\right]\\\\\n&= \\exp\\left[- \\frac{y^{\\lambda}}{\\theta^{\\lambda}} -\\lambda \\log(\\theta) - \\log(y) + \\lambda\\log(y) + \\log(\\lambda)  \\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nwhere\n\n\\(a(y) = y^{\\lambda}\\)\n\\(b(\\theta) = -\\frac{1}{\\theta^{\\lambda}}\\)\n\\(c(\\theta) = -\\lambda \\log(\\theta)\\)\n\\(d(y) = - \\log(y) + \\lambda\\log(y) + \\log(\\lambda)\\)\n\nUsing our previously helpful formula for calculating Fisher’s information, we can derive an expression for \\(I(\\theta)\\):\n\\[\\begin{align*}\nb'(\\theta) &= \\lambda \\theta^{-\\lambda - 1}\\\\\nc'(\\theta) &= - \\lambda\\theta^{-1}\\\\\nb''(\\theta) &= (-\\lambda - 1)\\lambda\\theta^{-\\lambda - 2}\\\\\nc''(\\theta) &= \\lambda\\theta^{-2}\\\\\nI(\\theta) &= \\frac{b''(\\beta)c'(\\beta) }{b'(\\beta)} - c''(\\beta)\\\\\n&= \\frac{\\left[ (-\\lambda-1)\\lambda\\theta^{-\\lambda - 2}\\right]\\left[-\\lambda\\theta^{-1}\\right]}{\\lambda\\theta^{-\\lambda-1}} - \\lambda\\theta^{-2}\\\\\n&= (-\\lambda)(-\\lambda-1)(\\lambda)(\\lambda^{-1})(\\theta^{-\\lambda-3})(\\theta^{\\lambda+1})-\\lambda\\theta^{-2}\\\\\n&= (-\\lambda)(-\\lambda-1)(\\theta^{-2})-\\lambda\\theta^{-2}\\\\\n&= (\\lambda\\theta^{-2})(\\lambda+1)-\\lambda\\theta^{-2}\\\\\n&= \\lambda\\theta^{-2}((\\lambda+1)-1)\\\\\n&= \\lambda\\theta^{-2}(\\lambda)\\\\\n&= \\left(\\frac{\\lambda}{\\theta}\\right)^{2}\\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#problem-4",
    "href": "fall_24/b719/projects/hw_2.html#problem-4",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "Problem 4",
    "text": "Problem 4\n\nSuppose \\(Y_1, ..., Y_n\\) are independent random variables, each with the Pareto distribution:\n\\[\nf(y_i;\\theta) = \\frac{\\theta}{y_i^{\\theta + 1}}, \\text{ where } y_i &gt; 1, \\theta &gt; 0 \\text{ for all } i = 1, ..., n\n\\]\nand\n\\[\nE(Y_i) = (\\beta_0 + \\beta_1x_i)^2\n\\]\nPart (a): Does this distribution have the canonical form?\n\nLet’s first examine this PDF using the definition of the exponential family:\n\\[\n\\begin{align*}\nf(y_i; \\theta) &= \\frac{\\theta}{y_i^{\\theta + 1}}\\\\\n&= \\exp\\left[\\log(\\theta) - (\\theta + 1)\\log(y_i)\\right]\\\\\n&= \\exp\\left[- \\log(y_i)\\theta + \\log(\\theta) - \\log(y_i) \\right]\\\\\n&= \\exp\\left[a(y_i)b(\\theta) + c(\\theta) + d(y_i)\\right],\n\\end{align*}\n\\]\nwhere\n\n\\(a(y_i) = - \\log(y_i)\\)\n\\(b(\\theta) = \\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y_i) = -\\log(y_i)\\)\n\nBecause \\(a(y_i) \\ne y_i\\), this distribution does not have the canonical form.\n\nPart (b): Are the distributions of all the \\(Y_i\\)’s of the same form?\n\nBecause each \\(Y_i\\) follows the same distribution, \\(b_i(\\theta) = b(\\theta)\\), \\(c_i(\\theta) = c(\\theta)\\) and \\(d_i(y) = d(y)\\) for all \\(i = 1,2,...,n\\). This being the case, we know that all \\(Y_i\\)’s are of the same form.\n\nPart (c): What is the link function? Is it monotone and differentiable?\n\nThe link function is a transformation of \\(E(Y_i)\\) such that \\(g(E(Y_i)) = X_i^T\\beta\\). Because we know that \\(E(Y_i) = (X_i^T\\beta)^2\\), the link function is \\(\\sqrt{E(Y_i)}\\), which is both differentiable and monotone.\n\nPart (d): Given your answers in (a) to (c), is this a generalized linear model? Give reasons for your answer.\n\nWhile two of the three properties of GLMs are met by the pareto distribution, importantly, it does not have the canonical form. This is not a generalized linear model."
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#end-of-problem-set",
    "href": "fall_24/b719/projects/hw_2.html#end-of-problem-set",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "End of problem set",
    "text": "End of problem set\nBack"
  },
  {
    "objectID": "fall_24/b719/projects/hw_4.html",
    "href": "fall_24/b719/projects/hw_4.html",
    "title": "BIOSTAT 719 - Homework 3",
    "section": "",
    "text": "(10 points) Consider a 2 × 2 contingency table from a prospective study in which people who were or were not exposed to some pollutant (X) are followed up and, after several years, categorized according to the presence or absence of a disease (D). The following table shows counts for each cell.\n\n\n\n\nDiseased (D = 1)\nNot Diseased (D = 0)\n\n\n\n\nExposed (X = 1)\na\nb\n\n\nNot Exposed (X = 0)\nc\nd\n\n\n\nConsider a logistic regression model, \\(\\text{logit}(\\pi) = \\beta_0 + \\beta_1X\\), where π = P rob(D = 1). Utilizing the \\(X^TW X\\) expression for the information matrix (with appropriate matrix W), show that variance of MLE of \\(\\beta_1\\) (i.e. logarithm of OR) can be expressed as \\(\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\\).\n\nHow on earth are we going to tackle this one, you ask? I have no idea. But I do know that the variance of the MLE of \\(\\beta_1\\) is the \\((2,2)\\)’th element of the variance-covariance matrix, which is the inverse of the Information matrix. I also know that \\(I = X^TWX\\), which means we’re going to have to define \\(X\\) and \\(W\\) in terms of a, b, d, and d if we’re going to solve this problem.\nI’m going to start by thinking about the design matrix, \\(X\\). We have a single predictor, \\(X = \\begin{cases} 1, \\quad \\text{Exposed}\\\\0, \\quad \\text{Not exposed}\\end{cases}\\). This means that the design matrix will be a \\(n\\times 2\\) matrix that can be organized in the following way:\n\\[\nX = \\begin{bmatrix} 1 & X_1 = 1\\\\ \\vdots &\\vdots\\\\1 &X_{a + b} = 1\\\\1 &X_{a + b + 1} = 0\\\\\\vdots &\\vdots\\\\1 &X_{a + b + c + d} = 0 \\end{bmatrix}\n\\]\nWe also know that the the matrix \\(W\\) is a diagonal matrix where \\(w_{ii} = \\frac{1}{Var(Y_i)}\\left(\\frac{\\partial\\pi}{\\partial\\eta}\\right)^2\\). Because we’re dealing with logistic regression, we know that the link function is \\(g(\\pi_i) = \\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right)\\), which means \\(\\pi_i = \\frac{1}{1 + e^{-\\eta_i}} \\implies \\frac{\\partial\\pi}{\\partial\\eta} = \\frac{e^{-\\eta_i}}{(1 + e^{-\\eta_i})^2}\\).\nWe also know that each \\(Y_i \\sim Bern(\\pi) \\implies Var(Y_i) = \\pi(1 - \\pi) = \\frac{1}{1 + e^{-\\eta}} - \\frac{1}{(1 + e^{-\\eta})^2} \\implies \\frac{1}{Var(Y_i)} = \\frac{e^{-\\eta_i}}{(1 + e^{-\\eta_i})^2}\\). Because \\(\\frac{1}{X}\\left(X\\right)^2 = X\\), \\(w_{ii} = \\frac{1}{Var(Y_i)}\\left(\\frac{\\partial\\pi}{\\partial\\eta}\\right)^2 = \\frac{1}{Var(Y_i)}\\left(Var(Y_i)\\right)^2 = Var(Y_i) = \\pi_i(1 - \\pi_i)\\). Super cool.\nLet’s now see if we can derive a formula for the information matrix, \\(I\\).\n\\[\\begin{align*}\nI &= X^T W X\\\\\n&= \\begin{bmatrix}\n1 & \\cdots  & 1 \\\\\nX_1 = 1 & \\cdots & X_{a + b + c + d} = 0\n\\end{bmatrix}\n\\begin{bmatrix}\nw_{11} & 0 & \\cdots & 0\\\\\n0 & w_{22} & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 &\\cdots & w_{nn}\n\\end{bmatrix}\n\\begin{bmatrix} 1 & X_1 = 1\\\\\n\\vdots &\\vdots\\\\\n1 &X_{a + b + c + d} = 0\n\\end{bmatrix} \\quad (\\text{Note: } a + b + c + d = n)\\\\\n&= \\begin{bmatrix}\nw_{11} & \\cdots  &  w_{(a + b, a + b)} & w_{(a + b + 1, a + b + 1)} & \\cdots &w_{nn}\\\\\nw_{11}  & \\cdots & w_{(a + b, a + b)} & 0 & \\cdots& 0\n\\end{bmatrix}\n\\begin{bmatrix} 1 & X_1 = 1\\\\ \\vdots &\\vdots\\\\1 &X_{a + b} = 1\\\\1 &X_{a + b + 1} = 0\\\\\\vdots &\\vdots\\\\1 &X_{a + b + c + d} = 0 \\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\sum_{i = 1}^n w_{ii} &\n\\sum_{i = 1}^{a + b} w_{ii}\\\\\n\\sum_{i = 1}^{a + b} w_{ii} &\n\\sum_{i = 1}^{a + b} w_{ii}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\sum_{i = 1}^n \\pi_i(1 - \\pi_i) &\n\\sum_{i = 1}^{a + b} \\pi_i(1 - \\pi_i)\\\\\n\\sum_{i = 1}^{a + b} \\pi_i(1 - \\pi_i) &\n\\sum_{i = 1}^{a + b} \\pi_i(1 - \\pi_i)\n\\end{bmatrix}\n\\end{align*}\\]\nWe can further simplify this when we refer back to the contingency table. Because we are interested in the probability \\(P(D = 1)\\), and because there are only two possible values for \\(X \\in \\{0, 1\\}\\), we only have two possible values for \\(\\pi_i\\). Let us define \\(\\pi_0 = P(D = 1| X = 0)\\), and \\(\\pi_1 = P(D = 1 | X = 1)\\). Thus, for \\(Y_{i \\in [a, b]}\\), \\(\\pi_i = \\pi_1\\), and for \\(Y_{i \\in [a + b + 1, a + b + c + d]}\\), \\(\\pi_i = \\pi_0\\). Thus, the information matrix can be expressed as follows:\n\\[\nI = \\begin{bmatrix}(a + b)\\pi_1 + (c + d)\\pi_0 & (a + b)\\pi_1\\\\(a + b)\\pi_1 &(a + b)\\pi_1\\end{bmatrix}\n\\]\nTo find the inverse of this matrix, let’s first calculate the determinant:\n\\[\\begin{align*}\ndet(I) &= \\frac{1}{cell_{1,1}cell_{2,2} - cell_{1,2}cell_{2,1}}\\\\\n&= \\frac{1}{\\pi_1\\pi_0(a + b)(c  + d)}\\\\\n\\end{align*}\\]\nNow we’re ready to calculate the inverse of this matrix, which will yield the variance-covariance matrix for \\(\\beta_0\\) and \\(\\beta_1\\) (note that \\(\\pi_1 = a/(a + b)\\) and \\(\\pi_0 = c/(c + d)\\)):\n\\[\\begin{align*}\nI^{-1} &= \\det(I)\n\\begin{bmatrix}\n\\text{cell}_{2,2} & -\\text{cell}_{1,2} \\\\\n-\\text{cell}_{2,1} & \\text{cell}_{1,1}\n\\end{bmatrix}\\\\\n&= \\frac{1}{(a + b)(c + d)\\pi_1 \\pi_0}\n\\begin{bmatrix}\n(a + b)\\pi_1  & -(a + b)\\pi_1 \\\\\n-(a + b)\\pi_1 & (a + b)\\pi_1+ (c + d)\\pi_0\n\\end{bmatrix}\\\\\n&= \\frac{1}{(a + b)(c + d)\\pi_1 \\pi_0}\n\\begin{bmatrix}\n(a + b)\\frac{a}{a + b}  & -(a + b)\\frac{a}{a + b} \\\\\n-(a + b)\\frac{a}{a + b} & (a + b)\\frac{a}{a + b}+ (c + d)\\frac{c}{c + d}\n\\end{bmatrix}\\\\\n&= \\frac{1}{(a + b)(c + d)\\pi_1 \\pi_0}\n\\begin{bmatrix}\na  & -a \\\\\n-a & a + c\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\frac{1}{c}  & -\\frac{1}{c}  \\\\\n-\\frac{1}{c}  & \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\n\\end{bmatrix}\n\\end{align*}\\]\nThe variance of \\(\\beta_1\\) is found in the bottom-right cell. Thus, \\(Var(\\beta_1) = \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\\)."
  },
  {
    "objectID": "fall_24/b719/projects/hw_4.html#problem-1",
    "href": "fall_24/b719/projects/hw_4.html#problem-1",
    "title": "BIOSTAT 719 - Homework 3",
    "section": "",
    "text": "(10 points) Consider a 2 × 2 contingency table from a prospective study in which people who were or were not exposed to some pollutant (X) are followed up and, after several years, categorized according to the presence or absence of a disease (D). The following table shows counts for each cell.\n\n\n\n\nDiseased (D = 1)\nNot Diseased (D = 0)\n\n\n\n\nExposed (X = 1)\na\nb\n\n\nNot Exposed (X = 0)\nc\nd\n\n\n\nConsider a logistic regression model, \\(\\text{logit}(\\pi) = \\beta_0 + \\beta_1X\\), where π = P rob(D = 1). Utilizing the \\(X^TW X\\) expression for the information matrix (with appropriate matrix W), show that variance of MLE of \\(\\beta_1\\) (i.e. logarithm of OR) can be expressed as \\(\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\\).\n\nHow on earth are we going to tackle this one, you ask? I have no idea. But I do know that the variance of the MLE of \\(\\beta_1\\) is the \\((2,2)\\)’th element of the variance-covariance matrix, which is the inverse of the Information matrix. I also know that \\(I = X^TWX\\), which means we’re going to have to define \\(X\\) and \\(W\\) in terms of a, b, d, and d if we’re going to solve this problem.\nI’m going to start by thinking about the design matrix, \\(X\\). We have a single predictor, \\(X = \\begin{cases} 1, \\quad \\text{Exposed}\\\\0, \\quad \\text{Not exposed}\\end{cases}\\). This means that the design matrix will be a \\(n\\times 2\\) matrix that can be organized in the following way:\n\\[\nX = \\begin{bmatrix} 1 & X_1 = 1\\\\ \\vdots &\\vdots\\\\1 &X_{a + b} = 1\\\\1 &X_{a + b + 1} = 0\\\\\\vdots &\\vdots\\\\1 &X_{a + b + c + d} = 0 \\end{bmatrix}\n\\]\nWe also know that the the matrix \\(W\\) is a diagonal matrix where \\(w_{ii} = \\frac{1}{Var(Y_i)}\\left(\\frac{\\partial\\pi}{\\partial\\eta}\\right)^2\\). Because we’re dealing with logistic regression, we know that the link function is \\(g(\\pi_i) = \\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right)\\), which means \\(\\pi_i = \\frac{1}{1 + e^{-\\eta_i}} \\implies \\frac{\\partial\\pi}{\\partial\\eta} = \\frac{e^{-\\eta_i}}{(1 + e^{-\\eta_i})^2}\\).\nWe also know that each \\(Y_i \\sim Bern(\\pi) \\implies Var(Y_i) = \\pi(1 - \\pi) = \\frac{1}{1 + e^{-\\eta}} - \\frac{1}{(1 + e^{-\\eta})^2} \\implies \\frac{1}{Var(Y_i)} = \\frac{e^{-\\eta_i}}{(1 + e^{-\\eta_i})^2}\\). Because \\(\\frac{1}{X}\\left(X\\right)^2 = X\\), \\(w_{ii} = \\frac{1}{Var(Y_i)}\\left(\\frac{\\partial\\pi}{\\partial\\eta}\\right)^2 = \\frac{1}{Var(Y_i)}\\left(Var(Y_i)\\right)^2 = Var(Y_i) = \\pi_i(1 - \\pi_i)\\). Super cool.\nLet’s now see if we can derive a formula for the information matrix, \\(I\\).\n\\[\\begin{align*}\nI &= X^T W X\\\\\n&= \\begin{bmatrix}\n1 & \\cdots  & 1 \\\\\nX_1 = 1 & \\cdots & X_{a + b + c + d} = 0\n\\end{bmatrix}\n\\begin{bmatrix}\nw_{11} & 0 & \\cdots & 0\\\\\n0 & w_{22} & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 &\\cdots & w_{nn}\n\\end{bmatrix}\n\\begin{bmatrix} 1 & X_1 = 1\\\\\n\\vdots &\\vdots\\\\\n1 &X_{a + b + c + d} = 0\n\\end{bmatrix} \\quad (\\text{Note: } a + b + c + d = n)\\\\\n&= \\begin{bmatrix}\nw_{11} & \\cdots  &  w_{(a + b, a + b)} & w_{(a + b + 1, a + b + 1)} & \\cdots &w_{nn}\\\\\nw_{11}  & \\cdots & w_{(a + b, a + b)} & 0 & \\cdots& 0\n\\end{bmatrix}\n\\begin{bmatrix} 1 & X_1 = 1\\\\ \\vdots &\\vdots\\\\1 &X_{a + b} = 1\\\\1 &X_{a + b + 1} = 0\\\\\\vdots &\\vdots\\\\1 &X_{a + b + c + d} = 0 \\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\sum_{i = 1}^n w_{ii} &\n\\sum_{i = 1}^{a + b} w_{ii}\\\\\n\\sum_{i = 1}^{a + b} w_{ii} &\n\\sum_{i = 1}^{a + b} w_{ii}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\sum_{i = 1}^n \\pi_i(1 - \\pi_i) &\n\\sum_{i = 1}^{a + b} \\pi_i(1 - \\pi_i)\\\\\n\\sum_{i = 1}^{a + b} \\pi_i(1 - \\pi_i) &\n\\sum_{i = 1}^{a + b} \\pi_i(1 - \\pi_i)\n\\end{bmatrix}\n\\end{align*}\\]\nWe can further simplify this when we refer back to the contingency table. Because we are interested in the probability \\(P(D = 1)\\), and because there are only two possible values for \\(X \\in \\{0, 1\\}\\), we only have two possible values for \\(\\pi_i\\). Let us define \\(\\pi_0 = P(D = 1| X = 0)\\), and \\(\\pi_1 = P(D = 1 | X = 1)\\). Thus, for \\(Y_{i \\in [a, b]}\\), \\(\\pi_i = \\pi_1\\), and for \\(Y_{i \\in [a + b + 1, a + b + c + d]}\\), \\(\\pi_i = \\pi_0\\). Thus, the information matrix can be expressed as follows:\n\\[\nI = \\begin{bmatrix}(a + b)\\pi_1 + (c + d)\\pi_0 & (a + b)\\pi_1\\\\(a + b)\\pi_1 &(a + b)\\pi_1\\end{bmatrix}\n\\]\nTo find the inverse of this matrix, let’s first calculate the determinant:\n\\[\\begin{align*}\ndet(I) &= \\frac{1}{cell_{1,1}cell_{2,2} - cell_{1,2}cell_{2,1}}\\\\\n&= \\frac{1}{\\pi_1\\pi_0(a + b)(c  + d)}\\\\\n\\end{align*}\\]\nNow we’re ready to calculate the inverse of this matrix, which will yield the variance-covariance matrix for \\(\\beta_0\\) and \\(\\beta_1\\) (note that \\(\\pi_1 = a/(a + b)\\) and \\(\\pi_0 = c/(c + d)\\)):\n\\[\\begin{align*}\nI^{-1} &= \\det(I)\n\\begin{bmatrix}\n\\text{cell}_{2,2} & -\\text{cell}_{1,2} \\\\\n-\\text{cell}_{2,1} & \\text{cell}_{1,1}\n\\end{bmatrix}\\\\\n&= \\frac{1}{(a + b)(c + d)\\pi_1 \\pi_0}\n\\begin{bmatrix}\n(a + b)\\pi_1  & -(a + b)\\pi_1 \\\\\n-(a + b)\\pi_1 & (a + b)\\pi_1+ (c + d)\\pi_0\n\\end{bmatrix}\\\\\n&= \\frac{1}{(a + b)(c + d)\\pi_1 \\pi_0}\n\\begin{bmatrix}\n(a + b)\\frac{a}{a + b}  & -(a + b)\\frac{a}{a + b} \\\\\n-(a + b)\\frac{a}{a + b} & (a + b)\\frac{a}{a + b}+ (c + d)\\frac{c}{c + d}\n\\end{bmatrix}\\\\\n&= \\frac{1}{(a + b)(c + d)\\pi_1 \\pi_0}\n\\begin{bmatrix}\na  & -a \\\\\n-a & a + c\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\frac{1}{c}  & -\\frac{1}{c}  \\\\\n-\\frac{1}{c}  & \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\n\\end{bmatrix}\n\\end{align*}\\]\nThe variance of \\(\\beta_1\\) is found in the bottom-right cell. Thus, \\(Var(\\beta_1) = \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\\)."
  },
  {
    "objectID": "fall_24/b719/projects/hw_4.html#problem-2",
    "href": "fall_24/b719/projects/hw_4.html#problem-2",
    "title": "BIOSTAT 719 - Homework 3",
    "section": "Problem 2",
    "text": "Problem 2\n(5 points) Consider data with 100 subjects evenly split between men and women. Probability of disease (π) is less than 1/2 for men and less than 1/2 for women. The following logistic regression model was considered:\n\\[\n\\text{logit} (\\pi) = \\beta_0 + \\beta_1 \\cdot \\text{sex},\n\\]\nwith sex coded 1 for female and 0 for male. The information matrix evaluated at the maximum likelihood estimates has the first column equal to \\([18.5, 8.0]^T\\)\nCompute number of diseased men and number of diseased women.\n[Hint: Use the derived information matrix in Question 1]\nI contingency tables helpful. Let’s construct one for this problem:\n\n\n\n\nDiseased (D = 1)\nNot Diseasee (D = 0)\n\n\n\n\nFemale\na\nb\n\n\nMale\nc\nd\n\n\n\nRecall that the information matrix is \\(I = \\begin{bmatrix}(a + b)\\pi_1 + (c + d)\\pi_0 & (a + b)\\pi_1\\\\(a + b)\\pi_1 &(a + b)\\pi_1 \\end{bmatrix} = \\begin{bmatrix}a + c & a\\\\a &a \\end{bmatrix}\\). Thus, \\(a = 8\\) and \\(a + c = 18.5\\), which means that \\(c = 10.5\\). However, interpreting this is obviously proposterous. There very well may have been 8 diseased women, but it doesn’t make sense to have 10.5 diseased men. This means there is an error somewhere, but I haven’t been able to find it."
  },
  {
    "objectID": "fall_24/b823/notes/notes.html",
    "href": "fall_24/b823/notes/notes.html",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "",
    "text": "September\n\nWeek of September 3, 2024 - September 5, 2024\nWeek of September 10, 2024 - September 12, 2024\n\n\n\nOctober\n\n\nNovember"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html",
    "href": "fall_24/b823/notes/september/sep_3.html",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Relational models are built on first-order predicate logic. What does that mean? Let’s ask ChatGPT.\n\nAI Summary of first-order predicate logic from ChatGPT: First-order predicate logic (FOPL), also known as first-order logic (FOL), is a formal system used in mathematics, philosophy, and computer science to express statements about objects and their relationships using quantifiers (like “for all” and “there exists”) and predicates that denote properties or relations between objects. In relational data modeling, FOPL is applied to define the structure and constraints of data by expressing rules and conditions that must hold true for data entries in a database. For instance, it can be used to specify relationships between entities (like “every employee works for a department”) and enforce constraints such as uniqueness, referential integrity, or conditional dependencies, ensuring that the data adheres to the logical rules of the modeled domain.\n\nRelations are tables; tuples are rows\n\n\n\n\n\nAn entity is an identifiable “thing” of interest in the domain of interest. For example, an entity in a retail buisiness domain is a “customer.” A “Customer” is an identifiable “thing” of interest, represented by attributes like CustomerID (a unique identifier), Name, Email, and Address, which uniquely identify and describe each customer and relate to other entities, such as “Orders” or “Products.” The set of attributes that uniquely identify the entity is the natural key.\nThe E-R model focuses on the relationships betwen entities\nIn relational data modeling, there are two main stages of database design: the logical model and the physical model\n\nLogical Model: This defines the structure of the data independent of how it will be physically stored. It focuses on what data needs to be stored, the relationships between different entities (like “Customer” and “Order”), and the rules or constraints (like uniqueness or referential integrity). The logical model is typically represented using entity-relationship diagrams (ERDs) or similar notations that show entities, attributes, and relationships without concern for specific technical details.\nPhysical Model: This represents how the data will be stored on the database system. It translates the logical model into tables, columns, indexes, and keys, and considers performance, storage optimization, and access methods. The physical model depends on the specific database management system (DBMS) used (like MySQL, PostgreSQL, or Oracle) and addresses how data is actually stored on disk, how it will be indexed, and how queries will be optimized.\n\n\n\n\n\n\nHere are the three main relationship cardinalities:\n\n\\(\\{0, 1\\}:1\\) (0 or 1 to 1)\n\\(\\{0, 1\\}:n\\) (0 or 1 to many)\n\\(n:n\\) (many to many)\n\nCrow’s Feet: I’m not going to try to draw the crow’s feet, but they’re essentially a notation to write the relationship cardinalities\nResolving \\(n:n\\) relationships:\n\nSuppose we have a table of instructors that reference a table of courses (assume each course can have multiple instructors, like Gennevieve and Zeck). This is a many-to-many relationship problem. This is a problem (I believe) because the many-to-many relationships can’t be represented directly using just foreign keys in a relational database.\nTo solve this problem, you have a middle-man table. In this “junction” table (or whatever you call it), you have one-to-many relationship pointing at both the course table and the instructor table. If you know that only one professor will teach certain lessons of the course, you can have the middle-man table be a “lessons” table. Just know that “instructor” and “course” are considered strong entities, but “lessons” are considered weak because they only exist in the context of a course and an instructor.\n\n\n\n\n\n\nForeign keys\n\nThere are implicit foreign keys and explicit foreign keys. I don’t totally understand how implicit keys work, but essentially if the instructor teaches certain lessons and those lessons are part of a specific course, then the foreign key is explicit if the lesson doesn’t have any information about the instructor or the course. That’s why I don’t understand how they work. Regardless, if the lesson does have the instructor’s ID and the course ID, then the foreign key is explicit\n\nPrimary keys\n\nA surrogate primary key is a kind of like a local variable. It would the the “ID” in the table that’s generated by the system (e.g. autoincrement).\nA natural key is a unique identifier that’s more like a global variable (i.e. isn’t only contained in the database). An email address is a good example of a natural key.\n\nNote that a composite key is two or more attributes that uniquely identify an entity\n\n\n\n\n\nNormalization is used to minimize redundancy and improve data integrity (good things)\nFirst normal form\n\nDefinition: A relation is in first normal form iff no attribute domain has relations as elements.\nWhat the heck does that mean? Basically, all attributes have to be “atomic,” i.e. must not be tables, lists, arrays, etc.\n\nSecond normal form\n\nDefinition: A relation is in 2NF iff it is in 1NF and it does not have any non-prime attribute functionally dependent on any proper subset of any candidate key of the relation.\nCome again? Apart from being first normal, if any table has a composite natural key (more than one element), no column in that table depends on only a part of the composite key.\n\nThird normal form\n\nDefinition: A relation R is in 3NF iff it is in 2NF and every non-prime attribute of R is non-transitively dependent on every key of R.\nOne more time: I don’t know what this means. But here’s what Hilmar said: For every table, any attribute that is not part of a natural key depends directly on every key for the table.\n\nThe last thing I’ll mention is that Hilmar said that you definitely want your database in first normal form, preferably second, but you can sometimes get away with not having it in third\n\n\n\n\n\n\nAn entity is a thing of interest in a domain of interest that has attributes. The set of attributes that uniquely identify the entity is the natural key.\nIn the example of the instructors and courses, the lessons table acted as a middle-man or reference table that changed the many-to-many relationship problem into two one-to-many relationships.\nA primary key identifies an entity in the table; a foreign key identifies an entity in another table; a composite natural key is a set of two or more attributes that uniquely identify an entity; a surrogate key is like a local variable that is only understood by the database and doesn’t exist in the real world (like using “autoincrement”)"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#relational-data-modeling",
    "href": "fall_24/b823/notes/september/sep_3.html#relational-data-modeling",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Relational models are built on first-order predicate logic. What does that mean? Let’s ask ChatGPT.\n\nAI Summary of first-order predicate logic from ChatGPT: First-order predicate logic (FOPL), also known as first-order logic (FOL), is a formal system used in mathematics, philosophy, and computer science to express statements about objects and their relationships using quantifiers (like “for all” and “there exists”) and predicates that denote properties or relations between objects. In relational data modeling, FOPL is applied to define the structure and constraints of data by expressing rules and conditions that must hold true for data entries in a database. For instance, it can be used to specify relationships between entities (like “every employee works for a department”) and enforce constraints such as uniqueness, referential integrity, or conditional dependencies, ensuring that the data adheres to the logical rules of the modeled domain.\n\nRelations are tables; tuples are rows\n\n\n\n\n\nAn entity is an identifiable “thing” of interest in the domain of interest. For example, an entity in a retail buisiness domain is a “customer.” A “Customer” is an identifiable “thing” of interest, represented by attributes like CustomerID (a unique identifier), Name, Email, and Address, which uniquely identify and describe each customer and relate to other entities, such as “Orders” or “Products.” The set of attributes that uniquely identify the entity is the natural key.\nThe E-R model focuses on the relationships betwen entities\nIn relational data modeling, there are two main stages of database design: the logical model and the physical model\n\nLogical Model: This defines the structure of the data independent of how it will be physically stored. It focuses on what data needs to be stored, the relationships between different entities (like “Customer” and “Order”), and the rules or constraints (like uniqueness or referential integrity). The logical model is typically represented using entity-relationship diagrams (ERDs) or similar notations that show entities, attributes, and relationships without concern for specific technical details.\nPhysical Model: This represents how the data will be stored on the database system. It translates the logical model into tables, columns, indexes, and keys, and considers performance, storage optimization, and access methods. The physical model depends on the specific database management system (DBMS) used (like MySQL, PostgreSQL, or Oracle) and addresses how data is actually stored on disk, how it will be indexed, and how queries will be optimized.\n\n\n\n\n\n\nHere are the three main relationship cardinalities:\n\n\\(\\{0, 1\\}:1\\) (0 or 1 to 1)\n\\(\\{0, 1\\}:n\\) (0 or 1 to many)\n\\(n:n\\) (many to many)\n\nCrow’s Feet: I’m not going to try to draw the crow’s feet, but they’re essentially a notation to write the relationship cardinalities\nResolving \\(n:n\\) relationships:\n\nSuppose we have a table of instructors that reference a table of courses (assume each course can have multiple instructors, like Gennevieve and Zeck). This is a many-to-many relationship problem. This is a problem (I believe) because the many-to-many relationships can’t be represented directly using just foreign keys in a relational database.\nTo solve this problem, you have a middle-man table. In this “junction” table (or whatever you call it), you have one-to-many relationship pointing at both the course table and the instructor table. If you know that only one professor will teach certain lessons of the course, you can have the middle-man table be a “lessons” table. Just know that “instructor” and “course” are considered strong entities, but “lessons” are considered weak because they only exist in the context of a course and an instructor.\n\n\n\n\n\n\nForeign keys\n\nThere are implicit foreign keys and explicit foreign keys. I don’t totally understand how implicit keys work, but essentially if the instructor teaches certain lessons and those lessons are part of a specific course, then the foreign key is explicit if the lesson doesn’t have any information about the instructor or the course. That’s why I don’t understand how they work. Regardless, if the lesson does have the instructor’s ID and the course ID, then the foreign key is explicit\n\nPrimary keys\n\nA surrogate primary key is a kind of like a local variable. It would the the “ID” in the table that’s generated by the system (e.g. autoincrement).\nA natural key is a unique identifier that’s more like a global variable (i.e. isn’t only contained in the database). An email address is a good example of a natural key.\n\nNote that a composite key is two or more attributes that uniquely identify an entity\n\n\n\n\n\nNormalization is used to minimize redundancy and improve data integrity (good things)\nFirst normal form\n\nDefinition: A relation is in first normal form iff no attribute domain has relations as elements.\nWhat the heck does that mean? Basically, all attributes have to be “atomic,” i.e. must not be tables, lists, arrays, etc.\n\nSecond normal form\n\nDefinition: A relation is in 2NF iff it is in 1NF and it does not have any non-prime attribute functionally dependent on any proper subset of any candidate key of the relation.\nCome again? Apart from being first normal, if any table has a composite natural key (more than one element), no column in that table depends on only a part of the composite key.\n\nThird normal form\n\nDefinition: A relation R is in 3NF iff it is in 2NF and every non-prime attribute of R is non-transitively dependent on every key of R.\nOne more time: I don’t know what this means. But here’s what Hilmar said: For every table, any attribute that is not part of a natural key depends directly on every key for the table.\n\nThe last thing I’ll mention is that Hilmar said that you definitely want your database in first normal form, preferably second, but you can sometimes get away with not having it in third"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#three-take-aways",
    "href": "fall_24/b823/notes/september/sep_3.html#three-take-aways",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "An entity is a thing of interest in a domain of interest that has attributes. The set of attributes that uniquely identify the entity is the natural key.\nIn the example of the instructors and courses, the lessons table acted as a middle-man or reference table that changed the many-to-many relationship problem into two one-to-many relationships.\nA primary key identifies an entity in the table; a foreign key identifies an entity in another table; a composite natural key is a set of two or more attributes that uniquely identify an entity; a surrogate key is like a local variable that is only understood by the database and doesn’t exist in the real world (like using “autoincrement”)"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#follow-up-question-about-implicit-foreign-keys",
    "href": "fall_24/b823/notes/september/sep_3.html#follow-up-question-about-implicit-foreign-keys",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Follow-up Question About Implicit Foreign Keys",
    "text": "Follow-up Question About Implicit Foreign Keys\nAfter last lecture, I had the following question that was answered today:\n\nQuestion: We talked about implicit foreign keys and explicit foreign keys. If the foreign key is implicit, how on the good green earth would you be able to reference the foreign entity?\nAnswer: Foreign keys are only implicit in the logical model. In the physical implementation, they most certainly are explicit."
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#sql---introduction",
    "href": "fall_24/b823/notes/september/sep_3.html#sql---introduction",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "SQL - Introduction",
    "text": "SQL - Introduction\n\nYou can categorize SQL commands into four groups:\n\nData Definition Language\n\nThis is for executing tasks like creating a table and defining columns\n\nData Manipulation Language\n\nThis is for updating the tables/rows after they’re created\n\nData Query Language\n\nThis is for getting data\n\nData Control Language\n\nI’m not sure what this does yet…\n\n\nKnitr supports a variety of “engines,” including the SQL engine\n\nQuestion: is an engine like a kernal?\n\nIn R, DBI is a handy package to remember. It can be used to create a database connection and work with datatables.\n\nYou can choose to create an in-memory database using “:memory:” as an argument. Why would we want to create an in-memory database? What’s the advantage over a set of dataframes or datatables?\n\nI just asked ChatGPT this same question, and here’s the response:\n\nAn in-memory database in R, created using DBI with “:memory:”, offers several advantages over data frames or data tables. It provides faster data access by using RAM instead of disk storage, allows complex SQL queries and joins, supports transactional consistency, and efficiently manages memory allocation. In-memory databases are ideal for temporary data analysis, reducing disk I/O overhead, and can handle larger datasets more effectively, offering a powerful alternative for data manipulation tasks in R.\n\n\n\n\nDDL (“D-efinition”)\n\nHere are some common verbs:\n\nCREATE/DROP/ALTER TABLE\nCREATE/DROP VIEW\nCREATE/DROP INDEX\n\nDDL consists of SQL commands that return a status or a count, but not a result set\nWhen creating a table, you can define a foreign key in a couple ways:\n\nThe first way is to specify that it’s a foreign key when you create the column name:\n\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64),   Instructor_OID INTEGER NOT NULL       -- foreign key constraint can be part of column definition       REFERENCES Instructor (Instructor_OID),   PRIMARY KEY (Name, Instructor_OID) );\n\nThe second way is to do it in two statements:\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64) NOT NULL,   Instructor_OID INTEGER NOT NULL   -- foreign key constraint can be defined separately   FOREIGN KEY (Instructor_OID)       REFERENCES Instructor (Instructor_OID)   PRIMARY KEY (Name, Instructor_OID) );\n\nKeep in mind that there are three ways to specify how the RDBMS handles a deletion of the foreign entity (instructor). The first way is set the value to NULL, but in this case, where Instructor_OID is part of the composite natural key (and cannot be null anyway), that would create an error. The second way (which is the default) is to restrict, which means if the instructor is deleted and his Instructor_OID is found somewhere as a foreign key in the Lesson table, then an error will arise. This is how it’s specified:\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64),   Instructor_OID INTEGER NOT NULL       -- foreign key constraint can be part of column definition       REFERENCES Instructor (Instructor_OID)       ON DELETE RESTRICT,   PRIMARY KEY (Name, Instructor_OID) );\n\nThe last way is to use CASCADE, which relies on this logic. If the professor teaching the lesson doesn’t exist, and if that implies that the lesson no longer exists, then just remove the lesson! So if the professor is deleted, the lesson is automatically deleted. So be careful when using CASCADE! Here’s how you specify it:\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64),   Instructor_OID INTEGER NOT NULL       -- foreign key constraint can be part of column definition       REFERENCES Instructor (Instructor_OID)       ON DELETE CASCADE,   PRIMARY KEY (Name, Instructor_OID) );\n\nOne last thing: For “weak” entities (Lessons), the natural primary key will include a foreign key\nCreating an index can speed up a query, but they also increase the transaction cost\nAltering tables: just know it’s a thing with lots of documentation, but we’re not going to get into the nitty gritty of it in this course\n\n\n\nDML (“M-anagement”)\n\nCommon DML commands:\n\nINSERT\nUPDATE\nDELETE\n\nReturns a status or a count\nUPDATE and DELETE don’t raise an error if the query isn’t matched, so look for the count that it returns!\n\n\n\nDQL (“Q-ery”)\n\nSelect statement consists of:\n\nSELECT: which columns (or values) to report\nFROM: which table(s) to query and how to join tables\nWHERE: conditions to be met for rows to be reported\nGROUP BY: how to aggregate rows by certain columns\nHAVING: conditions to be met for aggregated rows\nORDER BY: how to order the rows in the report\n\nI thought this subquery example was kind of neat:\n\nSELECT DISTINCT Name FROM Lesson\nWHERE Instructor_OID IN (\n   SELECT Instructor_OID\n   FROM Instructor WHERE Name LIKE 'h%'\n)"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#three-take-aways-1",
    "href": "fall_24/b823/notes/september/sep_3.html#three-take-aways-1",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nWhen creating tables that have foreign keys, it is often important to impose constraints on the foreign key. One of those constraints is to ensure the proper handling of the current entity in the case the foreign entity were deleted. Write the SQL command to delete the current entity if the foreign entity is deleted.\nWhen creating tables that have foreign keys, it is often important to impose constraints on the foreign key. One of those constraints is to ensure the proper handling of the current entity in the case the foreign entity were deleted. Write the SQL command to raise an error if the foreign entity is deleted.\nFor any “weak” entity, what will always be included in the natural primary key?\n\nBack"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW1-Relmod.html",
    "href": "fall_24/b823/projects/downloads/HW1-Relmod.html",
    "title": "Biostat 823 - Fall 2024: Homework 1",
    "section": "",
    "text": "Note: The two subquestions below both require you to create diagrams. You can use a diagramming tool of your choice, or you can use the same Rmarkdown-supported “plugin” (mermaid) used in the course slides. (BTW you can use mermaid also in some plain Markdown renderers, such as Hackmd.io. For example, here’s the E-R diagram from the course slides in Hackmd.io.) If you choose to use a scanned copy of your handwriting, make sure it’s clear and readable.\n\n(10 points) Create an E-R diagram to represent a physical data model for the following scenario, and use crow’s foot notation to show relationship cardinalities (similar to page 11 in module “Relational Data Modeling”).\nScenario: Cancer patients who are receiving routine chemotheapies under certain treatment guidelines (plans).\nRemark 1: In clinical practice, there are usually existing mature treatment guidelines for treating a certain type of cancer. We focus on guidelines for chemotherapy. The treatment guidelines consist of the number of sessions, the chemotherapeutic drugs for each session, and how the drugs are administrated for each session (e.g., injection or taken orally).\nRemark 2: Choose a set of entities to represent the data you would like to collect at a minimum and explain your choices. You don’t have to be very comprehensive.\n(15 points) Normalize the following database of inpatients to relational models in 1NF, 2NF, and 3NF. Create physical models (not logical ones); however, ignore indexes etc. (Note: Diagnosis depends on symptoms, and treatment is prescribed based on diagnosis.)\n\n\nInpatient database (not normalized)\n\n\n\n\n\n\nDatatype\nField\n\n\n\n\nString\nName\n\n\nString\nRoom\n\n\nInteger\nAge\n\n\nArray\nDoctors\n\n\nArray\nSymptons\n\n\nString\nDiagnosis\n\n\nArray\nTreatments"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-1-25-points",
    "href": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-1-25-points",
    "title": "Biostat 823 - Fall 2024: Homework 1",
    "section": "",
    "text": "Note: The two subquestions below both require you to create diagrams. You can use a diagramming tool of your choice, or you can use the same Rmarkdown-supported “plugin” (mermaid) used in the course slides. (BTW you can use mermaid also in some plain Markdown renderers, such as Hackmd.io. For example, here’s the E-R diagram from the course slides in Hackmd.io.) If you choose to use a scanned copy of your handwriting, make sure it’s clear and readable.\n\n(10 points) Create an E-R diagram to represent a physical data model for the following scenario, and use crow’s foot notation to show relationship cardinalities (similar to page 11 in module “Relational Data Modeling”).\nScenario: Cancer patients who are receiving routine chemotheapies under certain treatment guidelines (plans).\nRemark 1: In clinical practice, there are usually existing mature treatment guidelines for treating a certain type of cancer. We focus on guidelines for chemotherapy. The treatment guidelines consist of the number of sessions, the chemotherapeutic drugs for each session, and how the drugs are administrated for each session (e.g., injection or taken orally).\nRemark 2: Choose a set of entities to represent the data you would like to collect at a minimum and explain your choices. You don’t have to be very comprehensive.\n(15 points) Normalize the following database of inpatients to relational models in 1NF, 2NF, and 3NF. Create physical models (not logical ones); however, ignore indexes etc. (Note: Diagnosis depends on symptoms, and treatment is prescribed based on diagnosis.)\n\n\nInpatient database (not normalized)\n\n\n\n\n\n\nDatatype\nField\n\n\n\n\nString\nName\n\n\nString\nRoom\n\n\nInteger\nAge\n\n\nArray\nDoctors\n\n\nArray\nSymptons\n\n\nString\nDiagnosis\n\n\nArray\nTreatments"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-2-25-points",
    "href": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-2-25-points",
    "title": "Biostat 823 - Fall 2024: Homework 1",
    "section": "Question 2 (25 points)",
    "text": "Question 2 (25 points)\nSuppose you are implementing an E-R model for the following scenario:\n\n\n\n\n\n\nerDiagram\n    Room       ||--o{ Student : \"accommodates\"\n    Building   ||--|{ Room    : \"consists of\"\n    Building {\n      integer building_id PK\n      string  name\n      string  address\n    }\n    Room {\n      integer room_id PK\n      integer number\n      string  building_id FK\n    }\n    Student {\n      integer student_id PK\n      string  name\n      integer room_id FK\n    }\n\n\n\n\n\n\n\nScenario: Your school is welcoming a new cohort of undergraduate students this year. Every new student will be assigned to a room in an on-campus dormitory building. A building can contain many rooms, and rooms in one building are numbered by its floor level and sequence (e.g., 1089 means room 89 on the 10-th floor). You will be recording these on-campus address information for these students. The E-R model diagram is show on the right.\nThen, referring to pages 7 to 28 in Lesson “Structured Query Language”, answer the following questions (a to g). You should submit a reproducible literate programming notebook (Jupyter or Rmarkdown) with SQL statements to generate the answers for these questions (samples of reproducible notebooks for SQL can be found in the course’s GitHub repository. Using SQLite as the RDBMS is not mandatory but highly recommended.\nNote: You are only expected to provide answers (SQL codes) that are reproducible and comparable to those in lecture slides. You’re not expected to be extremely comprehensive or implement what hasn’t been covered in the lecture so far.\n\n(5 points) Create three tables: Building, Room, and Student. You need to satisfy the constraints that if a building is deleted, its rooms must be also deleted concurrently, and that a room cannot be removed if there are students assigned to that room. Also, for each entity (table), decide which attributes (columns) constitute(s) the natural primary key, and declare a unique constraint on those columns. Assume that columns that are neither primary nor foreign keys are not required (i.e., NULLable).\n(5 points) Populate the following data into your tables (you can find the data in CSV format for copy&pasting below):\n\n\n\n\nBuildings\n\n\nname\naddress\n\n\n\n\nGreen Light Building\n1000 Univ. Rd\n\n\nLakeview Building\n1080 Univ. Rd\n\n\nEast Grand Complex\n3810 Univ. Rd\n\n\nMountainview Building\nNULL\n\n\n\n\n\n\nRooms\n\n\nroom\nbuilding\n\n\n\n\n1001\nGreen Light Building\n\n\n1301\nEast Grand Complex\n\n\n1311\nLakeview Building\n\n\n\n\n\n\nStudents\n\n\nstudent_id\nname\nroom\nbuilding\n\n\n\n\n20240001\nAlpha Beta\n1001\nGreen Light Building\n\n\n20240002\nGamma Delta\n1311\nLakeview Building\n\n\n\n\n\n\n(4 points) Add one row to table “Student”, where the student: id is 20240003, name is “Theta Epsilon”, lives in room with number 1301 at East Grand Complex. Suppose you don’t know the room_id or the building_id (i.e., you need to include a query for room_id and building_id in your statement).\n(3 points) Select the buildings with missing (i.e., NULL) building address.\n(2 points) Update the address of Mountainview Building as “2420 Univ. Rd”.\n(3 points) Display buildings (name and address) on the 1000 block (addresses in the range of 1000 to 1100) of Univ. Rd.\n(3 points) Display the name and id of students who live in a room on the 13-th floor in a building on the 1000 block of Univ. Rd.\n\n\nBonus question (5 points)\nEven though after correctly populating the data as per b) above there should be no enforceable constraint violations (i.e., foreign keys, primary keys, uniqueness, NOT NULL), the data as given nonetheless are not fully consistent with the depicted E-R model. Where is the inconsistency, and what is its nature? (Hint: consider the relationship cardinalities as depicted in the diagram, and whether the given dataset satisfies them.)\n\n\nData (if useful)\nThe above data tables in CSV format:\n\nwrite.csv(buildings, quote = FALSE, row.names = FALSE)\n\nname,address\nGreen Light Building,1000 Univ. Rd\nLakeview Building,1080 Univ. Rd\nEast Grand Complex,3810 Univ. Rd\nMountainview Building,NULL\n\nwrite.csv(rooms, quote = FALSE, row.names = FALSE)\n\nroom,building\n1001,Green Light Building\n1301,East Grand Complex\n1311,Lakeview Building\n\nwrite.csv(students, quote = FALSE, row.names = FALSE)\n\nstudent_id,name,room,building\n20240001,Alpha Beta,1001,Green Light Building\n20240002,Gamma Delta,1311,Lakeview Building"
  },
  {
    "objectID": "fall_24/b823/projects/hw1.html",
    "href": "fall_24/b823/projects/hw1.html",
    "title": "BIOSTAT 823 - Homework 1",
    "section": "",
    "text": "erDiagram\n    Room       ||--o{ Student : \"accommodates\"\n    Building   ||--|{ Room    : \"consists of\"\n    Building {\n      integer building_id PK\n      string  name\n      string  address\n    }\n    Room {\n      integer room_id PK\n      integer number\n      string  building_id FK\n    }\n    Student {\n      integer student_id PK\n      string  name\n      integer room_id FK\n    }"
  }
]
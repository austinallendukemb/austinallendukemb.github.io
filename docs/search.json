[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Introduction to Statistical Theory and Methods I (BIOSTAT 701)\nApplied Biostatistical Methods I (BIOSTAT 702)\nIntroduction to the Practice of Biostatistics I (BIOSTAT 703)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nProgramming, Data Structures, and Algorithms in C++ (ECE 551D)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)\n\n\n\n\nIntroduction to Statistical Theory and Methods II (BIOSTAT 704)\nApplied Biostatistical Methods II (BIOSTAT 705)\nIntroduction to the Practice of Biostatistics II (BIOSTAT 706)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nSoftware Tools for Data Science (BIOSTAT 821)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)\n\n\n\n\nStatistical Methods for Learning and Discovery (BIOSTAT 707)\nGeneralized Linear Models (BIOSTAT 719)\nStatistical Programming for Big Data (BIOSTAT 823)\n\n(Note: This semester is in progress)\n\n\n\n\nLongitudinal and Correlated Data Analysis (BIOSTAT 718)\nIntroduction to Applied Bayesian Analysis (BIOSTAT 724)\nSpecial Topics in Biomedical Engineering: Spatial Omics (BME 590)\nComputational Sequence Biology (CBB 561)\n\n(Note: This semester has not yet started)\nThis is Austin Allen’s website for his Duke MB Program Projects. Looking for Austin’s personal website? Navigate here."
  },
  {
    "objectID": "index.html#fall-2023",
    "href": "index.html#fall-2023",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Introduction to Statistical Theory and Methods I (BIOSTAT 701)\nApplied Biostatistical Methods I (BIOSTAT 702)\nIntroduction to the Practice of Biostatistics I (BIOSTAT 703)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nProgramming, Data Structures, and Algorithms in C++ (ECE 551D)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)"
  },
  {
    "objectID": "index.html#spring-2024",
    "href": "index.html#spring-2024",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Introduction to Statistical Theory and Methods II (BIOSTAT 704)\nApplied Biostatistical Methods II (BIOSTAT 705)\nIntroduction to the Practice of Biostatistics II (BIOSTAT 706)\nBiostatistics Career Preparation and Development I (BIOSTAT 801)\nSoftware Tools for Data Science (BIOSTAT 821)\n\n(Note: This website was created after this semester was completed, so projects may not have been uploaded yet)"
  },
  {
    "objectID": "index.html#fall-2024",
    "href": "index.html#fall-2024",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Statistical Methods for Learning and Discovery (BIOSTAT 707)\nGeneralized Linear Models (BIOSTAT 719)\nStatistical Programming for Big Data (BIOSTAT 823)\n\n(Note: This semester is in progress)"
  },
  {
    "objectID": "index.html#spring-2025",
    "href": "index.html#spring-2025",
    "title": "Austin’s MB Projects",
    "section": "",
    "text": "Longitudinal and Correlated Data Analysis (BIOSTAT 718)\nIntroduction to Applied Bayesian Analysis (BIOSTAT 724)\nSpecial Topics in Biomedical Engineering: Spatial Omics (BME 590)\nComputational Sequence Biology (CBB 561)\n\n(Note: This semester has not yet started)\nThis is Austin Allen’s website for his Duke MB Program Projects. Looking for Austin’s personal website? Navigate here."
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html",
    "href": "fall_24/b823/notes/september/sep_3.html",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Relational models are built on first-order predicate logic. What does that mean? Let’s ask ChatGPT.\n\nAI Summary of first-order predicate logic from ChatGPT: First-order predicate logic (FOPL), also known as first-order logic (FOL), is a formal system used in mathematics, philosophy, and computer science to express statements about objects and their relationships using quantifiers (like “for all” and “there exists”) and predicates that denote properties or relations between objects. In relational data modeling, FOPL is applied to define the structure and constraints of data by expressing rules and conditions that must hold true for data entries in a database. For instance, it can be used to specify relationships between entities (like “every employee works for a department”) and enforce constraints such as uniqueness, referential integrity, or conditional dependencies, ensuring that the data adheres to the logical rules of the modeled domain.\n\nRelations are tables; tuples are rows\n\n\n\n\n\nAn entity is an identifiable “thing” of interest in the domain of interest. For example, an entity in a retail buisiness domain is a “customer.” A “Customer” is an identifiable “thing” of interest, represented by attributes like CustomerID (a unique identifier), Name, Email, and Address, which uniquely identify and describe each customer and relate to other entities, such as “Orders” or “Products.” The set of attributes that uniquely identify the entity is the natural key.\nThe E-R model focuses on the relationships betwen entities\nIn relational data modeling, there are two main stages of database design: the logical model and the physical model\n\nLogical Model: This defines the structure of the data independent of how it will be physically stored. It focuses on what data needs to be stored, the relationships between different entities (like “Customer” and “Order”), and the rules or constraints (like uniqueness or referential integrity). The logical model is typically represented using entity-relationship diagrams (ERDs) or similar notations that show entities, attributes, and relationships without concern for specific technical details.\nPhysical Model: This represents how the data will be stored on the database system. It translates the logical model into tables, columns, indexes, and keys, and considers performance, storage optimization, and access methods. The physical model depends on the specific database management system (DBMS) used (like MySQL, PostgreSQL, or Oracle) and addresses how data is actually stored on disk, how it will be indexed, and how queries will be optimized.\n\n\n\n\n\n\nHere are the three main relationship cardinalities:\n\n\\(\\{0, 1\\}:1\\) (0 or 1 to 1)\n\\(\\{0, 1\\}:n\\) (0 or 1 to many)\n\\(n:n\\) (many to many)\n\nCrow’s Feet: I’m not going to try to draw the crow’s feet, but they’re essentially a notation to write the relationship cardinalities\nResolving \\(n:n\\) relationships:\n\nSuppose we have a table of instructors that reference a table of courses (assume each course can have multiple instructors, like Gennevieve and Zeck). This is a many-to-many relationship problem. This is a problem (I believe) because the many-to-many relationships can’t be represented directly using just foreign keys in a relational database.\nTo solve this problem, you have a middle-man table. In this “junction” table (or whatever you call it), you have one-to-many relationship pointing at both the course table and the instructor table. If you know that only one professor will teach certain lessons of the course, you can have the middle-man table be a “lessons” table. Just know that “instructor” and “course” are considered strong entities, but “lessons” are considered weak because they only exist in the context of a course and an instructor.\n\n\n\n\n\n\nForeign keys\n\nThere are implicit foreign keys and explicit foreign keys. I don’t totally understand how implicit keys work, but essentially if the instructor teaches certain lessons and those lessons are part of a specific course, then the foreign key is explicit if the lesson doesn’t have any information about the instructor or the course. That’s why I don’t understand how they work. Regardless, if the lesson does have the instructor’s ID and the course ID, then the foreign key is explicit\n\nPrimary keys\n\nA surrogate primary key is a kind of like a local variable. It would the the “ID” in the table that’s generated by the system (e.g. autoincrement).\nA natural key is a unique identifier that’s more like a global variable (i.e. isn’t only contained in the database). An email address is a good example of a natural key.\n\nNote that a composite key is two or more attributes that uniquely identify an entity\n\n\n\n\n\nNormalization is used to minimize redundancy and improve data integrity (good things)\nFirst normal form\n\nDefinition: A relation is in first normal form iff no attribute domain has relations as elements.\nWhat the heck does that mean? Basically, all attributes have to be “atomic,” i.e. must not be tables, lists, arrays, etc.\n\nSecond normal form\n\nDefinition: A relation is in 2NF iff it is in 1NF and it does not have any non-prime attribute functionally dependent on any proper subset of any candidate key of the relation.\nCome again? Apart from being first normal, if any table has a composite natural key (more than one element), no column in that table depends on only a part of the composite key.\n\nThird normal form\n\nDefinition: A relation R is in 3NF iff it is in 2NF and every non-prime attribute of R is non-transitively dependent on every key of R.\nOne more time: I don’t know what this means. But here’s what Hilmar said: For every table, any attribute that is not part of a natural key depends directly on every key for the table.\n\nThe last thing I’ll mention is that Hilmar said that you definitely want your database in first normal form, preferably second, but you can sometimes get away with not having it in third\n\n\n\n\n\n\nAn entity is a thing of interest in a domain of interest that has attributes. The set of attributes that uniquely identify the entity is the natural key.\nIn the example of the instructors and courses, the lessons table acted as a middle-man or reference table that changed the many-to-many relationship problem into two one-to-many relationships.\nA primary key identifies an entity in the table; a foreign key identifies an entity in another table; a composite natural key is a set of two or more attributes that uniquely identify an entity; a surrogate key is like a local variable that is only understood by the database and doesn’t exist in the real world (like using “autoincrement”)"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#relational-data-modeling",
    "href": "fall_24/b823/notes/september/sep_3.html#relational-data-modeling",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Relational models are built on first-order predicate logic. What does that mean? Let’s ask ChatGPT.\n\nAI Summary of first-order predicate logic from ChatGPT: First-order predicate logic (FOPL), also known as first-order logic (FOL), is a formal system used in mathematics, philosophy, and computer science to express statements about objects and their relationships using quantifiers (like “for all” and “there exists”) and predicates that denote properties or relations between objects. In relational data modeling, FOPL is applied to define the structure and constraints of data by expressing rules and conditions that must hold true for data entries in a database. For instance, it can be used to specify relationships between entities (like “every employee works for a department”) and enforce constraints such as uniqueness, referential integrity, or conditional dependencies, ensuring that the data adheres to the logical rules of the modeled domain.\n\nRelations are tables; tuples are rows\n\n\n\n\n\nAn entity is an identifiable “thing” of interest in the domain of interest. For example, an entity in a retail buisiness domain is a “customer.” A “Customer” is an identifiable “thing” of interest, represented by attributes like CustomerID (a unique identifier), Name, Email, and Address, which uniquely identify and describe each customer and relate to other entities, such as “Orders” or “Products.” The set of attributes that uniquely identify the entity is the natural key.\nThe E-R model focuses on the relationships betwen entities\nIn relational data modeling, there are two main stages of database design: the logical model and the physical model\n\nLogical Model: This defines the structure of the data independent of how it will be physically stored. It focuses on what data needs to be stored, the relationships between different entities (like “Customer” and “Order”), and the rules or constraints (like uniqueness or referential integrity). The logical model is typically represented using entity-relationship diagrams (ERDs) or similar notations that show entities, attributes, and relationships without concern for specific technical details.\nPhysical Model: This represents how the data will be stored on the database system. It translates the logical model into tables, columns, indexes, and keys, and considers performance, storage optimization, and access methods. The physical model depends on the specific database management system (DBMS) used (like MySQL, PostgreSQL, or Oracle) and addresses how data is actually stored on disk, how it will be indexed, and how queries will be optimized.\n\n\n\n\n\n\nHere are the three main relationship cardinalities:\n\n\\(\\{0, 1\\}:1\\) (0 or 1 to 1)\n\\(\\{0, 1\\}:n\\) (0 or 1 to many)\n\\(n:n\\) (many to many)\n\nCrow’s Feet: I’m not going to try to draw the crow’s feet, but they’re essentially a notation to write the relationship cardinalities\nResolving \\(n:n\\) relationships:\n\nSuppose we have a table of instructors that reference a table of courses (assume each course can have multiple instructors, like Gennevieve and Zeck). This is a many-to-many relationship problem. This is a problem (I believe) because the many-to-many relationships can’t be represented directly using just foreign keys in a relational database.\nTo solve this problem, you have a middle-man table. In this “junction” table (or whatever you call it), you have one-to-many relationship pointing at both the course table and the instructor table. If you know that only one professor will teach certain lessons of the course, you can have the middle-man table be a “lessons” table. Just know that “instructor” and “course” are considered strong entities, but “lessons” are considered weak because they only exist in the context of a course and an instructor.\n\n\n\n\n\n\nForeign keys\n\nThere are implicit foreign keys and explicit foreign keys. I don’t totally understand how implicit keys work, but essentially if the instructor teaches certain lessons and those lessons are part of a specific course, then the foreign key is explicit if the lesson doesn’t have any information about the instructor or the course. That’s why I don’t understand how they work. Regardless, if the lesson does have the instructor’s ID and the course ID, then the foreign key is explicit\n\nPrimary keys\n\nA surrogate primary key is a kind of like a local variable. It would the the “ID” in the table that’s generated by the system (e.g. autoincrement).\nA natural key is a unique identifier that’s more like a global variable (i.e. isn’t only contained in the database). An email address is a good example of a natural key.\n\nNote that a composite key is two or more attributes that uniquely identify an entity\n\n\n\n\n\nNormalization is used to minimize redundancy and improve data integrity (good things)\nFirst normal form\n\nDefinition: A relation is in first normal form iff no attribute domain has relations as elements.\nWhat the heck does that mean? Basically, all attributes have to be “atomic,” i.e. must not be tables, lists, arrays, etc.\n\nSecond normal form\n\nDefinition: A relation is in 2NF iff it is in 1NF and it does not have any non-prime attribute functionally dependent on any proper subset of any candidate key of the relation.\nCome again? Apart from being first normal, if any table has a composite natural key (more than one element), no column in that table depends on only a part of the composite key.\n\nThird normal form\n\nDefinition: A relation R is in 3NF iff it is in 2NF and every non-prime attribute of R is non-transitively dependent on every key of R.\nOne more time: I don’t know what this means. But here’s what Hilmar said: For every table, any attribute that is not part of a natural key depends directly on every key for the table.\n\nThe last thing I’ll mention is that Hilmar said that you definitely want your database in first normal form, preferably second, but you can sometimes get away with not having it in third"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#three-take-aways",
    "href": "fall_24/b823/notes/september/sep_3.html#three-take-aways",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "An entity is a thing of interest in a domain of interest that has attributes. The set of attributes that uniquely identify the entity is the natural key.\nIn the example of the instructors and courses, the lessons table acted as a middle-man or reference table that changed the many-to-many relationship problem into two one-to-many relationships.\nA primary key identifies an entity in the table; a foreign key identifies an entity in another table; a composite natural key is a set of two or more attributes that uniquely identify an entity; a surrogate key is like a local variable that is only understood by the database and doesn’t exist in the real world (like using “autoincrement”)"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#follow-up-question-about-implicit-foreign-keys",
    "href": "fall_24/b823/notes/september/sep_3.html#follow-up-question-about-implicit-foreign-keys",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Follow-up Question About Implicit Foreign Keys",
    "text": "Follow-up Question About Implicit Foreign Keys\nAfter last lecture, I had the following question that was answered today:\n\nQuestion: We talked about implicit foreign keys and explicit foreign keys. If the foreign key is implicit, how on the good green earth would you be able to reference the foreign entity?\nAnswer: Foreign keys are only implicit in the logical model. In the physical implementation, they most certainly are explicit."
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#sql---introduction",
    "href": "fall_24/b823/notes/september/sep_3.html#sql---introduction",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "SQL - Introduction",
    "text": "SQL - Introduction\n\nYou can categorize SQL commands into four groups:\n\nData Definition Language\n\nThis is for executing tasks like creating a table and defining columns\n\nData Manipulation Language\n\nThis is for updating the tables/rows after they’re created\n\nData Query Language\n\nThis is for getting data\n\nData Control Language\n\nI’m not sure what this does yet…\n\n\nKnitr supports a variety of “engines,” including the SQL engine\n\nQuestion: is an engine like a kernal?\n\nIn R, DBI is a handy package to remember. It can be used to create a database connection and work with datatables.\n\nYou can choose to create an in-memory database using “:memory:” as an argument. Why would we want to create an in-memory database? What’s the advantage over a set of dataframes or datatables?\n\nI just asked ChatGPT this same question, and here’s the response:\n\nAn in-memory database in R, created using DBI with “:memory:”, offers several advantages over data frames or data tables. It provides faster data access by using RAM instead of disk storage, allows complex SQL queries and joins, supports transactional consistency, and efficiently manages memory allocation. In-memory databases are ideal for temporary data analysis, reducing disk I/O overhead, and can handle larger datasets more effectively, offering a powerful alternative for data manipulation tasks in R.\n\n\n\n\nDDL (“D-efinition”)\n\nHere are some common verbs:\n\nCREATE/DROP/ALTER TABLE\nCREATE/DROP VIEW\nCREATE/DROP INDEX\n\nDDL consists of SQL commands that return a status or a count, but not a result set\nWhen creating a table, you can define a foreign key in a couple ways:\n\nThe first way is to specify that it’s a foreign key when you create the column name:\n\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64),   Instructor_OID INTEGER NOT NULL       -- foreign key constraint can be part of column definition       REFERENCES Instructor (Instructor_OID),   PRIMARY KEY (Name, Instructor_OID) );\n\nThe second way is to do it in two statements:\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64) NOT NULL,   Instructor_OID INTEGER NOT NULL   -- foreign key constraint can be defined separately   FOREIGN KEY (Instructor_OID)       REFERENCES Instructor (Instructor_OID)   PRIMARY KEY (Name, Instructor_OID) );\n\nKeep in mind that there are three ways to specify how the RDBMS handles a deletion of the foreign entity (instructor). The first way is set the value to NULL, but in this case, where Instructor_OID is part of the composite natural key (and cannot be null anyway), that would create an error. The second way (which is the default) is to restrict, which means if the instructor is deleted and his Instructor_OID is found somewhere as a foreign key in the Lesson table, then an error will arise. This is how it’s specified:\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64),   Instructor_OID INTEGER NOT NULL       -- foreign key constraint can be part of column definition       REFERENCES Instructor (Instructor_OID)       ON DELETE RESTRICT,   PRIMARY KEY (Name, Instructor_OID) );\n\nThe last way is to use CASCADE, which relies on this logic. If the professor teaching the lesson doesn’t exist, and if that implies that the lesson no longer exists, then just remove the lesson! So if the professor is deleted, the lesson is automatically deleted. So be careful when using CASCADE! Here’s how you specify it:\n\n{sql eval = FALSE} CREATE TABLE Lesson (   Name VARCHAR(64),   Instructor_OID INTEGER NOT NULL       -- foreign key constraint can be part of column definition       REFERENCES Instructor (Instructor_OID)       ON DELETE CASCADE,   PRIMARY KEY (Name, Instructor_OID) );\n\nOne last thing: For “weak” entities (Lessons), the natural primary key will include a foreign key\nCreating an index can speed up a query, but they also increase the transaction cost\nAltering tables: just know it’s a thing with lots of documentation, but we’re not going to get into the nitty gritty of it in this course\n\n\n\nDML (“M-anagement”)\n\nCommon DML commands:\n\nINSERT\nUPDATE\nDELETE\n\nReturns a status or a count\nUPDATE and DELETE don’t raise an error if the query isn’t matched, so look for the count that it returns!\n\n\n\nDQL (“Q-ery”)\n\nSelect statement consists of:\n\nSELECT: which columns (or values) to report\nFROM: which table(s) to query and how to join tables\nWHERE: conditions to be met for rows to be reported\nGROUP BY: how to aggregate rows by certain columns\nHAVING: conditions to be met for aggregated rows\nORDER BY: how to order the rows in the report\n\nI thought this subquery example was kind of neat:\n\nSELECT DISTINCT Name FROM Lesson\nWHERE Instructor_OID IN (\n   SELECT Instructor_OID\n   FROM Instructor WHERE Name LIKE 'h%'\n)"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_3.html#three-take-aways-1",
    "href": "fall_24/b823/notes/september/sep_3.html#three-take-aways-1",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nWhen creating tables that have foreign keys, it is often important to impose constraints on the foreign key. One of those constraints is to ensure the proper handling of the current entity in the case the foreign entity were deleted. Write the SQL command to delete the current entity if the foreign entity is deleted.\nWhen creating tables that have foreign keys, it is often important to impose constraints on the foreign key. One of those constraints is to ensure the proper handling of the current entity in the case the foreign entity were deleted. Write the SQL command to raise an error if the foreign entity is deleted.\nFor any “weak” entity, what will always be included in the natural primary key?\n\nBack"
  },
  {
    "objectID": "fall_24/b823/notes/notes.html",
    "href": "fall_24/b823/notes/notes.html",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "",
    "text": "September\n\nWeek of September 3, 2024 - September 5, 2024\nWeek of September 10, 2024 - September 12, 2024\n\n\n\nOctober\n\n\nNovember"
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html",
    "href": "fall_24/b719/projects/hw_2.html",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "",
    "text": "Show that the following probability density functions belong to the exponential family. Define a(·), b(·), c(·), d(·) components.\n\nPart (a): Pareto distribution \\(f(y; \\theta) = \\theta y^{-\\theta-1}\\)\nFor all of these, we must show that the function holds the form \\(\\exp\\left[ a(y)b(\\theta) + c(\\theta) + d(y) \\right]\\). Let’s rewrite the Pareto PDF just a smidge:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-\\theta \\log(y)+ \\log(\\theta) - \\log(y)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = \\log(y)\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = -\\log(y)\\)\n\nWe can clearly see that the Pareto distribution is a member of the exponential family.\nPart (b): Exponential distribution \\(f(y; \\theta) = \\theta^{-y\\theta}\\)\nOnce again, let’s rewrite this PDF:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-y\\theta + \\log(\\theta)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = 0\\)\n\nThe exponential distribution, thankfully, is a member of the exponential family. Otherwise, I think that would get awkward at family reunions.\nPart (c): Negative binomial distribution \\(f(y; \\theta) = {y + r - 1 \\choose r - 1} \\theta^r(1 - \\theta)^y\\), where \\(r\\) is known\nUsing the same method, we get the following:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[y\\log(1 - \\theta) + r\\log(\\theta) + \\log\\left[{y + r - 1\\choose r - 1} \\right]\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(1 - \\theta)\\)\n\\(c(\\theta) = r\\log(\\theta)\\)\n\\(d(y) = \\log\\left[{y + r - 1\\choose r - 1} \\right]\\)\n\nThe negative binomial distribution is also part of the exponential family.\nPart (d): Extreme value (Gumbel) distribution \\(f(y; \\theta) = \\frac{1}{\\Phi} exp\\left \\{  \\frac{y - \\theta}{\\Phi} - exp\\left[ \\frac{y - \\theta}{\\Phi} \\right]  \\right \\}\\), where \\(\\Phi &gt; 0\\) is considered a nuissance parameter.\nSimilar to how we conducted part (c), we are going to ignore nuissance parameters (i.e. treat them as constants) and focus on the parameter of interest, \\(\\theta\\).\nWe can rewrite the PDF as follows:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[ -exp\\left[ \\frac{y - \\theta}{\\Phi} \\right] - \\frac{\\theta}{\\Phi} + \\frac{y}{\\Phi} + \\log\\left(\\frac{1}{\\Phi} \\right)\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = -e^{y/\\Phi}\\)\n\\(b(\\theta) = e^{-\\theta/\\Phi}\\)\n\\(c(\\theta) = -\\frac{\\theta}{\\Phi}\\)\n\\(d(y) = \\frac{y}{\\Phi} + \\log\\left[\\frac{1}{\\Phi} \\right]\\)\n\nThis, too, is part of the exponential family."
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#problem-1",
    "href": "fall_24/b719/projects/hw_2.html#problem-1",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "",
    "text": "Show that the following probability density functions belong to the exponential family. Define a(·), b(·), c(·), d(·) components.\n\nPart (a): Pareto distribution \\(f(y; \\theta) = \\theta y^{-\\theta-1}\\)\nFor all of these, we must show that the function holds the form \\(\\exp\\left[ a(y)b(\\theta) + c(\\theta) + d(y) \\right]\\). Let’s rewrite the Pareto PDF just a smidge:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-\\theta \\log(y)+ \\log(\\theta) - \\log(y)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = \\log(y)\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = -\\log(y)\\)\n\nWe can clearly see that the Pareto distribution is a member of the exponential family.\nPart (b): Exponential distribution \\(f(y; \\theta) = \\theta^{-y\\theta}\\)\nOnce again, let’s rewrite this PDF:\n\\[\\begin{align*}\nf(y; \\theta) &= \\exp[-y\\theta + \\log(\\theta)]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = -\\theta\\)\n\\(c(\\theta) = \\log(\\theta)\\)\n\\(d(y) = 0\\)\n\nThe exponential distribution, thankfully, is a member of the exponential family. Otherwise, I think that would get awkward at family reunions.\nPart (c): Negative binomial distribution \\(f(y; \\theta) = {y + r - 1 \\choose r - 1} \\theta^r(1 - \\theta)^y\\), where \\(r\\) is known\nUsing the same method, we get the following:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[y\\log(1 - \\theta) + r\\log(\\theta) + \\log\\left[{y + r - 1\\choose r - 1} \\right]\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(1 - \\theta)\\)\n\\(c(\\theta) = r\\log(\\theta)\\)\n\\(d(y) = \\log\\left[{y + r - 1\\choose r - 1} \\right]\\)\n\nThe negative binomial distribution is also part of the exponential family.\nPart (d): Extreme value (Gumbel) distribution \\(f(y; \\theta) = \\frac{1}{\\Phi} exp\\left \\{  \\frac{y - \\theta}{\\Phi} - exp\\left[ \\frac{y - \\theta}{\\Phi} \\right]  \\right \\}\\), where \\(\\Phi &gt; 0\\) is considered a nuissance parameter.\nSimilar to how we conducted part (c), we are going to ignore nuissance parameters (i.e. treat them as constants) and focus on the parameter of interest, \\(\\theta\\).\nWe can rewrite the PDF as follows:\n\\[\\begin{align*}\nf(y; \\theta) &= exp\\left[ -exp\\left[ \\frac{y - \\theta}{\\Phi} \\right] - \\frac{\\theta}{\\Phi} + \\frac{y}{\\Phi} + \\log\\left(\\frac{1}{\\Phi} \\right)\\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nWhere:\n\n\\(a(y) = -e^{y/\\Phi}\\)\n\\(b(\\theta) = e^{-\\theta/\\Phi}\\)\n\\(c(\\theta) = -\\frac{\\theta}{\\Phi}\\)\n\\(d(y) = \\frac{y}{\\Phi} + \\log\\left[\\frac{1}{\\Phi} \\right]\\)\n\nThis, too, is part of the exponential family."
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#problem-2",
    "href": "fall_24/b719/projects/hw_2.html#problem-2",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "Problem 2",
    "text": "Problem 2\n\nConsider a randome variable Y with the following Gamma distribution with a scale parameter, \\(\\beta\\), of interest, and a known shape parameter \\(\\alpha\\):\n\n\\[\nf(y; \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}y^{\\alpha - 1}e^{-y\\beta}, \\text{  where } y &gt; 0, \\quad \\alpha &gt; 0, \\quad \\beta &gt; 0\n\\]\nPart (a):\n\nDoes this distribution belong to the exponential family?\n\nTo answer this question, let’s once again see if we can rewrite this function in terms of a(.), b(.), c(.), and d(.). Note that because \\(\\beta\\) is the parameter of interest, we treat \\(\\alpha\\) as the nuissance parameter.\n\\[\n\\begin{align*}\nf(y; \\theta) &= exp\\left[ -y\\beta + \\alpha \\log(\\beta) - \\log\\left( \\Gamma(\\alpha)\\right)+ \\alpha\\log(y) - \\log(y)\\right]\\\\\n&= \\exp[a(y)b(\\beta) + c(\\beta) + d(y)],\n\\end{align*}\n\\]\nWhere:\n\n\\(a(y) = y\\)\n\\(b(\\beta) = -\\beta\\)\n\\(c(\\theta) = \\alpha \\log(\\beta) - \\log\\left( \\Gamma(\\alpha)\\right)\\)\n\\(d(y) = \\alpha\\log(y) - \\log(y)\\)\n\nYes, the Gamma distribution belongs to the exponential family. This will assist us in deriving expectation and variance.\nPart (b):\n\nDerive expectation of Y\n\nRecall that for any random variable \\(Y\\) with a distribution belonging to the exponential family, \\(E[a(y)] = -c'(\\theta)/b'(\\theta)\\). We can solve for these values and evaluate \\(E(a(y))\\), which is \\(E[Y]\\).\n\\[\\begin{align*}\n-c'(\\beta) &= - \\frac{d c(\\beta)}{d\\beta}\\\\\n&= -\\frac{d}{d\\beta}\\left[ \\alpha \\log(\\beta) - \\log\\left( \\Gamma(\\alpha)\\right)\\right]\\\\\n&= -\\frac{\\alpha}{\\beta}\\\\\nb'(\\beta) &= \\frac{d b(\\beta)}{d\\beta}\\\\\n&= \\frac{d}{d\\beta} - \\beta\\\\\n&= -1\\\\\n-c'(\\beta)/b'(\\beta) &= -\\frac{\\alpha}{\\beta}/-1\\\\\n&= \\frac{\\alpha}{\\beta}\n\\end{align*}\\]\nPart (c):\n\nDerive the variance of Y\n\nRecall another important property of the exponential family. For any random variable \\(Y\\) with a distribution belonging to the exponential family, \\(V[a(y)] = \\frac{b''(\\theta)c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\).\n\\[\\begin{align*}\nb'(\\beta) &= -1\\\\\nc'(\\beta) &= \\frac{\\alpha}{\\beta}\\\\\nb''(\\beta) &= 0\\\\\nc''(\\beta) &= -\\frac{\\alpha}{\\beta^2}\\\\\nV[a(y)] &= \\frac{b''(\\beta)c'(\\beta) - c''(\\beta)b'(\\beta)}{[b'(\\beta)]^3}\\\\\n&= \\frac{0 - \\frac{\\alpha}{\\beta^2}}{[-1]^3}\\\\\n&= \\frac{\\alpha}{\\beta^2}\n\\end{align*}\\]\nPart (d):\n\nDerive variance of the score statistic\n\nRecall that the variance of the score statistic, also known as the Fisher’s information, can be expressed as follows: \\(V[U] = I(\\theta) = \\frac{b''(\\theta) c'(\\theta)}{b'(\\theta)} - c''(\\theta)\\). Once again, we can plug and chug.\n\\[\\begin{align*}\nb'(\\beta) &= -1\\\\\nc'(\\beta) &= \\frac{\\alpha}{\\beta}\\\\\nb''(\\beta) &= 0\\\\\nc''(\\beta) &= -\\frac{\\alpha}{\\beta^2}\\\\\nV[U] &= \\frac{b''(\\beta)c'(\\beta) }{b'(\\beta)} - c''(\\beta)\\\\\n&= 0 - \\left( -\\frac{\\alpha}{\\beta^2}\\right)\\\\\n&= \\frac{\\alpha}{\\beta^2}\n\\end{align*}\\]"
  },
  {
    "objectID": "fall_24/b719/projects/hw_2.html#problem-3",
    "href": "fall_24/b719/projects/hw_2.html#problem-3",
    "title": "BIOSTAT 719 - Homework 2",
    "section": "Problem 3",
    "text": "Problem 3\n\nDerive expression for information (consider \\(\\theta\\) the parameter of interest) for a single observation from the Weibull distribution:\n\n\\[\nf(y; \\lambda , \\theta) = \\frac{\\lambda y^{\\lambda - 1}}{\\theta^{\\lambda}}\\exp\\left[ -\\left(\\frac{y}{\\theta}\\right)^{\\lambda}\\right],\n\\]\n\nwhere \\(y \\ge 0, \\lambda &gt; 0,\\) and \\(\\theta &gt; 0\\). Show all your work.\n\nLet’s start by identifying a(.), b(.), c(.), and d(.).\n\\[\\begin{align*}\nf(y; \\lambda, \\theta) &= \\frac{\\lambda y^{\\lambda - 1}}{\\theta^{\\lambda}}\\exp\\left[ -\\left(\\frac{y}{\\theta}\\right)^{\\lambda}\\right]\\\\\n&= \\exp\\left[- \\frac{y^{\\lambda}}{\\theta^{\\lambda}} -\\lambda \\log(\\theta) - \\log(y) + \\lambda\\log(y) + \\log(\\lambda)  \\right]\\\\\n&= \\exp[a(y)b(\\theta) + c(\\theta) + d(y)],\n\\end{align*}\\]\nwhere\n\n\\(a(y) = y^{\\lambda}\\)\n\\(b(\\theta) = -\\frac{1}{\\theta^{\\lambda}}\\)\n\\(c(\\theta) = -\\lambda \\log(\\theta)\\)\n\\(d(y) = - \\log(y) + \\lambda\\log(y) + \\log(\\lambda)\\)\n\nUsing our previously helpful formula for calculating Fisher’s information, we can derive an expression for the Fisher’s information:"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html",
    "href": "fall_24/b719/notes/september/sep_3.html",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "When an outcome is continuous, we consider a LR model like to be: \\(E(Y_i) = \\mu_i = X_i^T \\beta\\), where \\(T_i \\sim N(\\mu_i, \\sigma^2)\\)\nFor models with non-continuous outcomes, the distribution of \\(Y_i\\) is not normal. However, they may be in the exponential family and can share some of the nice properties of normality\nExponential family definition:\n\nDistribution of Y belongs to the exponential family if it can be written in the form \\(f(y; \\theta) = exp[a(y)b(\\theta) + c(\\theta) + d(y)]\\)\nCanonical form: if \\(a(y) = y\\), the distribution is in canonical form and \\(b(\\theta)\\) is said to be the natural parameter\nIf there are other parameters other than the parameter of interest (say \\(\\sigma^2\\) when we’re interested in \\(\\mu\\)), these are said to be nuisance parameters\n\n\n\n\n\n\n\n\nIs the Poisson PDF in canonical form? If so, what is the natural parameter?\n\nPoisson PDF: \\(f(y; \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}, \\quad y = 0, 1, 2, ...\\)\n\n\\(f(y; \\theta) = exp[y \\log (\\theta) - \\theta - log(y!)]\\)\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(\\theta)\\)\n\\(c(\\theta) = -\\theta\\)\n\\(d(y) = -\\log(y!)\\)\n\nYes, it’s in canonical form, and the natural parameter is \\(\\log(\\theta)\\). Easy peasy.\n\n\n\n\nWhat is the nuisance parameter? Is it in canonical form? What (if yes) is the natural parameter?\n\n\\[\\begin{align*}\nf(y; \\theta) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[-\\frac{1}{2\\sigma^2}(y - \\mu)^2\\right]\\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2}\\right]\\\\\n&= exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2} -\\frac{1}{2}log(2\\pi\\sigma^2)  \\right]\\\\\n\\end{align*}\\]\n\n\\(a(y) = y\\)\n\\(b(\\mu) = \\frac{\\mu}{\\sigma^2}\\)\n\\(c(\\mu) = \\frac{\\mu^2}{2\\sigma^2}\\)\n\\(d(y) = - \\frac{y^2}{2\\sigma^2} - \\frac{log(2\\pi\\sigma^2)}{2}\\)\n\nThe nuisance parameter is \\(\\sigma^2\\). It is in canonical form, and the natural parameter is \\(\\mu/\\sigma^2\\).\n\n\n\n\nWhy do we care about the exponential family? Because it has some really useful properties. Specifically, we’re going to learn about calculating the mean and variance.\n\nProperty 1: \\(E[a(y)] = -c'(\\theta)/b'(\\theta)\\)\nProperty 2: \\(Var[a(y)] = \\frac{b''(\\theta) c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\)\n\n\n\n\nRecall that the score function (or score statistic) is the first derivative of the log-likelihood function. Note that because it is a function of Y, it is considered a random variable. There are two extremely interesting properties of the score function.\nLet the random variable U represent the score function.\n\nProperty 1: \\(E[U] = 0\\)\nProperty 2: \\(Var[U] = \\frac{b''(\\theta)c'(\\theta)}{b'(\\theta)} - c''(\\theta) = I(\\theta)\\), where \\(I(\\theta)\\) is the Fisher’s information of \\(\\theta\\). Isn’t that crazy? The variance of the first derivative of the log-likelihood is the Fisher’s information.\n\n\n\n\n\nSuppose f(y; ) is a PDF. How must f(y; ) be written to conclude that it belongs to the exponential family?\nProperty 1 of the exponential family has to do with E[a(y)]. What is the property?\nProperty 2 of the exponential family has to do with V(a(y)). What is the property?"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#exponential-family",
    "href": "fall_24/b719/notes/september/sep_3.html#exponential-family",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "When an outcome is continuous, we consider a LR model like to be: \\(E(Y_i) = \\mu_i = X_i^T \\beta\\), where \\(T_i \\sim N(\\mu_i, \\sigma^2)\\)\nFor models with non-continuous outcomes, the distribution of \\(Y_i\\) is not normal. However, they may be in the exponential family and can share some of the nice properties of normality\nExponential family definition:\n\nDistribution of Y belongs to the exponential family if it can be written in the form \\(f(y; \\theta) = exp[a(y)b(\\theta) + c(\\theta) + d(y)]\\)\nCanonical form: if \\(a(y) = y\\), the distribution is in canonical form and \\(b(\\theta)\\) is said to be the natural parameter\nIf there are other parameters other than the parameter of interest (say \\(\\sigma^2\\) when we’re interested in \\(\\mu\\)), these are said to be nuisance parameters"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#examples",
    "href": "fall_24/b719/notes/september/sep_3.html#examples",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Is the Poisson PDF in canonical form? If so, what is the natural parameter?\n\nPoisson PDF: \\(f(y; \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}, \\quad y = 0, 1, 2, ...\\)\n\n\\(f(y; \\theta) = exp[y \\log (\\theta) - \\theta - log(y!)]\\)\n\\(a(y) = y\\)\n\\(b(\\theta) = \\log(\\theta)\\)\n\\(c(\\theta) = -\\theta\\)\n\\(d(y) = -\\log(y!)\\)\n\nYes, it’s in canonical form, and the natural parameter is \\(\\log(\\theta)\\). Easy peasy.\n\n\n\n\nWhat is the nuisance parameter? Is it in canonical form? What (if yes) is the natural parameter?\n\n\\[\\begin{align*}\nf(y; \\theta) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[-\\frac{1}{2\\sigma^2}(y - \\mu)^2\\right]\\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2}\\right]\\\\\n&= exp\\left[\\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}-\\frac{y^2}{2\\sigma^2} -\\frac{1}{2}log(2\\pi\\sigma^2)  \\right]\\\\\n\\end{align*}\\]\n\n\\(a(y) = y\\)\n\\(b(\\mu) = \\frac{\\mu}{\\sigma^2}\\)\n\\(c(\\mu) = \\frac{\\mu^2}{2\\sigma^2}\\)\n\\(d(y) = - \\frac{y^2}{2\\sigma^2} - \\frac{log(2\\pi\\sigma^2)}{2}\\)\n\nThe nuisance parameter is \\(\\sigma^2\\). It is in canonical form, and the natural parameter is \\(\\mu/\\sigma^2\\)."
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#properties-of-the-exponential-family",
    "href": "fall_24/b719/notes/september/sep_3.html#properties-of-the-exponential-family",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Why do we care about the exponential family? Because it has some really useful properties. Specifically, we’re going to learn about calculating the mean and variance.\n\nProperty 1: \\(E[a(y)] = -c'(\\theta)/b'(\\theta)\\)\nProperty 2: \\(Var[a(y)] = \\frac{b''(\\theta) c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\)"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#the-score-function",
    "href": "fall_24/b719/notes/september/sep_3.html#the-score-function",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Recall that the score function (or score statistic) is the first derivative of the log-likelihood function. Note that because it is a function of Y, it is considered a random variable. There are two extremely interesting properties of the score function.\nLet the random variable U represent the score function.\n\nProperty 1: \\(E[U] = 0\\)\nProperty 2: \\(Var[U] = \\frac{b''(\\theta)c'(\\theta)}{b'(\\theta)} - c''(\\theta) = I(\\theta)\\), where \\(I(\\theta)\\) is the Fisher’s information of \\(\\theta\\). Isn’t that crazy? The variance of the first derivative of the log-likelihood is the Fisher’s information."
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#three-take-aways",
    "href": "fall_24/b719/notes/september/sep_3.html#three-take-aways",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "",
    "text": "Suppose f(y; ) is a PDF. How must f(y; ) be written to conclude that it belongs to the exponential family?\nProperty 1 of the exponential family has to do with E[a(y)]. What is the property?\nProperty 2 of the exponential family has to do with V(a(y)). What is the property?"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#information---review",
    "href": "fall_24/b719/notes/september/sep_3.html#information---review",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Information - Review",
    "text": "Information - Review\n\nWe find an MLE of \\(\\theta\\) setting U (the score function) to 0. We also know that the variance of \\(U\\) is the Fisher’s information. But what is information? How is it related to MLEs?\nInformation is a way to measure the amount of information that your data carry about the unkown parameter, \\(\\theta\\). As an aside, I verified that \\(I = 1/\\text{curvature}\\), which is really interesting."
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#glms",
    "href": "fall_24/b719/notes/september/sep_3.html#glms",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "GLMs",
    "text": "GLMs\n\nResearch Process (again)\n\nModel specification\n\n\\(y\\sim \\text{exponential family}\\)\nLinear association between Y and X with model parameter \\(E(y) = X^T\\beta\\)\nYou got yourself a GLM (this is where we are going to be in the process for the next week or two)\n\nEstimation\n\n\\(y\\sim N(\\mu, \\sigma^2)\\): \\(\\hat{\\mu}\\)\n\\(y\\sim N(\\beta X, \\sigma^2)\\): \\(\\hat{\\beta}\\)\n\\(y\\sim Pois(\\beta X)\\): \\(\\hat{\\beta}\\), this gets complicated\n\nModel fit\n\nAIC, LRT, deviance\n\nInference\n\n\n\nDefinition of the Generalized Linear Model\nGLM is defined in terms of a set of independent random variables \\(Y_1, ..., Y_n\\), each with a distribution from the exponential family and having the following three properties:\n\nThe distribution of each \\(Y_i\\) has the canonical form and depends on a single parameter \\(\\theta_i\\)\nThe distibution of all the \\(Y_i\\)’s are of the same form so that \\(b_i(.) = b(.)\\), \\(c_i(.) = c(.)\\), and \\(d_i(.) = d(.)\\).\nSuppose \\(E(Y_i) = \\mu_i\\), where \\(\\mu\\) is some function of \\(\\theta_i\\). For a GLM, there exists a transformation of \\(\\mu_i\\) such that \\(g(\\mu_i) = X_i^T\\beta = \\eta_i\\)\n\n\\(g(.)\\) is a monotone, differentiable function called the link function\n\\(X^T_i\\) is the 1 X p design vector (ith row of the design matrix X)\n\\(\\beta\\) is the p X 1 vector of parameters\n\n\n\n\nThe Link Function\n\nFor linear regression, the link function is the identity function: \\(g(\\mu_i) = \\mu_i\\)\nFor logistic regression, the link function is the logit function: \\(g(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = X^T_i \\beta\\)\nFor Poisson regression, the link function is \\(g(\\theta_i) = X^T_i\\beta = \\log(\\theta_i)\\)\nThe inverse link function gives us \\(\\mu_i\\), which can be very useful (think getting \\(\\pi\\) in logistic regression)\n\nThe inverse link function for logistic regression (called the expit) is \\(\\frac{1}{1 + e^{-x^T_i\\beta}}\\)"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_3.html#three-take-aways-1",
    "href": "fall_24/b719/notes/september/sep_3.html#three-take-aways-1",
    "title": "September 3, 2024 - September 5, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nFor a set of independent random variables to be a GLM, there are three properties that need to be met. The first is that the distribution of each Y_i depends on a single parameter and has what special form?\nFor a set of independent random variables to be a GLM, there are three properties that need to be met. The third and most important property is that for E(Y_i) = , there exists a transformation of such that … (fill in the blank).\nWhat is the form of the inverse link function for logistic regression? (Reminder: this is the function known as the expit function)\n\nBack"
  },
  {
    "objectID": "fall_24/b719/notes/notes.html",
    "href": "fall_24/b719/notes/notes.html",
    "title": "Generalized Linear Models: Course Notes",
    "section": "",
    "text": "August\n\nWeek of August 27, 2024 - August 29, 2024\n\n\n\nSeptember\n\nWeek of September 3, 2024 - September 5, 2024\n\n\n\nOctober\n\n\nNovember\nBack"
  },
  {
    "objectID": "fall_24/b719/index.html",
    "href": "fall_24/b719/index.html",
    "title": "Generalized Linear Models: Class Projects",
    "section": "",
    "text": "August\n\nHomework 1\n\n\n\nSeptember\n\n\nOctober\n\n\nNovember\nTo review notes taken during class, follow this link.\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html",
    "href": "fall_24/b707/notes/september/sep_9.html",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "",
    "text": "Here’s a cheat-sheet for plotting:\n\nContinuous variable\n\nHistograms\nKernel density\nBox plot\nviolin plot\nQQ-plot\n\nCategorical variable\n\nBar chart\nPi chart (ew)\n\nVisualizing relationship between variables\n\nHexbin plots\nScatter plot matrix\n3D plots\nTrellis plots and heatmap\n\nTime-series and longitudinal data visualizations\n\nTime series plots\nSpaghetti plots\nLasagna plots (yum)\n\nSpecializaed visualization techniques\n\nNetworks and maps, survival plots, waterfall and forest plots, Venn diagrams\n\n\n\n\n\n\nHistogram\n\nWhen should you plot frequency? When should you plot density?\n\nIf sample size is important, plot frequency\nWhen comparing the shape of the data to another distribution, use density\n\n\nDensity plot\n\nWhat is density?\n\nThe area under the curve = 1\n\n\nBox plot\n\nWhat information do we lose going from a density plot/histogram to a box plot?\n\nWe lose important information about the shape of the distribution. Looking at the box plot is like looking down at an aerial map of a mountain range. We can get an idea of its shape, but we can’t necessarily see the contours of the peaks and ridges that we would get from the ground looking up\n\nWhat information do we gain?\n\nWe gain median + IQR information, plus whiskers\n\n\nViolin plot\n\nAttempts to bridge the density and the box plot\nHere’s my question. Why not overlay the box plot at the bottom of a histogram/density curve?\n\nHexbin plot\n\nThis is like a heat map plotting two continuous variables. I think the advantage of this over a 2D scatterplot is that if we have very large datasets, it gives us additional information about the density of the data\n\nQQ-plot\n\nWe can set a QQ-plot to see where the expected quantiles and the actual quantiles align for any distribution\n\nDistribution of the p-value\n\nDid you know that under the null hypothesis, the p-value is distributed uniformly between 0 and 1? So, if you have LOTS and LOTS of p-values (think GWAS), you can create a density plot to see if it appears to be uniformly distributed\n\n\n\n\n\n\nBar plot\n\nWhat’s the difference between ggplot2::geom_bar and ggplot2::geom_col?\n\ngeom_bar is for frequency bar plots, and geom_col is for a numeric y value (like revenue) and multiple categories (departments A, B, and C)\n\n\nPie plot\n\nEmphasizes proportions\nOften hard to interpret because our eyes aren’t great at area\n\n\n\n\n\n\nGroup vs Stacked bar plots\n\nGrouped bar plots emphasizes comparison within groups\nStacked bar plots emphasizes comparison between groups\n\nScatter plot matrix\n\nUseful to look at relationships between multiple continous variables\nCan add correlation as another dimension to the plot (e.g. color)\n\n3D scatter plots\n\nCan check linearity in 3 dimensions\n\nTrellis plot\n\nStratify to see within group detail\nIs often helpful to have all groups in a single plot too for comparison between groups\n\nHeatmaps\n\nGood for correlation, gene mapping, etc.\n\nTime\n\nTime-series\nSpaghettie plot\nLasagna plot\n\nNetworks\n\nMapping knowledge networks\n\nArc diagram\n\nSimilar information as a network graph, emphasizes different things\n\nMap plots\n\nOften a “heat” map\n\nK-M curves\nWaterfall plots\nForest plot\n\nUseful in meta-analysis\n\nVenn diagram\n\nOverlapping data\n\n\n\n\n\n\n\nPicture a QQ plot generated with this R code: x &lt;- runif(1000, 0, 1); car::qqPlot(x). This will create a plot shaped like an ‘S’. This is because the y values left of the line are much HIGHER/LOWER than expected in a normal distribution, and the y values right of the line are much HIGHER/LOWER than expected in a normal distribution. Using this logic, plots with both tails above the line will be RIGHT/LEFT skewed (think: lower y-values onto the line shifts distribution in what direction?), and plots with both tails below the line will be RIGHT/LEFT skewed (think: raising y-values onto the line shifts distribution in what direction?).\nIn what type of analyses are forest plots particularly useful?\nHistograms can be constructed to show either frequency counts or density. Explain what density is (i.e. the actual value on the y-axis).\n\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html#data-visualization",
    "href": "fall_24/b707/notes/september/sep_9.html#data-visualization",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "",
    "text": "Here’s a cheat-sheet for plotting:\n\nContinuous variable\n\nHistograms\nKernel density\nBox plot\nviolin plot\nQQ-plot\n\nCategorical variable\n\nBar chart\nPi chart (ew)\n\nVisualizing relationship between variables\n\nHexbin plots\nScatter plot matrix\n3D plots\nTrellis plots and heatmap\n\nTime-series and longitudinal data visualizations\n\nTime series plots\nSpaghetti plots\nLasagna plots (yum)\n\nSpecializaed visualization techniques\n\nNetworks and maps, survival plots, waterfall and forest plots, Venn diagrams\n\n\n\n\n\n\nHistogram\n\nWhen should you plot frequency? When should you plot density?\n\nIf sample size is important, plot frequency\nWhen comparing the shape of the data to another distribution, use density\n\n\nDensity plot\n\nWhat is density?\n\nThe area under the curve = 1\n\n\nBox plot\n\nWhat information do we lose going from a density plot/histogram to a box plot?\n\nWe lose important information about the shape of the distribution. Looking at the box plot is like looking down at an aerial map of a mountain range. We can get an idea of its shape, but we can’t necessarily see the contours of the peaks and ridges that we would get from the ground looking up\n\nWhat information do we gain?\n\nWe gain median + IQR information, plus whiskers\n\n\nViolin plot\n\nAttempts to bridge the density and the box plot\nHere’s my question. Why not overlay the box plot at the bottom of a histogram/density curve?\n\nHexbin plot\n\nThis is like a heat map plotting two continuous variables. I think the advantage of this over a 2D scatterplot is that if we have very large datasets, it gives us additional information about the density of the data\n\nQQ-plot\n\nWe can set a QQ-plot to see where the expected quantiles and the actual quantiles align for any distribution\n\nDistribution of the p-value\n\nDid you know that under the null hypothesis, the p-value is distributed uniformly between 0 and 1? So, if you have LOTS and LOTS of p-values (think GWAS), you can create a density plot to see if it appears to be uniformly distributed\n\n\n\n\n\n\nBar plot\n\nWhat’s the difference between ggplot2::geom_bar and ggplot2::geom_col?\n\ngeom_bar is for frequency bar plots, and geom_col is for a numeric y value (like revenue) and multiple categories (departments A, B, and C)\n\n\nPie plot\n\nEmphasizes proportions\nOften hard to interpret because our eyes aren’t great at area\n\n\n\n\n\n\nGroup vs Stacked bar plots\n\nGrouped bar plots emphasizes comparison within groups\nStacked bar plots emphasizes comparison between groups\n\nScatter plot matrix\n\nUseful to look at relationships between multiple continous variables\nCan add correlation as another dimension to the plot (e.g. color)\n\n3D scatter plots\n\nCan check linearity in 3 dimensions\n\nTrellis plot\n\nStratify to see within group detail\nIs often helpful to have all groups in a single plot too for comparison between groups\n\nHeatmaps\n\nGood for correlation, gene mapping, etc.\n\nTime\n\nTime-series\nSpaghettie plot\nLasagna plot\n\nNetworks\n\nMapping knowledge networks\n\nArc diagram\n\nSimilar information as a network graph, emphasizes different things\n\nMap plots\n\nOften a “heat” map\n\nK-M curves\nWaterfall plots\nForest plot\n\nUseful in meta-analysis\n\nVenn diagram\n\nOverlapping data"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_9.html#three-take-aways",
    "href": "fall_24/b707/notes/september/sep_9.html#three-take-aways",
    "title": "September 9, 2024 - September 11, 2024",
    "section": "",
    "text": "Picture a QQ plot generated with this R code: x &lt;- runif(1000, 0, 1); car::qqPlot(x). This will create a plot shaped like an ‘S’. This is because the y values left of the line are much HIGHER/LOWER than expected in a normal distribution, and the y values right of the line are much HIGHER/LOWER than expected in a normal distribution. Using this logic, plots with both tails above the line will be RIGHT/LEFT skewed (think: lower y-values onto the line shifts distribution in what direction?), and plots with both tails below the line will be RIGHT/LEFT skewed (think: raising y-values onto the line shifts distribution in what direction?).\nIn what type of analyses are forest plots particularly useful?\nHistograms can be constructed to show either frequency counts or density. Explain what density is (i.e. the actual value on the y-axis).\n\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/notes.html",
    "href": "fall_24/b707/notes/notes.html",
    "title": "Course Notes for Statistical Methods for Learning and Discovery",
    "section": "",
    "text": "August\n\nWeek of August 26, 2024 - August 28, 2024\n\n\n\nSeptember\n\nWeek of September 4, 2024\nWeek of September 9, 2024 - September 11, 2024\n\n\n\nOctober\n\n\nNovember\nBack"
  },
  {
    "objectID": "fall_24/b707/index.html",
    "href": "fall_24/b707/index.html",
    "title": "Statistical Methods for Learning and Discovery",
    "section": "",
    "text": "September\n\nHomework 1\n\n\n\nOctober\n\n\nNovember\nTo review notes taken during class, follow this link.\nBack"
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "Austin's MB Projects",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/httpx-0.27.2.dist-info/licenses/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/httpx-0.27.2.dist-info/licenses/LICENSE.html",
    "title": "Austin's MB Projects",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Austin’s MB Projects",
    "section": "",
    "text": "This website was created by Austin Allen. All projects shown here were completed during his time in the Master of Biostatistics (MB) Program at Duke University. Projects are organized by course.\nWhile these projects were assigned by instructors and were completed between August 2023 and May 2025, you can check out Austin’s personal website to see projects selected by him to demonstrate skills as well as to have a little fun."
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/httpcore-1.0.5.dist-info/licenses/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/httpcore-1.0.5.dist-info/licenses/LICENSE.html",
    "title": "Austin's MB Projects",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/idna-3.8.dist-info/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/idna-3.8.dist-info/LICENSE.html",
    "title": "Austin's MB Projects",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "fall_24/b707/bsenv/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "fall_24/b707/bsenv/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "Austin's MB Projects",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html",
    "href": "fall_24/b707/notes/august/aug_26.html",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "Prediction and inference.\n\nWe care about both. How do we manage that “tug-of-war”?\n\nParametric vs Non-parametric\n\nParametric models are conveniently powerful, but they require assumptions to be met.\nNon-parametric models are convenient in that they do not require assumptions to be met, but the trade off is power\n\nAccuracy vs Interpretability\n\nChuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. On the far end (high flexibility, low interpretability) were support vector machines. On the near end (high interpretability, low flexibility) was LASSO. Several others were somewhere in the middle.\n\n\n\n\n\n\nCreate reproducible reports\nCreate high-quality graphs\nExplore a new dataset\nBuild a predictive model\n\n\n\n\n\nThe first day of class, Chuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. What was on the near end of the spectrum (high interpretability, low flexibility)? What was on the far end of the spectrum (high interpretability, low flexibility)?\nWhat are the four goals for this semester?\nDescribe the trade-off between power and flexibiliy in the context of parametric and non-parametric models."
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#trade-offs",
    "href": "fall_24/b707/notes/august/aug_26.html#trade-offs",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "Prediction and inference.\n\nWe care about both. How do we manage that “tug-of-war”?\n\nParametric vs Non-parametric\n\nParametric models are conveniently powerful, but they require assumptions to be met.\nNon-parametric models are convenient in that they do not require assumptions to be met, but the trade off is power\n\nAccuracy vs Interpretability\n\nChuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. On the far end (high flexibility, low interpretability) were support vector machines. On the near end (high interpretability, low flexibility) was LASSO. Several others were somewhere in the middle."
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#goals-by-the-end-of-the-semester",
    "href": "fall_24/b707/notes/august/aug_26.html#goals-by-the-end-of-the-semester",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "Create reproducible reports\nCreate high-quality graphs\nExplore a new dataset\nBuild a predictive model"
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#three-take-aways",
    "href": "fall_24/b707/notes/august/aug_26.html#three-take-aways",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "",
    "text": "The first day of class, Chuang showed a diagram of several tools we’re familiar with on a plot with “Interpretability” on the Y-axis and “Flexibility” on the X-axis. What was on the near end of the spectrum (high interpretability, low flexibility)? What was on the far end of the spectrum (high interpretability, low flexibility)?\nWhat are the four goals for this semester?\nDescribe the trade-off between power and flexibiliy in the context of parametric and non-parametric models."
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#reproducible-research",
    "href": "fall_24/b707/notes/august/aug_26.html#reproducible-research",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nSteps in Research\n\nSpecify the population\nState the research question\nFormulate the hypothesis\nDesign the experiment\nSpecify the experimentor\nSpecify the data collection process\nCreate an analysis plan\nSpecify the analyst\nWrite the code\nEstimate the truth\nMake claims\n\n\n\nImportant concepts of a scientific study\n\nPublications\nReproducible: Given same data, plan, code, etc. obtain the exact results\nReplicable: Given the same population, hypothesis and design, obtain similar results\nFalse discovery: The claim at the end of a study is not equal to the claim you would make if you could observe all data from the population, given your hypothesis, study design, and analysis plan\nP-hacking: Given a population, hypothesis, design, data, plan and analysis, the code changes to match the desired statement\nFile drawer effect: The probability of publication depends on the caim made at the conclusion of a scientific study\n\n\n\nReproducibility Do’s and Don’ts\n\nDo:\n\nIdentify source data\nAutomate process\nDocument code\nUse version control\nTrack software environment\nSet random number generator\nConsider full pipeline\n\nDon’t:\n\nOnly save output\nDo things by hand\nPoint and click"
  },
  {
    "objectID": "fall_24/b707/notes/august/aug_26.html#three-take-aways-1",
    "href": "fall_24/b707/notes/august/aug_26.html#three-take-aways-1",
    "title": "August 26, 2024 - August 28, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nThe claim at the end of a study is not equal to the claim you would make if you could observe all data from the population, given your hypothesis, study design, and analysis plan. What is this?\nGiven a population, hypothesis, design, data, plan and analysis, the code changes to match the desired statement. What is this?\nThe probability of publication depends on the caim made at the conclusion of a scientific study. What is this?\n\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html",
    "href": "fall_24/b707/notes/september/sep_4.html",
    "title": "September 4, 2024",
    "section": "",
    "text": "What’s the difference between the joins? Left, right, inner, outer, and full - not to mention the option to exclude A from B, etc.\nWhat’s the difference between merge and join?\n\nMerge loses the order of observations\n\n\n\n\n\n\n\n\nCentral tendancy\n\nMean, median, mode\n\nVariability\n\nStandard deviation, IQR\n\nSkewness\n\n\n\n\n\nCorrelation\n\nWhat’s the difference between Pearson’s \\(r\\), Spearman’s \\(\\rho\\), and Kendall’s \\(\\tau\\)?\n\nPearson’s \\(r\\) is parametric\nSpearman’s \\(\\rho\\) is non-parametric and is rank-based\nKendall’s \\(\\tau\\) is non-parametric and can handle categorical variables\n\n\nVariability by group\nTrajectory\n\nSlope\n\n\n\n\n\n\n\n\n\nUnivariate non-response\n\nSingle variable missing data\n\nMultivariate two patterns\n\nI think this is when there are two patterns in the data. Some variables have no missing data, and other variables have missing data in the same places\n\nMonotone\n\nThis is when missingness is exacerbated with variables. For example, let’s say we yave X1, X2, and X3. X1 is missing values, and because X2 and X3 depend on X1, any missingness in X1 is also present in X2 and X3. Then suppose X3 depends on X2, any missingness in X2 shows up in X3 as well. Finally, X3 may have its own missing values. What we get is a steady increase in missingness from X1 to X2 and X3.\n\n\nGeneral\n\nNo patterns are evidence\n\nFile matching\n\nA good example is in a full join of two datasets. Suppose one dataset had variable A and the other had variable B, and both datasets had unique individuals. We will observe a pattern where there is no common missingness for A and B, nor will there be any individual who has both A and B present.\n\nFactor analysis\n\nThis is when there is an entire variable is missing\n\n\n\n\n\n\nMissing Completely At Random (MCAR)\n\n\\(E(Y^c) = E(Y^O)\\)\nA patient is scheduled for a visit but breaks his arms while skiing\nApproach: use complete case (exclude missing values)\n\nMissing At Random (MAR)\n\n\\(E(Y^c) = E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is recorded\nApproach: multiple imputation\n\nMissing Not At Random (MNAR)\n\n\\(E(Y^c) \\ne E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is NOT recorded\nApproach: Model-based correction\n\n\n\n\n\n\n\nWhich non-parametric test of correlation can handle categorical variables?\nIf values are missing completely at random (MCAR), what should your approach be?\nIf values are missing at random (MAR), what should your approach be?\n\nBack"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#coding-concepts",
    "href": "fall_24/b707/notes/september/sep_4.html#coding-concepts",
    "title": "September 4, 2024",
    "section": "",
    "text": "What’s the difference between the joins? Left, right, inner, outer, and full - not to mention the option to exclude A from B, etc.\nWhat’s the difference between merge and join?\n\nMerge loses the order of observations"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#describing-variables",
    "href": "fall_24/b707/notes/september/sep_4.html#describing-variables",
    "title": "September 4, 2024",
    "section": "",
    "text": "Central tendancy\n\nMean, median, mode\n\nVariability\n\nStandard deviation, IQR\n\nSkewness\n\n\n\n\n\nCorrelation\n\nWhat’s the difference between Pearson’s \\(r\\), Spearman’s \\(\\rho\\), and Kendall’s \\(\\tau\\)?\n\nPearson’s \\(r\\) is parametric\nSpearman’s \\(\\rho\\) is non-parametric and is rank-based\nKendall’s \\(\\tau\\) is non-parametric and can handle categorical variables\n\n\nVariability by group\nTrajectory\n\nSlope"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#missing-data",
    "href": "fall_24/b707/notes/september/sep_4.html#missing-data",
    "title": "September 4, 2024",
    "section": "",
    "text": "Univariate non-response\n\nSingle variable missing data\n\nMultivariate two patterns\n\nI think this is when there are two patterns in the data. Some variables have no missing data, and other variables have missing data in the same places\n\nMonotone\n\nThis is when missingness is exacerbated with variables. For example, let’s say we yave X1, X2, and X3. X1 is missing values, and because X2 and X3 depend on X1, any missingness in X1 is also present in X2 and X3. Then suppose X3 depends on X2, any missingness in X2 shows up in X3 as well. Finally, X3 may have its own missing values. What we get is a steady increase in missingness from X1 to X2 and X3.\n\n\nGeneral\n\nNo patterns are evidence\n\nFile matching\n\nA good example is in a full join of two datasets. Suppose one dataset had variable A and the other had variable B, and both datasets had unique individuals. We will observe a pattern where there is no common missingness for A and B, nor will there be any individual who has both A and B present.\n\nFactor analysis\n\nThis is when there is an entire variable is missing\n\n\n\n\n\n\nMissing Completely At Random (MCAR)\n\n\\(E(Y^c) = E(Y^O)\\)\nA patient is scheduled for a visit but breaks his arms while skiing\nApproach: use complete case (exclude missing values)\n\nMissing At Random (MAR)\n\n\\(E(Y^c) = E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is recorded\nApproach: multiple imputation\n\nMissing Not At Random (MNAR)\n\n\\(E(Y^c) \\ne E(Y^O | X^O)\\)\nA patient is scheduled for a visit but does not come because he is sick; the sickness is NOT recorded\nApproach: Model-based correction"
  },
  {
    "objectID": "fall_24/b707/notes/september/sep_4.html#three-take-aways",
    "href": "fall_24/b707/notes/september/sep_4.html#three-take-aways",
    "title": "September 4, 2024",
    "section": "",
    "text": "Which non-parametric test of correlation can handle categorical variables?\nIf values are missing completely at random (MCAR), what should your approach be?\nIf values are missing at random (MAR), what should your approach be?\n\nBack"
  },
  {
    "objectID": "fall_24/b707/projects/hw1.html",
    "href": "fall_24/b707/projects/hw1.html",
    "title": "BIOSTAT 707 - Homework 1",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\nb707_hw1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn [ ]:\n\n\n\n\nimport pandas as pd\nfrom numpy import floor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions¶\n\n\n\nThe goal is to get comfortable with Jupyter notebook and executable documents. You will need to submit a Jupyter notebook that should compile by itself along with the pdf. If you use any special packages make sure that their use is documented so that the user (me!) knows what needs to be loaded.\n\n\nIf your program does not compile properly it will be sent back and you will be asked to submit another version.\n\n\n\nThe website https://www.causeweb.org/tshs/surgery-timing/ describes a study looking at the time of day a surgery is performed. Download and read the associated article: “Operation timing and 30-day mortality after elective general surgery” by Sessler et al. posted on the Sakai website.\n\n\nThe study data are available in a number of formats on the website. Load the RData object directly from the website. Use the command: load(url(“http://mywebsite.com/mydata.RData”)) where ‘mywebsite.com/mydata.RData’ refers to the link address of the data you wish to use.\n\n\nRecreate Tables 1 & 2 from the paper. Comment on any discrepancies. (You don’t need to format the tables in the same way)\n\n\nFigures 3 & 4 report adjusted probability estimates. What were these probabilities adjusted for? Is this study reproducible based on the published article?\n\n\n\n\nWork¶\n\n\nLet’s start by loading in the data and building table 1.\n\n\nTable 1¶\n\n\n\n\n\n\n\n\n\n\n\n\nIn [ ]:\n\n\n\n\nimport pandas as pd\n\n# Import data\ndata_url = \"https://causeweb.org/tshs/datasets/Surgery%20Timing.xlsx\"\nsurgery_dataset = pd.read_excel(data_url)\n\n# Specify variables for table 1\nbaseline_vars = [\n    \"age\",\n    \"gender\",\n    \"race\",\n    \"asa_status\",\n    \"bmi\",\n    \"baseline_cancer\",\n    \"baseline_cvd\",\n    \"baseline_dementia\",\n    \"baseline_diabetes\",\n    \"baseline_osteoart\",\n    \"baseline_psych\",\n    \"baseline_pulmonary\",\n    \"baseline_charlson\",\n    \"mortality_rsi\",\n    \"complication_rsi\",\n]\n\ncontinuous_vars = [\n    \"age\",\n    \"bmi\",\n    \"baseline_charlson\",\n    \"mortality_rsi\",\n    \"complication_rsi\",\n]\ncategorical_vars = [\n    \"gender\",\n    \"race\",\n    \"asa_status\",\n    \"baseline_cancer\",\n    \"baseline_cvd\",\n    \"baseline_dementia\",\n    \"baseline_diabetes\",\n    \"baseline_osteoart\",\n    \"baseline_psych\",\n    \"baseline_pulmonary\",\n]\n\n# Create the first table\ntable_1 = pd.DataFrame({\"Factor\": [], \"Statistic\": []})\n\n# Summary for continuous variables\ncontinuous_summary = (\n    surgery_dataset[continuous_vars].agg([\"mean\", \"std\"]).transpose()\n)\ncontinuous_summary = continuous_summary.rename(\n    columns={\"mean\": \"Mean\", \"std\": \"Std. Dev\"}\n)\n\n\n# Summary for categorical variables\nall_cat_summaries: dict[str, dict[str, list[any]]] = {}\n\nfor var in categorical_vars:\n    counts = surgery_dataset[var].value_counts()\n    percentages = surgery_dataset[var].value_counts(normalize=True) * 100\n    cat_summary = {\n        \"Level\": counts.index,\n        \"Count\": counts.values,\n        \"Percentage\": percentages.values,\n    }\n    all_cat_summaries[var] = cat_summary\n\ntable_index = 0\n# Create a new table with formatted values\n# Example corrected code to add rows:\nfor var in baseline_vars:\n    if var in continuous_vars:\n        # Corrected to use .iloc and assign correctly formatted summary stats\n        var_mean = round(continuous_summary.loc[var].iloc[0], 2)\n        var_std_dev = round(continuous_summary.loc[var].iloc[1], 3)\n        table_1.loc[table_index] = [var, f\"{var_mean} ± {var_std_dev}\"]\n        table_index += 1\n    elif var in categorical_vars:\n        var_info = all_cat_summaries[var]\n        var_levels = var_info[\"Level\"]\n        var_counts = var_info[\"Count\"]\n        var_percents = var_info[\"Percentage\"]\n\n        table_1.loc[table_index] = [var, \"\"]\n        table_index += 1\n\n        for i in range(len(var_levels)):\n            level = var_levels[i]\n            lvl_count = var_counts[i]\n            lvl_pct = var_percents[i]\n            table_1.loc[table_index] = [\n                f\" {level}\",\n                f\"{lvl_count} ({round(lvl_pct, 1)})\",\n            ]\n            table_index += 1\n    else:\n        raise Exception(f\"Variable {var} was not included\\n\")\n\nprint(table_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                Factor       Statistic\n0                  age  57.66 ± 15.044\n1               gender                \n2                  0.0    17230 (53.8)\n3                  1.0    14768 (46.2)\n4                 race                \n5                  1.0    26488 (84.0)\n6                  2.0     3790 (12.0)\n7                  3.0      1243 (3.9)\n8           asa_status                \n9                  1.0    17261 (54.0)\n10                 2.0    13677 (42.7)\n11                 3.0      1055 (3.3)\n12                 bmi   29.45 ± 7.268\n13     baseline_cancer                \n14                   0    21043 (65.8)\n15                   1    10958 (34.2)\n16        baseline_cvd                \n17                   1    16176 (50.5)\n18                   0    15825 (49.5)\n19   baseline_dementia                \n20                   0    31759 (99.2)\n21                   1       242 (0.8)\n22   baseline_diabetes                \n23                   0    27835 (87.0)\n24                   1     4166 (13.0)\n25   baseline_osteoart                \n26                   0    26282 (82.1)\n27                   1     5719 (17.9)\n28      baseline_psych                \n29                   0    29091 (90.9)\n30                   1      2910 (9.1)\n31  baseline_pulmonary                \n32                   0    28508 (89.1)\n33                   1     3493 (10.9)\n34   baseline_charlson    1.18 ± 1.875\n35       mortality_rsi   -0.53 ± 1.038\n36    complication_rsi   -0.41 ± 1.204\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2¶\n\n\n\n\n\n\n\n\n\n\n\n\nIn [ ]:\n\n\n\n\n# Create Table 2\ntable_2 = pd.DataFrame({\"Factor\": [], \"N (%) of cases\": []})\ntable_2_vars = [\"hour\", \"dow\", \"month\", \"moonphase\"]\nall_tab2_summaries: dict[str, dict[str, list[any]]] = {}\ntable_2_data = surgery_dataset\ntable_2_data[\"hour\"] = floor(surgery_dataset[\"hour\"])\nfor var in table_2_vars:\n    counts = surgery_dataset[var].value_counts()\n    percentages = surgery_dataset[var].value_counts(normalize=True) * 100\n    cat_summary = {\n        \"Level\": counts.index,\n        \"Count\": counts.values,\n        \"Percentage\": percentages.values,\n    }\n    all_cat_summaries[var] = cat_summary\n\ntable_index = 0\nfor var in table_2_vars:\n    var_info = all_cat_summaries[var]\n    var_levels = var_info[\"Level\"]\n    var_counts = var_info[\"Count\"]\n    var_percents = var_info[\"Percentage\"]\n\n    table_2.loc[table_index] = [var, \"\"]\n    table_index += 1\n\n    for i in range(len(var_levels)):\n        level = var_levels[i]\n        lvl_count = var_counts[i]\n        lvl_pct = var_percents[i]\n        table_2.loc[table_index] = [\n            f\" {level}\",\n            f\"{lvl_count} ({round(lvl_pct, 1)})\",\n        ]\n        table_index += 1\nprint(table_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       Factor N (%) of cases\n0        hour               \n1         7.0   10631 (33.2)\n2         8.0    3807 (11.9)\n3        11.0     2855 (8.9)\n4        12.0     2763 (8.6)\n5        13.0     2623 (8.2)\n6        10.0     2501 (7.8)\n7        14.0     2063 (6.4)\n8         9.0     1664 (5.2)\n9        15.0     1267 (4.0)\n10       16.0      745 (2.3)\n11        6.0      562 (1.8)\n12       17.0      356 (1.1)\n13       18.0      163 (0.5)\n14       19.0        1 (0.0)\n15        dow               \n16          2    7008 (21.9)\n17          1    7005 (21.9)\n18          3    6266 (19.6)\n19          5    6087 (19.0)\n20          4    5635 (17.6)\n21      month               \n22          9    3208 (10.0)\n23          8     3177 (9.9)\n24          6     2994 (9.4)\n25          4     2698 (8.4)\n26          3     2697 (8.4)\n27         10     2689 (8.4)\n28          1     2670 (8.3)\n29          5     2654 (8.3)\n30         11     2544 (7.9)\n31          2     2506 (7.8)\n32          7     2325 (7.3)\n33         12     1839 (5.7)\n34  moonphase               \n35          4    8142 (25.4)\n36          2    8100 (25.3)\n37          3    8051 (25.2)\n38          1    7708 (24.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside from the fact that my tables were largely out of order (and the awkward moon-phase issue in my table), there are only a few discrepancies.\n\n\n\nTable 1\n\n\nBMI is different, most likely in how they handled missing values. No documentation is recorded\n\n\nI failed in my table to output the risk factors using mean and IQR\n\n\n\n\nTable 2\n\n\nI did not see the correct way to concatenate “moonphase,” so I left it as it was. This created an obvious problem\n\n\nThey did not include the 1 case at 19:00. I don’t see that this was documented, but I can clearly see that they lumped it in with 18:00\n\n\n\n\n\nFigures 3 and 4¶\n\n\nNow let’s talk about figures 3 and 4.\n\n\nWhat I believe they mean when they say that the probabilities were “adjusted” was that individuals were ommitted from the model based on Risk Stratification Index (RSI).\n\n\nConclusion¶\n\n\nDo I think this study is reproducible? No. At best, it may be replicable, but I’m left with an unclear sense of who and who was not excluded from the model. Figure 2 states that 76 individuals were excluded because of unavailable 30-day mortality information, but it doesn’t say anything about individuals excluded for RSI."
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html",
    "href": "fall_24/b719/notes/august/aug_27.html",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "",
    "text": "The textbook for this course is “An Introduction to Generalized Linear Models, Fourth Edition” by Chapman and Hall.\nShe uses R and SAS.\nCourse grading scale:\n\nAttendance and pop quiz: 5%\nHomework: 20%\nMidterm: 25%\nFinal: 30%\nGroup Research Project: 20%\n\nHomework\n\nThere will be 9 weekly homework assignments. Each are due at the beginning of the class on the due day.\nSubmit as a PDF document.\n10% deduction in score for late submissions.\n\nExams\n\nMidterm and final exams will be in-class.\nThere is a one-page cheat sheet allowed (double-sided).\nRules for exams:\n\nOnly bring calculator (non-graphing) and pencil + eraser\nExams are proctored.\n\n\nGroup research project\n\nWill require analysis of a data set, final report, and final presentation.\n\nPop quizzes\n\nThe primary purpose of the pop quiz is to check attendance.\n\nCommunications\n\nTeams channel in MS Teams\n\nAI Policies\n\nAI is allowed. However, be transparent about use.\nFact checking is my own responsibility.\nAI-generated content without proper acknowledgement or references will be treated as plagiarism.\n\n\n\n\n\n\n\n\nHere’s the Syllabus\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake ownership of your projects. This is one of the hardest things to teach to students.\nMake a check-list for all that needs to be done in all the analyses you come across.\nHwanhee once met an incredibly driven student that was studying in her program while simultaneously working on a part-time PhD program and conducting government research in Korea. But even with his full plate, he took meticulous notes during each class period and would pester her about the lectures. She said that she was annoyed at him and asked him why he cared so much. He responded, “This is my last chance to go through this course material in this class. I won’t get another chance, and I have to learn this now.”"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#syllabus",
    "href": "fall_24/b719/notes/august/aug_27.html#syllabus",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "",
    "text": "The textbook for this course is “An Introduction to Generalized Linear Models, Fourth Edition” by Chapman and Hall.\nShe uses R and SAS.\nCourse grading scale:\n\nAttendance and pop quiz: 5%\nHomework: 20%\nMidterm: 25%\nFinal: 30%\nGroup Research Project: 20%\n\nHomework\n\nThere will be 9 weekly homework assignments. Each are due at the beginning of the class on the due day.\nSubmit as a PDF document.\n10% deduction in score for late submissions.\n\nExams\n\nMidterm and final exams will be in-class.\nThere is a one-page cheat sheet allowed (double-sided).\nRules for exams:\n\nOnly bring calculator (non-graphing) and pencil + eraser\nExams are proctored.\n\n\nGroup research project\n\nWill require analysis of a data set, final report, and final presentation.\n\nPop quizzes\n\nThe primary purpose of the pop quiz is to check attendance.\n\nCommunications\n\nTeams channel in MS Teams\n\nAI Policies\n\nAI is allowed. However, be transparent about use.\nFact checking is my own responsibility.\nAI-generated content without proper acknowledgement or references will be treated as plagiarism.\n\n\n\n\n\n\n\n\nHere’s the Syllabus"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#three-take-aways",
    "href": "fall_24/b719/notes/august/aug_27.html#three-take-aways",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "",
    "text": "Take ownership of your projects. This is one of the hardest things to teach to students.\nMake a check-list for all that needs to be done in all the analyses you come across.\nHwanhee once met an incredibly driven student that was studying in her program while simultaneously working on a part-time PhD program and conducting government research in Korea. But even with his full plate, he took meticulous notes during each class period and would pester her about the lectures. She said that she was annoyed at him and asked him why he cared so much. He responded, “This is my last chance to go through this course material in this class. I won’t get another chance, and I have to learn this now.”"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#review-of-linear-regression-concepts",
    "href": "fall_24/b719/notes/august/aug_27.html#review-of-linear-regression-concepts",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Review of Linear Regression Concepts",
    "text": "Review of Linear Regression Concepts\n\nSay we have two variables, X and Y (predictor and response). Y (the response or outcome) is regarded as a random variable. Explanatory variables (X) are treated as fixed by the experimental design.\nTypes of outcomes:\n\nContinous (e.g. BMI, SBP) - linear regression\nBinary (e.g. death \\(\\in\\) {1, 0}) - logistic regression\nCategorical (nominal, ordinal) - nominal/ordinal logistic regression\nCounts (e.g. number of hospitalizations) - Poisson regression"
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#likelihood-and-mles",
    "href": "fall_24/b719/notes/august/aug_27.html#likelihood-and-mles",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Likelihood and MLEs",
    "text": "Likelihood and MLEs\n\n\\(f(y; \\theta)\\) is a probability distribution, where \\(\\theta\\) represents the parameters of the distribution\n\\(L(\\theta; y)\\) is a likelihood function: \\(L(\\theta; y) = \\prod_{i = 1}^n f(y; \\theta)\\)\n\\(l(\\theta; y)\\) is the log likelihood function: \\(l(\\theta; y) = \\sum log f(y; \\theta)\\)\nMLE \\(\\hat{\\theta}\\) of parameter $$ is the value which maximizes the likelihood function: \\(L(\\hat{\\theta}; y) \\ge L(\\theta; y)\\), for all \\(\\theta \\in \\Omega\\) (this is also true for the log-likelihood function)\nHow do you get the MLE? Take the derivative of the log-likelihood function, set to 0, and solve. Then take the second derivative and check the sign."
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#linear-model-specifications",
    "href": "fall_24/b719/notes/august/aug_27.html#linear-model-specifications",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Linear Model Specifications",
    "text": "Linear Model Specifications\n\nModel Fitting Process:\n\n\nSpecify model\n\nFind the probability distribution of Y (what type of variable? what tests are appropriate? etc.)\nEquation linking response and explanatory variable\n\nEstimation of parameters in the model\ncheck adequacy of model (residuals, deviance, AIC, etc.)\nMake inference - confidence intervals, interpretation of results, hypothesis testing\n\n\nIn class, there was an example with women from the town and country (1, 0) and some sort of response variable (I think it was a count). Hwanhee said that in this case, where we have a factor variable, there are two main ways we can think about Y. First, we can construct our Y vector as a \\(n\\text{ x } 1\\) dimmensional vector. The model formula, then, would look like this: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon\\), where \\(X_i = \\begin{cases}1, \\quad \\text{From town}\\\\0, \\quad \\text{From city} \\end{cases}\\). The other way to think about this is to construct our \\(Y\\) vector to be \\(j \\text{ x } k\\) dimmensional, where \\(j\\) is the number of levels of our factor variable and \\(k\\) is the number of individuals in group \\(j\\). In this case, the model formula would be \\(Y_{jk} = \\mu + \\alpha_j + \\epsilon_k\\), where \\(\\mu\\) is the mean of the reference group and \\(\\alpha\\) is the difference between the means of the reference group and group \\(j\\)."
  },
  {
    "objectID": "fall_24/b719/notes/august/aug_27.html#three-take-aways-1",
    "href": "fall_24/b719/notes/august/aug_27.html#three-take-aways-1",
    "title": "August 27, 2024 - August 29, 2024",
    "section": "Three Take-aways",
    "text": "Three Take-aways\n\nFor a response and explanatory variable, X is considered fixed and Y is considered a random variable.\nThe four steps for the calculus method for finding an MLE is as follows:\n\nFind the log-likelihood function: \\(\\ell (\\theta | y) = \\sum_{i = 1}^n \\log \\left( f(y; \\theta)\\right)\\)\nTake the derivative\nSet to zero and solve\nTake the second derivative\n\nThe are two primary ways of structuring a model formula with a factor-level variable:\n\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\), where \\(X\\) is the factor-level variable\n\\(Y_{jk} = \\mu + Z_{jk} \\alpha + epsilon_{jk}\\), where \\(\\mu\\) is the mean of the reference group and \\(\\alpha\\) is the difference in means of the two groups (assuming two groups)\n\n\nBack"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_10.html",
    "href": "fall_24/b719/notes/september/sep_10.html",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "This method can be used anytime we are unable to use another method (like the calculus method) for some reason (e.g. closed form, unable to differentiate etc.)\nThe underlying theory for this method is to find tangential lines to the first derivative of the log-likelihood function at informed values of \\(\\theta\\). As we update \\(\\theta\\), we will approach the tangential line at \\(\\theta^{(m)}\\) where \\(y(\\theta^{(m)}) = 0\\)\nMethod:\n\nStart with a reasonable value for \\(\\theta^{(0)}\\). For example, you might try the sample mean or a value close to what you believe could represent the MLE\nI’m going to be skipping some of the math in this step and cutting to the chase. Again, you can find this formula by solving for the equation of the line tangent to the score function at \\(\\theta^{(m)}\\). \\(\\theta^{(m)} = \\theta^{(m - 1)} - \\frac{U(\\theta^{(m - 1)})}{U'(\\theta^{(m - 1)})}\\)\nIterate through values of \\(m\\) until \\(\\theta^{(m)} - \\theta^{(m - 1)}\\) is satisfactorily small\n\nHere’s an example in R that looks at data distributed i.i.d. from a Poisson distribution: \\(Y_i \\sim POI(\\theta)\\)\n\n\n###################################### R ######################################\n# Count of cyclones distributed as Y_i \\sim POI(\\theta)\nseason &lt;- 1:13\ncyclone_count &lt;- c(6, 5, 4, 6, 6, 3, 12, 7, 4, 2, 6, 7, 4)\n\n# Function to find U and dU of Poisson distribution\npoisson_function &lt;- function(y, theta) {\n    # y: vector of sample values\n    # theta: arbitrary value of theta for log-liklihood input\n    n &lt;- length(y)\n    u &lt;- sum(y) * (1 / theta) - n\n    du &lt;- -sum(y) * (1 / theta^2)\n    return(list(u = u, du = du))\n}\n\ntheta0 &lt;- 3.5\ntmp.iter &lt;- NULL\nfor (i in 1:10) {\n    one &lt;- poisson_function(cyclone_count, theta0)\n    tmp.iter &lt;- rbind(tmp.iter, c(theta = theta0, u = one$u, du = one$du))\n    theta0 &lt;- theta0 - one$u / one$du\n}\n\n# Function to find MLE through the Newton-Raphson algorithm\nnewton_raphson_algorithm &lt;- function(f, y, theta_0) {\n    # f: function returning u and du of given PDF\n    # y: vector of sample values\n    # theta_0: initial value for theta\n\n    # Set theta_m for algorithm\n    theta_m &lt;- theta_0 + 1\n    # Set return matrix to null\n    return_matrix &lt;- NULL\n    # Set counter to protect against endless looping\n    loop_counter &lt;- 0\n    while (theta_m - theta_0 &gt; 0.0000001 & loop_counter &lt; 1000) {\n        if (loop_counter &gt; 0) {\n            theta_0 &lt;- theta_m\n        }\n        function_list &lt;- f(y, theta_0)\n        u &lt;- function_list$u\n        du &lt;- function_list$du\n        return_matrix &lt;- rbind(\n            return_matrix, c(theta = theta_0, u = u, du = du)\n        )\n        theta_m &lt;- theta_0 - u / du\n        loop_counter &lt;- loop_counter + 1\n    }\n\n    # Return the matrix\n    return(return_matrix)\n}\n\n# Let's test it out!\nmy_matrix &lt;- newton_raphson_algorithm(\n    poisson_function, cyclone_count, 3.5\n)\nprint(paste(\"The mean of the Cyclone Counts is\", mean(cyclone_count)))\n\n[1] \"The mean of the Cyclone Counts is 5.53846153846154\"\n\nprint(\"Matrix of Algorithm Iterations:\")\n\n[1] \"Matrix of Algorithm Iterations:\"\n\nprint(my_matrix)\n\n        theta            u        du\n[1,] 3.500000 7.571429e+00 -5.877551\n[2,] 4.788194 2.036983e+00 -3.140429\n[3,] 5.436827 2.430192e-01 -2.435799\n[4,] 5.536596 4.379219e-03 -2.348804\n[5,] 5.538461 1.474204e-06 -2.347223\n[6,] 5.538462 1.669775e-13 -2.347222\n\n###############################################################################\n\n\n\n\n\nThe method of scoring algorithm is extremely similar to the Newton-Raphson algorithm. However, if you know that your distribution meets the requirements for GLMs, then you can use the method of scoring algorithm. This can be easier to calculate (because of GLM properties), and it’s also a little more efficient than the Newton-Raphon algorithm\nThis is built on the idea that we can approximate \\(U'\\) with \\(E[U']\\), and \\(I = -E[U']\\)\nMethod\n\nSimilar to the NRA, we start with some value for \\(\\theta^{(0)}\\). We then implement the same iterative process as before, using this new formula to calculate \\(\\theta^{(m)}\\): \\(\\theta^{(m)} = \\theta^{(m - 1)} + \\frac{U(\\theta^{(m - 1)})}{I(\\theta^{(m - 1)})}\\)\n\n\n\n\n\n\n\nWhat is the formula to calculate ^{(m)} for the Newton-Raphson algorithm?\nWhat is the formula to calculate ^{(m)} for the method of scoring algorithm?\nThe Newton-Raphson algorithm and the method of scoring are very similar, but the method of scoring can be easier to implement and more efficient in application. What requirements must be satisfied to use this method?\n\n^{(m)} = ^{(m - 1)} - \n^{(m)} = ^{(m - 1)} + \nPDF must satisfy reqirements for GLMs"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_10.html#numerical-methods-for-approximating-mles",
    "href": "fall_24/b719/notes/september/sep_10.html#numerical-methods-for-approximating-mles",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "This method can be used anytime we are unable to use another method (like the calculus method) for some reason (e.g. closed form, unable to differentiate etc.)\nThe underlying theory for this method is to find tangential lines to the first derivative of the log-likelihood function at informed values of \\(\\theta\\). As we update \\(\\theta\\), we will approach the tangential line at \\(\\theta^{(m)}\\) where \\(y(\\theta^{(m)}) = 0\\)\nMethod:\n\nStart with a reasonable value for \\(\\theta^{(0)}\\). For example, you might try the sample mean or a value close to what you believe could represent the MLE\nI’m going to be skipping some of the math in this step and cutting to the chase. Again, you can find this formula by solving for the equation of the line tangent to the score function at \\(\\theta^{(m)}\\). \\(\\theta^{(m)} = \\theta^{(m - 1)} - \\frac{U(\\theta^{(m - 1)})}{U'(\\theta^{(m - 1)})}\\)\nIterate through values of \\(m\\) until \\(\\theta^{(m)} - \\theta^{(m - 1)}\\) is satisfactorily small\n\nHere’s an example in R that looks at data distributed i.i.d. from a Poisson distribution: \\(Y_i \\sim POI(\\theta)\\)\n\n\n###################################### R ######################################\n# Count of cyclones distributed as Y_i \\sim POI(\\theta)\nseason &lt;- 1:13\ncyclone_count &lt;- c(6, 5, 4, 6, 6, 3, 12, 7, 4, 2, 6, 7, 4)\n\n# Function to find U and dU of Poisson distribution\npoisson_function &lt;- function(y, theta) {\n    # y: vector of sample values\n    # theta: arbitrary value of theta for log-liklihood input\n    n &lt;- length(y)\n    u &lt;- sum(y) * (1 / theta) - n\n    du &lt;- -sum(y) * (1 / theta^2)\n    return(list(u = u, du = du))\n}\n\ntheta0 &lt;- 3.5\ntmp.iter &lt;- NULL\nfor (i in 1:10) {\n    one &lt;- poisson_function(cyclone_count, theta0)\n    tmp.iter &lt;- rbind(tmp.iter, c(theta = theta0, u = one$u, du = one$du))\n    theta0 &lt;- theta0 - one$u / one$du\n}\n\n# Function to find MLE through the Newton-Raphson algorithm\nnewton_raphson_algorithm &lt;- function(f, y, theta_0) {\n    # f: function returning u and du of given PDF\n    # y: vector of sample values\n    # theta_0: initial value for theta\n\n    # Set theta_m for algorithm\n    theta_m &lt;- theta_0 + 1\n    # Set return matrix to null\n    return_matrix &lt;- NULL\n    # Set counter to protect against endless looping\n    loop_counter &lt;- 0\n    while (theta_m - theta_0 &gt; 0.0000001 & loop_counter &lt; 1000) {\n        if (loop_counter &gt; 0) {\n            theta_0 &lt;- theta_m\n        }\n        function_list &lt;- f(y, theta_0)\n        u &lt;- function_list$u\n        du &lt;- function_list$du\n        return_matrix &lt;- rbind(\n            return_matrix, c(theta = theta_0, u = u, du = du)\n        )\n        theta_m &lt;- theta_0 - u / du\n        loop_counter &lt;- loop_counter + 1\n    }\n\n    # Return the matrix\n    return(return_matrix)\n}\n\n# Let's test it out!\nmy_matrix &lt;- newton_raphson_algorithm(\n    poisson_function, cyclone_count, 3.5\n)\nprint(paste(\"The mean of the Cyclone Counts is\", mean(cyclone_count)))\n\n[1] \"The mean of the Cyclone Counts is 5.53846153846154\"\n\nprint(\"Matrix of Algorithm Iterations:\")\n\n[1] \"Matrix of Algorithm Iterations:\"\n\nprint(my_matrix)\n\n        theta            u        du\n[1,] 3.500000 7.571429e+00 -5.877551\n[2,] 4.788194 2.036983e+00 -3.140429\n[3,] 5.436827 2.430192e-01 -2.435799\n[4,] 5.536596 4.379219e-03 -2.348804\n[5,] 5.538461 1.474204e-06 -2.347223\n[6,] 5.538462 1.669775e-13 -2.347222\n\n###############################################################################\n\n\n\n\n\nThe method of scoring algorithm is extremely similar to the Newton-Raphson algorithm. However, if you know that your distribution meets the requirements for GLMs, then you can use the method of scoring algorithm. This can be easier to calculate (because of GLM properties), and it’s also a little more efficient than the Newton-Raphon algorithm\nThis is built on the idea that we can approximate \\(U'\\) with \\(E[U']\\), and \\(I = -E[U']\\)\nMethod\n\nSimilar to the NRA, we start with some value for \\(\\theta^{(0)}\\). We then implement the same iterative process as before, using this new formula to calculate \\(\\theta^{(m)}\\): \\(\\theta^{(m)} = \\theta^{(m - 1)} + \\frac{U(\\theta^{(m - 1)})}{I(\\theta^{(m - 1)})}\\)"
  },
  {
    "objectID": "fall_24/b719/notes/september/sep_10.html#three-take-aways",
    "href": "fall_24/b719/notes/september/sep_10.html#three-take-aways",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "What is the formula to calculate ^{(m)} for the Newton-Raphson algorithm?\nWhat is the formula to calculate ^{(m)} for the method of scoring algorithm?\nThe Newton-Raphson algorithm and the method of scoring are very similar, but the method of scoring can be easier to implement and more efficient in application. What requirements must be satisfied to use this method?\n\n^{(m)} = ^{(m - 1)} - \n^{(m)} = ^{(m - 1)} + \nPDF must satisfy reqirements for GLMs"
  },
  {
    "objectID": "fall_24/b719/projects/hw_1.html",
    "href": "fall_24/b719/projects/hw_1.html",
    "title": "BIOSTAT 719 - Homework 1",
    "section": "",
    "text": "(6 points) [Textbook Exercise 1.6]\n\nThe data in Table 1.4 are the numbers of females and males in the progeny of 16 female light brown apple moths in Muswellbrook, New South Wales, Australia (from Lewis 1987).\n\n\n\n\n\n\n\n\n\n\n\nProgeny Group\nFemales\nMales\n\n\n\n\n1\n18\n11\n\n\n2\n31\n22\n\n\n3\n34\n27\n\n\n4\n33\n29\n\n\n5\n27\n24\n\n\n6\n33\n29\n\n\n7\n28\n25\n\n\n8\n23\n26\n\n\n9\n33\n38\n\n\n10\n12\n14\n\n\n11\n19\n23\n\n\n12\n25\n31\n\n\n13\n14\n20\n\n\n14\n4\n6\n\n\n15\n22\n34\n\n\n16\n7\n12\n\n\n\n\n\n\nLet \\(Y_i\\) denote the number of females in each of the 16 groups of progeny in each group (\\(i = 1, ..., 16\\)). Suppose the \\(Y_i\\)’s are independent random variables each with the Binomial distribution\n\n\\[\nf(y_i; \\theta) = {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}\n\\]\n\nFind the maximum likelihood estimator (MLE) of \\(\\theta\\) using calculus and evaluate it for these data.\n\nLet’s first find the MLE using calculus. Let’s review the steps:\n\nCalculate the log-likelihood function\nTake the first derivative of the log-likelihood function (this is the ‘score’ function)\nSet the score function equal to 0 and solve for \\(\\hat{\\theta}\\)\nTake the second derivative of the function to determine whether the function is concave down (i.e. has a maximum value at \\(\\hat{\\theta}\\))\n\nWith that in mind, let’s jump in!\n\\[\\begin{align*}\n\\ell (\\theta_i ; y_i) &= \\sum_{i = 1}^{16} \\log\\left( f(y_i; \\theta) \\right)\\\\\n&= \\sum_{i = 1}^{16} \\log\\left[ {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}  \\right]\\\\\n&= \\sum_{i = 1}^{16} \\log {n_i \\choose y_i} + \\log(\\theta)\\sum_{i = 1}^{16} y_i + log(1 - \\theta)\\sum_{i = 1}^{16}(n_i - y_i)  \\\\\n\\frac{d \\ell}{d\\theta} &= 0 + \\sum_{i = 1}^{16} \\frac{y_i}{\\theta} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\theta} \\\\\n\\underline{\\text{set}} \\quad 0 &=  \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} &= \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies  \\hat{\\theta} \\sum_{i = 1}^{16} n_i &= \\sum_{i = 1}^{16} y_i \\\\\n\\implies \\hat{\\theta} &= \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\n\\end{align*}\\]\nTo verify whether this is an MLE or not, let’s now complete step 4 and take the second derivative of the log-likelihood function to see whether it is concave down at \\(\\hat{\\theta}\\).\n\\[\\begin{align*}\n\\frac{d^2\\ell}{d\\theta^2} &= -\\sum_{i = 1}^{16} \\frac{y_i}{\\theta^2} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{(1 - \\theta)^2}\n\\end{align*}\\]\nNote that because \\(0 \\le \\theta \\le 1\\), and because \\(y = 0, 1, 2, ...\\), and because \\(n = 1, 2, 3, ...\\), the second derivative of the log-likelihood function will be negative for all values of \\(\\theta\\). That means that the first derivative has a maximum at \\(\\hat{\\theta}\\).\nThus, the MLE for a random sample of size 16 is \\(\\hat{\\theta} = \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\\). Intuitively, this makes sense, because this is essentially the total number of females in all progenies divided by the total number of individuals. This value for \\(\\hat{\\theta}\\) is evaluated to be \\(363/(371+363) = 0.49\\)."
  },
  {
    "objectID": "fall_24/b719/projects/hw_1.html#problem-1",
    "href": "fall_24/b719/projects/hw_1.html#problem-1",
    "title": "BIOSTAT 719 - Homework 1",
    "section": "",
    "text": "(6 points) [Textbook Exercise 1.6]\n\nThe data in Table 1.4 are the numbers of females and males in the progeny of 16 female light brown apple moths in Muswellbrook, New South Wales, Australia (from Lewis 1987).\n\n\n\n\n\n\n\n\n\n\n\nProgeny Group\nFemales\nMales\n\n\n\n\n1\n18\n11\n\n\n2\n31\n22\n\n\n3\n34\n27\n\n\n4\n33\n29\n\n\n5\n27\n24\n\n\n6\n33\n29\n\n\n7\n28\n25\n\n\n8\n23\n26\n\n\n9\n33\n38\n\n\n10\n12\n14\n\n\n11\n19\n23\n\n\n12\n25\n31\n\n\n13\n14\n20\n\n\n14\n4\n6\n\n\n15\n22\n34\n\n\n16\n7\n12\n\n\n\n\n\n\nLet \\(Y_i\\) denote the number of females in each of the 16 groups of progeny in each group (\\(i = 1, ..., 16\\)). Suppose the \\(Y_i\\)’s are independent random variables each with the Binomial distribution\n\n\\[\nf(y_i; \\theta) = {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}\n\\]\n\nFind the maximum likelihood estimator (MLE) of \\(\\theta\\) using calculus and evaluate it for these data.\n\nLet’s first find the MLE using calculus. Let’s review the steps:\n\nCalculate the log-likelihood function\nTake the first derivative of the log-likelihood function (this is the ‘score’ function)\nSet the score function equal to 0 and solve for \\(\\hat{\\theta}\\)\nTake the second derivative of the function to determine whether the function is concave down (i.e. has a maximum value at \\(\\hat{\\theta}\\))\n\nWith that in mind, let’s jump in!\n\\[\\begin{align*}\n\\ell (\\theta_i ; y_i) &= \\sum_{i = 1}^{16} \\log\\left( f(y_i; \\theta) \\right)\\\\\n&= \\sum_{i = 1}^{16} \\log\\left[ {n_i \\choose y_i}\\theta^{y_i}(1 - \\theta)^{n_i - y_i}  \\right]\\\\\n&= \\sum_{i = 1}^{16} \\log {n_i \\choose y_i} + \\log(\\theta)\\sum_{i = 1}^{16} y_i + log(1 - \\theta)\\sum_{i = 1}^{16}(n_i - y_i)  \\\\\n\\frac{d \\ell}{d\\theta} &= 0 + \\sum_{i = 1}^{16} \\frac{y_i}{\\theta} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\theta} \\\\\n\\underline{\\text{set}} \\quad 0 &=  \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies \\sum_{i = 1}^{16} \\frac{y_i}{\\hat{\\theta}} &= \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{1 - \\hat{\\theta}} \\\\\n\\implies  \\hat{\\theta} \\sum_{i = 1}^{16} n_i &= \\sum_{i = 1}^{16} y_i \\\\\n\\implies \\hat{\\theta} &= \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\n\\end{align*}\\]\nTo verify whether this is an MLE or not, let’s now complete step 4 and take the second derivative of the log-likelihood function to see whether it is concave down at \\(\\hat{\\theta}\\).\n\\[\\begin{align*}\n\\frac{d^2\\ell}{d\\theta^2} &= -\\sum_{i = 1}^{16} \\frac{y_i}{\\theta^2} - \\frac{\\sum_{i = 1}^{16} n_i - \\sum_{i = 1}^{16} y_i}{(1 - \\theta)^2}\n\\end{align*}\\]\nNote that because \\(0 \\le \\theta \\le 1\\), and because \\(y = 0, 1, 2, ...\\), and because \\(n = 1, 2, 3, ...\\), the second derivative of the log-likelihood function will be negative for all values of \\(\\theta\\). That means that the first derivative has a maximum at \\(\\hat{\\theta}\\).\nThus, the MLE for a random sample of size 16 is \\(\\hat{\\theta} = \\frac{1}{\\sum_{i = 1}^{16} n_i} \\sum_{i = 1}^{16} y_i\\). Intuitively, this makes sense, because this is essentially the total number of females in all progenies divided by the total number of individuals. This value for \\(\\hat{\\theta}\\) is evaluated to be \\(363/(371+363) = 0.49\\)."
  },
  {
    "objectID": "fall_24/b719/projects/hw_1.html#problem-2",
    "href": "fall_24/b719/projects/hw_1.html#problem-2",
    "title": "BIOSTAT 719 - Homework 1",
    "section": "Problem 2",
    "text": "Problem 2\n(12 points)\n\nLet \\(Y_i, ... , Y_n\\) be independent random variables from the exponential distribution\n\n\\[\nf(y_i; \\lambda) = \\lambda e^{-\\lambda y_i}, \\quad y_i &gt; 0, \\lambda &gt; 0\n\\]\nPart (a):\n\nWhat is the maximum likelihood estimator (MLE) of \\(\\lambda\\)? Show all the derivation details.\n\nLet’s follow the same four steps to find this out!\n\\[\\begin{align*}\n\\ell(\\lambda) &= \\sum_{i = 1}^n \\log [f(y; \\lambda)]\\\\\n&= \\sum_{i = 1}^n \\log [\\lambda e^{-\\lambda y_i}]\\\\\n&= \\sum_{i = 1}^n \\log (\\lambda) - \\sum_{i = 1}^n\\lambda y_i\\\\\n&= n \\log (\\lambda) - \\sum_{i = 1}^n\\lambda y_i\\\\\n\\frac{d\\ell}{d\\lambda} &= \\frac{n}{\\lambda} - \\sum_{i = 1}^n y_i\\\\\n\\underline{\\text{set}}\\quad 0 &= \\frac{n}{\\hat{\\lambda}} - \\sum_{i = 1}^n y_i\\\\\n\\implies \\frac{n}{\\hat{\\lambda}} &= \\sum_{i = 1}^n y_i\\\\\n\\implies \\hat{\\lambda} &= \\frac{n}{\\sum_{i = 1}^n y_i}\\\\\n\\implies \\hat{\\lambda} &= \\frac{1}{\\bar{Y}}\n\\end{align*}\\]\nThis is a fun result, especially considering the fact that the rate parameter is the inverse of the scale parameter. So, by the invariance property of MLEs, we know that the MLE for \\(\\theta\\) (the scale parameter) is \\(\\bar{Y}\\).\nHowever, remember that we still need to check the second derivative to ensure this actually is an MLE.\nWe can easily see that the second derivative of the log-likelihood function, \\(\\frac{d^2\\ell}{d\\lambda^2}\\), is \\(-\\frac{n}{\\lambda^2}\\). Considering that neither \\(n\\) nor \\(\\lambda\\) can take on a negative value, the first derivative (i.e. the score function) will always be concave down, and we can state with certainty that the MLE for \\(\\lambda\\) is \\(\\frac{1}{\\bar{Y}}\\)\nPart (b):\n\nSuppose \\(\\lambda = e^{\\beta}\\). Find the MLE of \\(\\beta\\).\n\nWe are given that \\(\\lambda = e^{\\beta}\\). Thus, \\(\\beta = \\log{\\lambda}\\). By the invariance property of MLEs, if \\(g(\\lambda)\\) is a function of \\(\\lambda\\), then the MLE of \\(g(\\theta)\\) is \\(g(\\hat{\\theta})\\).\nLet \\(g(\\theta) = \\log(\\lambda) = \\beta\\). \\(g(\\hat{\\lambda}) = \\log\\left( \\frac{1}{\\bar{Y}}\\right)\\), which is the MLE of \\(\\beta\\).\nPart (c):\n\nConsider 150 observations \\(y_i, (i = 1, 2, ..., 150)\\) from the exponential distribution with the sum of the 150 observations equal to 30. What is the numerical evaluation of the MLE of \\(\\lambda\\) and \\(\\beta\\)?\n\nEasy peasy. We did all the hard stuff already, so it’s just plug and chug time!\nFor \\(\\hat{\\lambda}\\):\n\\[\\begin{align*}\n\\hat{\\lambda} &= \\frac{1}{\\bar{Y}}\\\\\n&= \\frac{1}{\\frac{30}{150}}\\\\\n&= 5\n\\end{align*}\\]\nFor \\(\\hat{\\beta}\\):\n\\[\\begin{align*}\n\\hat{\\beta} &= \\log\\left[\\frac{1}{\\bar{Y}}\\right]\\\\\n&= \\log(5)\n\\end{align*}\\]\nThus, the numerical evaluations of the MLEs of \\(\\lambda\\) and \\(\\beta\\) are \\(5\\) and \\(\\log(5)\\), respectively."
  },
  {
    "objectID": "fall_24/b823/index.html",
    "href": "fall_24/b823/index.html",
    "title": "Statistical Programming for Big Data",
    "section": "",
    "text": "September\n\n\nOctober\n\n\nNovember\nTo review notes taken during class, follow this link."
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html",
    "href": "fall_24/b823/notes/september/sep_10.html",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "Navigate to https://dcc-ondemand-01.oit.duke.edu\nLogin with Duke credentials when prompted\nSelect Interactive Apps from the menu bar\nSelect Jupyter Lab Apptainer\nSelect the number of hours estimated (probably a good idea to increase it by an hour)\nALWAYS increase the memory above the default of 2 (app will not launch properly)\nSelect path\n\n/opt/apps/containers/community/biostat/biostat-823-jupyter.sif\nHit Select once you’ve identified everything correctly\n\nHit Launch\n\n\n\n\n\nOne interesting note about joins is that we’re usually taking a step backwards from normalization. This is totally acceptable and appropriate. Sometimes we need to summarize data in a different or non-standard way\nInner Joins\n\nInner Joins are the default, but it’s always important to specify for clarity\nExample: SELECT c.Name AS Course, c.Semester, l.Name AS Lesson FROM Course AS c INNER JOIN Lesson AS l ON (c.Course_OID = l.Course_OID);\nNo null values (I believe… unless there were existing NULL values in the selected columns before the join?)\nIf the column names are the same (as is the case in the example with Course_OID in both tables), we can shorten it to USING(...)\n\nOuter Joins\n\nSpecify whether it’s a left outer join or a right outer join\nKeeps all rows in the specified table (left or right), but it drops any rows in the adjacent table that do not match rows in the “outer” table\nExample: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all courses are kept whether or not they have a Room (think online class), but any room that does NOT have a course is dropped (i.e. the kitchen probably doesn’t make it onto the output table)\n\nExample: SELECT DISTINCT i.Name AS Instructor, c.Name AS Course, c.Semester, r.Name AS Room FROM Instructor AS i LEFT OUTER JOIN Lesson AS l USING (Instructor_OID) LEFT OUTER JOIN Course AS c USING (Course_OID) LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all instructors are listed; only courses taught by instructors are listed, but not every instructor has a course; next, only rooms that have a course are listed, but not every course has a room\n\n\nExistential subquery vs join\n\nSome queries that can e answered joining related facts may be easier to define and faster to execute by querying for existence of a fact. For example, which instructors are teaching in a given semester:\n\nSELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID)\nI’ll be honest, this is the most challenging bit of code for me in this lecture. I’m going to try to break it down a little (for my own sanity)\n\nFirst, we want to select an instructor name and email from the instructor table under a given condition. That’s simple enough\nThe given condition is that something exists\nThe ‘something’ that exists is that we are able to select 1 (the number) for every row in an inner-joined table\nThat inner-joined table is Lesson and Course joined on Course_OID, but only where the semester is Fall 2023. Thus, only lessons taught in courses from fall 2023 will appear in this joined table\nOh. One more condition. AND the lesson instructor is the same as the instructor we’re querying? I don’t totally understand this\nAnyhoot, if these conditions are met, there is a 1 selected from this table for every row, and if at least one row ‘exists’, then that instructor taught a course in fall 2023, and his or her name and email address will appear on the output table\n\n\n\n\n\n\n\n\nWhat is the address for the DCC On-Demand website?\nLook at this code: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID). Assume neither Course nor Room have any null values. What is the name of the column that could have null values in the output table?\nThe following code uses existential subquerying, which can be more efficient than a join if your goal is to check that a condition is met: SELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID). 1. What are the two columns in the output table? 2. Which professors will be displayed in the output table? Only those who had a BLANK during BLANK BLANK\n\nBack"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html#logging-into-the-jupyter-apptainer",
    "href": "fall_24/b823/notes/september/sep_10.html#logging-into-the-jupyter-apptainer",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "Navigate to https://dcc-ondemand-01.oit.duke.edu\nLogin with Duke credentials when prompted\nSelect Interactive Apps from the menu bar\nSelect Jupyter Lab Apptainer\nSelect the number of hours estimated (probably a good idea to increase it by an hour)\nALWAYS increase the memory above the default of 2 (app will not launch properly)\nSelect path\n\n/opt/apps/containers/community/biostat/biostat-823-jupyter.sif\nHit Select once you’ve identified everything correctly\n\nHit Launch"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html#joining-tables",
    "href": "fall_24/b823/notes/september/sep_10.html#joining-tables",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "One interesting note about joins is that we’re usually taking a step backwards from normalization. This is totally acceptable and appropriate. Sometimes we need to summarize data in a different or non-standard way\nInner Joins\n\nInner Joins are the default, but it’s always important to specify for clarity\nExample: SELECT c.Name AS Course, c.Semester, l.Name AS Lesson FROM Course AS c INNER JOIN Lesson AS l ON (c.Course_OID = l.Course_OID);\nNo null values (I believe… unless there were existing NULL values in the selected columns before the join?)\nIf the column names are the same (as is the case in the example with Course_OID in both tables), we can shorten it to USING(...)\n\nOuter Joins\n\nSpecify whether it’s a left outer join or a right outer join\nKeeps all rows in the specified table (left or right), but it drops any rows in the adjacent table that do not match rows in the “outer” table\nExample: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all courses are kept whether or not they have a Room (think online class), but any room that does NOT have a course is dropped (i.e. the kitchen probably doesn’t make it onto the output table)\n\nExample: SELECT DISTINCT i.Name AS Instructor, c.Name AS Course, c.Semester, r.Name AS Room FROM Instructor AS i LEFT OUTER JOIN Lesson AS l USING (Instructor_OID) LEFT OUTER JOIN Course AS c USING (Course_OID) LEFT OUTER JOIN Room AS r USING (Room_OID)\n\nIn this example, all instructors are listed; only courses taught by instructors are listed, but not every instructor has a course; next, only rooms that have a course are listed, but not every course has a room\n\n\nExistential subquery vs join\n\nSome queries that can e answered joining related facts may be easier to define and faster to execute by querying for existence of a fact. For example, which instructors are teaching in a given semester:\n\nSELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID)\nI’ll be honest, this is the most challenging bit of code for me in this lecture. I’m going to try to break it down a little (for my own sanity)\n\nFirst, we want to select an instructor name and email from the instructor table under a given condition. That’s simple enough\nThe given condition is that something exists\nThe ‘something’ that exists is that we are able to select 1 (the number) for every row in an inner-joined table\nThat inner-joined table is Lesson and Course joined on Course_OID, but only where the semester is Fall 2023. Thus, only lessons taught in courses from fall 2023 will appear in this joined table\nOh. One more condition. AND the lesson instructor is the same as the instructor we’re querying? I don’t totally understand this\nAnyhoot, if these conditions are met, there is a 1 selected from this table for every row, and if at least one row ‘exists’, then that instructor taught a course in fall 2023, and his or her name and email address will appear on the output table"
  },
  {
    "objectID": "fall_24/b823/notes/september/sep_10.html#three-take-aways",
    "href": "fall_24/b823/notes/september/sep_10.html#three-take-aways",
    "title": "September 10, 2024 - September 12, 2024",
    "section": "",
    "text": "What is the address for the DCC On-Demand website?\nLook at this code: SELECT c.Name AS Course, c.Semester, r.Name AS Room FROM Course AS c LEFT OUTER JOIN Room AS r USING (Room_OID). Assume neither Course nor Room have any null values. What is the name of the column that could have null values in the output table?\nThe following code uses existential subquerying, which can be more efficient than a join if your goal is to check that a condition is met: SELECT i.Name AS Instructor, i.Email FROM Instructor AS i WHERE EXISTS ( SELECT 1 FROM Lesson as l INNER JOIN Course c USING (Course_OID) WHERE c.Semester = 'Fall 2023' AND l.Instructor_OID = i.Instructor_OID). 1. What are the two columns in the output table? 2. Which professors will be displayed in the output table? Only those who had a BLANK during BLANK BLANK\n\nBack"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW1-Relmod.html",
    "href": "fall_24/b823/projects/downloads/HW1-Relmod.html",
    "title": "Biostat 823 - Fall 2024: Homework 1",
    "section": "",
    "text": "Note: The two subquestions below both require you to create diagrams. You can use a diagramming tool of your choice, or you can use the same Rmarkdown-supported “plugin” (mermaid) used in the course slides. (BTW you can use mermaid also in some plain Markdown renderers, such as Hackmd.io. For example, here’s the E-R diagram from the course slides in Hackmd.io.) If you choose to use a scanned copy of your handwriting, make sure it’s clear and readable.\n\n(10 points) Create an E-R diagram to represent a physical data model for the following scenario, and use crow’s foot notation to show relationship cardinalities (similar to page 11 in module “Relational Data Modeling”).\nScenario: Cancer patients who are receiving routine chemotheapies under certain treatment guidelines (plans).\nRemark 1: In clinical practice, there are usually existing mature treatment guidelines for treating a certain type of cancer. We focus on guidelines for chemotherapy. The treatment guidelines consist of the number of sessions, the chemotherapeutic drugs for each session, and how the drugs are administrated for each session (e.g., injection or taken orally).\nRemark 2: Choose a set of entities to represent the data you would like to collect at a minimum and explain your choices. You don’t have to be very comprehensive.\n(15 points) Normalize the following database of inpatients to relational models in 1NF, 2NF, and 3NF. Create physical models (not logical ones); however, ignore indexes etc. (Note: Diagnosis depends on symptoms, and treatment is prescribed based on diagnosis.)\n\n\nInpatient database (not normalized)\n\n\n\n\n\n\nDatatype\nField\n\n\n\n\nString\nName\n\n\nString\nRoom\n\n\nInteger\nAge\n\n\nArray\nDoctors\n\n\nArray\nSymptons\n\n\nString\nDiagnosis\n\n\nArray\nTreatments"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-1-25-points",
    "href": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-1-25-points",
    "title": "Biostat 823 - Fall 2024: Homework 1",
    "section": "",
    "text": "Note: The two subquestions below both require you to create diagrams. You can use a diagramming tool of your choice, or you can use the same Rmarkdown-supported “plugin” (mermaid) used in the course slides. (BTW you can use mermaid also in some plain Markdown renderers, such as Hackmd.io. For example, here’s the E-R diagram from the course slides in Hackmd.io.) If you choose to use a scanned copy of your handwriting, make sure it’s clear and readable.\n\n(10 points) Create an E-R diagram to represent a physical data model for the following scenario, and use crow’s foot notation to show relationship cardinalities (similar to page 11 in module “Relational Data Modeling”).\nScenario: Cancer patients who are receiving routine chemotheapies under certain treatment guidelines (plans).\nRemark 1: In clinical practice, there are usually existing mature treatment guidelines for treating a certain type of cancer. We focus on guidelines for chemotherapy. The treatment guidelines consist of the number of sessions, the chemotherapeutic drugs for each session, and how the drugs are administrated for each session (e.g., injection or taken orally).\nRemark 2: Choose a set of entities to represent the data you would like to collect at a minimum and explain your choices. You don’t have to be very comprehensive.\n(15 points) Normalize the following database of inpatients to relational models in 1NF, 2NF, and 3NF. Create physical models (not logical ones); however, ignore indexes etc. (Note: Diagnosis depends on symptoms, and treatment is prescribed based on diagnosis.)\n\n\nInpatient database (not normalized)\n\n\n\n\n\n\nDatatype\nField\n\n\n\n\nString\nName\n\n\nString\nRoom\n\n\nInteger\nAge\n\n\nArray\nDoctors\n\n\nArray\nSymptons\n\n\nString\nDiagnosis\n\n\nArray\nTreatments"
  },
  {
    "objectID": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-2-25-points",
    "href": "fall_24/b823/projects/downloads/HW1-Relmod.html#question-2-25-points",
    "title": "Biostat 823 - Fall 2024: Homework 1",
    "section": "Question 2 (25 points)",
    "text": "Question 2 (25 points)\nSuppose you are implementing an E-R model for the following scenario:\n\n\n\n\n\n\nerDiagram\n    Room       ||--o{ Student : \"accommodates\"\n    Building   ||--|{ Room    : \"consists of\"\n    Building {\n      integer building_id PK\n      string  name\n      string  address\n    }\n    Room {\n      integer room_id PK\n      integer number\n      string  building_id FK\n    }\n    Student {\n      integer student_id PK\n      string  name\n      integer room_id FK\n    }\n\n\n\n\n\n\n\nScenario: Your school is welcoming a new cohort of undergraduate students this year. Every new student will be assigned to a room in an on-campus dormitory building. A building can contain many rooms, and rooms in one building are numbered by its floor level and sequence (e.g., 1089 means room 89 on the 10-th floor). You will be recording these on-campus address information for these students. The E-R model diagram is show on the right.\nThen, referring to pages 7 to 28 in Lesson “Structured Query Language”, answer the following questions (a to g). You should submit a reproducible literate programming notebook (Jupyter or Rmarkdown) with SQL statements to generate the answers for these questions (samples of reproducible notebooks for SQL can be found in the course’s GitHub repository. Using SQLite as the RDBMS is not mandatory but highly recommended.\nNote: You are only expected to provide answers (SQL codes) that are reproducible and comparable to those in lecture slides. You’re not expected to be extremely comprehensive or implement what hasn’t been covered in the lecture so far.\n\n(5 points) Create three tables: Building, Room, and Student. You need to satisfy the constraints that if a building is deleted, its rooms must be also deleted concurrently, and that a room cannot be removed if there are students assigned to that room. Also, for each entity (table), decide which attributes (columns) constitute(s) the natural primary key, and declare a unique constraint on those columns. Assume that columns that are neither primary nor foreign keys are not required (i.e., NULLable).\n(5 points) Populate the following data into your tables (you can find the data in CSV format for copy&pasting below):\n\n\n\n\nBuildings\n\n\nname\naddress\n\n\n\n\nGreen Light Building\n1000 Univ. Rd\n\n\nLakeview Building\n1080 Univ. Rd\n\n\nEast Grand Complex\n3810 Univ. Rd\n\n\nMountainview Building\nNULL\n\n\n\n\n\n\nRooms\n\n\nroom\nbuilding\n\n\n\n\n1001\nGreen Light Building\n\n\n1301\nEast Grand Complex\n\n\n1311\nLakeview Building\n\n\n\n\n\n\nStudents\n\n\nstudent_id\nname\nroom\nbuilding\n\n\n\n\n20240001\nAlpha Beta\n1001\nGreen Light Building\n\n\n20240002\nGamma Delta\n1311\nLakeview Building\n\n\n\n\n\n\n(4 points) Add one row to table “Student”, where the student: id is 20240003, name is “Theta Epsilon”, lives in room with number 1301 at East Grand Complex. Suppose you don’t know the room_id or the building_id (i.e., you need to include a query for room_id and building_id in your statement).\n(3 points) Select the buildings with missing (i.e., NULL) building address.\n(2 points) Update the address of Mountainview Building as “2420 Univ. Rd”.\n(3 points) Display buildings (name and address) on the 1000 block (addresses in the range of 1000 to 1100) of Univ. Rd.\n(3 points) Display the name and id of students who live in a room on the 13-th floor in a building on the 1000 block of Univ. Rd.\n\n\nBonus question (5 points)\nEven though after correctly populating the data as per b) above there should be no enforceable constraint violations (i.e., foreign keys, primary keys, uniqueness, NOT NULL), the data as given nonetheless are not fully consistent with the depicted E-R model. Where is the inconsistency, and what is its nature? (Hint: consider the relationship cardinalities as depicted in the diagram, and whether the given dataset satisfies them.)\n\n\nData (if useful)\nThe above data tables in CSV format:\n\nwrite.csv(buildings, quote = FALSE, row.names = FALSE)\n\nname,address\nGreen Light Building,1000 Univ. Rd\nLakeview Building,1080 Univ. Rd\nEast Grand Complex,3810 Univ. Rd\nMountainview Building,NULL\n\nwrite.csv(rooms, quote = FALSE, row.names = FALSE)\n\nroom,building\n1001,Green Light Building\n1301,East Grand Complex\n1311,Lakeview Building\n\nwrite.csv(students, quote = FALSE, row.names = FALSE)\n\nstudent_id,name,room,building\n20240001,Alpha Beta,1001,Green Light Building\n20240002,Gamma Delta,1311,Lakeview Building"
  }
]